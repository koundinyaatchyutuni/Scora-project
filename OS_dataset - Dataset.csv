Text,Topic,Questions
"Operating System Tutorial:Operating System Tutorial provides the basic and advanced concepts of operating system . Our Operating system tutorial is designed for beginners, professionals and GATE aspirants. We have designed this tutorial after the completion of a deep research about every concept.The content is described in detailed manner and has the ability to answer most of your queries. The tutorial also contains the numerical examples based on previous year GATE questions which will help you to address the problems in a practical manner. Operating System can be defined as an interface between user and the hardware. It provides an environment to the user so that, the user can perform its task in convenient and efficient way.The Operating System Tutorial is divided into various parts based on its functions such as Process Management, Process Synchronization, Deadlocks and File Management. Operating System Definition and Function:In the Computer System (comprises of Hardware and software), Hardware can only understand machine code (in the form of 0 and 1) which doesn't make any sense to a naive user.We need a system which can act as an intermediary and manage all the processes and resources present in the system.An Operating System can be defined as an interface between user and hardware. It is responsible for the execution of all the processes, Resource Allocation, CPU management, File Management and many other tasks.The purpose of an operating system is to provide an environment in which a user can execute programs in convenient and efficient manner.Structure of a Computer System :A Computer System consists of:What does an Operating system do?:Operating System Index:Operating System TutorialProcess ManagementSynchronizationDeadlocksMemory ManagementFile ManagementPrerequisites:Before learning the operating system tutorial, you must have the basic knowledge about the way in which a computer system operates.Audience:Our operating system tutorial is designed to help beginners, professionals and GATE aspirants.Problem:We can assure you that you will not find any problem in this operating system tutorial. However, if you find any, you can post the problem in the contact form.",os tutorial,"

Easy Questions:
1. What is an Operating System?
2. What is the purpose of an Operating System?
3. What are the components of a Computer System?

Medium Questions:
1. What are the functions of an Operating System?
2. What are the prerequisites for learning Operating System?
3. What are the topics covered in Operating System Tutorial?

Hard Questions:
1. How does an Operating System manage resources?
2. How does an Operating System provide an environment to the user?
3. What are the challenges faced while designing an Operating System?

Multiple Choice Questions:
1. What is the purpose of an Operating System?
A. To provide an environment to the user
B. To manage resources
C. To execute programs
D. All of the above

2. What are the components of a Computer System?
A. Hardware
B. Software
C. Operating System
D. All of the above

3. What are the topics covered in Operating System Tutorial?
A. Process Management
B. Process Synchronization
C. Deadlocks
D. All of the above"
"Types of Operating Systems (OS):An operating system is a well-organized collection of programs that manages the computer hardware. It is a type of system software that is responsible for the smooth functioning of the computer system.Batch Operating System:In the 1970s, Batch processing was very popular. In this technique, similar types of jobs were batched together and executed in time. People were used to having a single computer which was called a mainframe.In Batch operating system, access is given to more than one person; they submit their respective jobs to the system for the execution. The system put all of the jobs in a queue on the basis of first come first serve and then executes the jobs one by one. The users collect their respective output when all the jobs get executed. The purpose of this operating system was mainly to transfer control from one job to another as soon as the job was completed. It contained a small set of programs called the resident monitor that always resided in one part of the main memory. The remaining part is used for servicing jobs.Advantages of Batch OS:Disadvantages of Batch OS:1. StarvationBatch processing suffers from starvation.For Example:There are five jobs J1, J2, J3, J4, and J5, present in the batch. If the execution time of J1 is very high, then the other four jobs will never be executed, or they will have to wait for a very long time. Hence the other processes get starved.2. Not InteractiveBatch Processing is not suitable for jobs that are dependent on the user's input. If a job requires the input of two numbers from the console, then it will never get it in the batch processing scenario since the user is not present at the time of execution.Multiprogramming Operating System:Multiprogramming is an extension to batch processing where the CPU is always kept busy. Each process needs two types of system time: CPU time and IO time.In a multiprogramming environment, when a process does its I/O, The CPU can start the execution of other processes. Therefore, multiprogramming improves the efficiency of the system.Advantages of Multiprogramming OS:Disadvantages of Multiprogramming OS:Multiprocessing Operating System:In Multiprocessing, Parallel computing is achieved. There are more than one processors present in the system which can execute more than one process at the same time. This will increase the throughput of the system.In Multiprocessing, Parallel computing is achieved. More than one processor present in the system can execute more than one process simultaneously, which will increase the throughput of the system.Advantages of Multiprocessing operating system:Disadvantages of Multiprocessing operating SystemMultitasking Operating System:The multitasking operating system is a logical extension of a multiprogramming system that enables multiple programs simultaneously. It allows a user to perform more than one computer task at the same time.Advantages of Multitasking operating system:Disadvantages of Multitasking operating system:Network Operating System:An Operating system, which includes software and associated protocols to communicate with other computers via a network conveniently and cost-effectively, is called Network Operating System.Advantages of Network Operating System:Disadvantages of Network Operating System:Real Time Operating System:In Real-Time Systems, each job carries a certain deadline within which the job is supposed to be completed, otherwise, the huge loss will be there, or even if the result is produced, it will be completely useless.The Application of a Real-Time system exists in the case of military applications, if you want to drop a missile, then the missile is supposed to be dropped with a certain precision.Advantages of Real-time operating system::Disadvantages of Real-time operating system::Time-Sharing Operating System:In the Time Sharing operating system, computer resources are allocated in a time-dependent fashion to several programs simultaneously. Thus it helps to provide a large number of user's direct access to the main computer. It is a logical extension of multiprogramming. In time-sharing, the CPU is switched among multiple programs given by different users on a scheduled basis.A time-sharing operating system allows many users to be served simultaneously, so sophisticated CPU scheduling schemes and Input/output management are required.Time-sharing operating systems are very difficult and expensive to build.Advantages of Time Sharing Operating System:Disadvantages of Time Sharing Operating System:Distributed Operating System:The Distributed Operating system is not installed on a single machine, it is divided into parts, and these parts are loaded on different machines. A part of the distributed Operating system is installed on each machine to make their communication possible. Distributed Operating systems are much more complex, large, and sophisticated than Network operating systems because they also have to take care of varying networking protocols.Advantages of Distributed Operating System:Disadvantages of Distributed Operating System:",types of operating systems,"

Easy Questions:
1. What is an operating system?
2. What is batch processing?
3. What is multiprogramming?

Medium Questions:
1. What are the advantages of batch operating system?
2. What are the disadvantages of multiprocessing operating system?
3. What is the purpose of a real-time operating system?

Hard Questions:
1. What are the differences between a distributed operating system and a network operating system?
2. What are the challenges of building a time-sharing operating system?
3. How does multitasking operating system improve the efficiency of the system?

Multiple Choice Questions:
1. What is the purpose of a batch operating system?
A. To transfer control from one job to another
B. To provide direct access to the main computer
C. To provide access to multiple users
D. To improve the efficiency of the system

2. What is the main advantage of multiprogramming?
A. Increased throughput
B. Improved efficiency
C. Reduced cost
D. Increased user access

3. What is the main disadvantage of a real-time operating system?
A. High cost
B. Limited user access
C. Missed deadlines
D. Poor performance"
"Process Management in OS:A Program does nothing unless its instructions are executed by a CPU. A program in execution is called a process. In order to accomplish its task, process needs the computer resources.There may exist more than one process in the system which may require the same resource at the same time. Therefore, the operating system has to manage all the processes and the resources in a convenient and efficient way. Some resources may need to be executed by one process at one time to maintain the consistency otherwise the system can become inconsistent and deadlock may occur. The operating system is responsible for the following activities in connection with Process Management",process management in os,".

Easy Questions:
1. What is a program in execution called?
2. What is the purpose of process management in an OS?
3. What is the responsibility of an OS in connection with process management?

Medium Questions:
1. What can happen if the system becomes inconsistent?
2. How does the OS manage processes and resources?
3. What is the importance of executing one process at one time?

Hard Questions:
1. How does the OS prevent deadlock from occurring?
2. What techniques are used to ensure the efficient management of processes and resources?
3. How does the OS ensure the consistency of the system?

Multiple Choice Questions:
1. What is the purpose of process management in an OS?
a. To manage all the processes and resources
b. To execute instructions by a CPU
c. To maintain the consistency of the system
d. To prevent deadlock from occurring

2. What is the responsibility of an OS in connection with process management?
a. To manage all the processes and resources
b. To execute instructions by a CPU
c. To maintain the consistency of the system
d. To prevent deadlock from occurring

3. What can happen if the system becomes inconsistent?
a. Deadlock may occur
b. The system may become efficient
c. The system may become reliable
d. The system may become stable"
"Attributes of a process:The Attributes of the process are used by the Operating System to create the process control block (PCB) for each of them. This is also called context of the process. Attributes which are stored in the PCB are described below.1. Process ID:When a process is created, a unique id is assigned to the process which is used for unique identification of the process in the system.2. Program counter:A program counter stores the address of the last instruction of the process on which the process was suspended. The CPU uses this address when the execution of this process is resumed.3. Process State:The Process, from its creation to the completion, goes through various states which are new, ready, running and waiting. We will discuss about them later in detail.4. Priority:Every process has its own priority. The process with the highest priority among the processes gets the CPU first. This is also stored on the process control block.5. General Purpose Registers:Every process has its own set of registers which are used to hold the data which is generated during the execution of the process.6. List of open files:During the Execution, Every process uses some files which need to be present in the main memory. OS also maintains a list of open files in the PCB.7. List of open devices:OS also maintain the list of all open devices which are used during the execution of the process.",os attributes of a process,"

Easy Questions:
1. What is a Process Control Block?
2. What is the purpose of a Program Counter?
3. What is the purpose of Priority in a Process?

Medium Questions:
1. What are the different states of a Process?
2. What is the purpose of General Purpose Registers?
3. What is the purpose of a List of Open Files?

Hard Questions:
1. How does the Operating System assign a unique ID to a Process?
2. How does the Operating System determine which Process has the highest Priority?
3. How does the Operating System maintain a List of Open Devices?

Multiple Choice Questions:
1. What is the purpose of a Program Counter?
A. To store the address of the last instruction of the process
B. To store the data generated during the execution of the process
C. To store the unique ID of the process
D. To store the priority of the process

2. What is the purpose of Priority in a Process?
A. To store the address of the last instruction of the process
B. To store the data generated during the execution of the process
C. To store the unique ID of the process
D. To determine which process gets the CPU first

3. How does the Operating System maintain a List of Open Devices?
A. By assigning a unique ID to each device
B. By assigning a priority to each device
C. By storing the address of the last instruction of the process
D. By storing the data generated during the execution of the process"
"Process States:State DiagramThe process, from its creation to completion, passes through various states. The minimum number of states is five.The names of the states are not standardized although the process may be in one of the following states during execution.1. New:A program which is going to be picked up by the OS into the main memory is called a new process.2. Ready:Whenever a process is created, it directly enters in the ready state, in which, it waits for the CPU to be assigned. The OS picks the new processes from the secondary memory and put all of them in the main memory. The processes which are ready for the execution and reside in the main memory are called ready state processes. There can be many processes present in the ready state.3. Running:One of the processes from the ready state will be chosen by the OS depending upon the scheduling algorithm. Hence, if we have only one CPU in our system, the number of running processes for a particular time will always be one. If we have n processors in the system then we can have n processes running simultaneously.4. Block or wait:From the Running state, a process can make the transition to the block or wait state depending upon the scheduling algorithm or the intrinsic behavior of the process. When a process waits for a certain resource to be assigned or for the input from the user then the OS move this process to the block or wait state and assigns the CPU to the other processes. 5. Completion or termination:When a process finishes its execution, it comes in the termination state. All the context of the process (Process Control Block) will also be deleted the process will be terminated by the Operating system.6. Suspend ready:A process in the ready state, which is moved to secondary memory from the main memory due to lack of the resources (mainly primary memory) is called in the suspend ready state.If the main memory is full and a higher priority process comes for the execution then the OS have to make the room for the process in the main memory by throwing the lower priority process out into the secondary memory. The suspend ready processes remain in the secondary memory until the main memory gets available. 7. Suspend wait:Instead of removing the process from the ready queue, it's better to remove the blocked process which is waiting for some resources in the main memory. Since it is already waiting for some resource to get available hence it is better if it waits in the secondary memory and make room for the higher priority process. These processes complete their execution once the main memory gets available and their wait is finished.Operations on the Process:1. Creation:Once the process is created, it will be ready and come into the ready queue (main memory) and will be ready for the execution.2. Scheduling:Out of the many processes present in the ready queue, the Operating system chooses one process and start executing it. Selecting the process which is to be executed next, is known as scheduling. 3. Execution:Once the process is scheduled for the execution, the processor starts executing it. Process may come to the blocked or wait state during the execution then in that case the processor starts executing the other processes. 4. Deletion/killing:Once the purpose of the process gets over then the OS will kill the process. The Context of the process (PCB) will be deleted and the process gets terminated by the Operating system.",os process states,"

Easy Questions:
1. What is the minimum number of states a process can have?
2. What is the name of the state when a process is ready for execution?
3. What is the name of the state when a process is waiting for a certain resource to be assigned?

Medium Questions:
1. What is the purpose of scheduling a process?
2. What is the name of the state when a process is moved to secondary memory from the main memory due to lack of resources?
3. What happens to the context of the process when it is deleted or killed?

Hard Questions:
1. What is the difference between suspend ready and suspend wait states?
2. How does the Operating System decide which process to execute next?
3. What is the difference between a new process and a ready process?

Multiple Choice Questions:
1. What is the name of the state when a process is ready for execution?
A. New
B. Ready
C. Running
D. Block or Wait

2. What is the purpose of scheduling a process?
A. To select the process which is to be executed next
B. To move the process from the ready queue to the block or wait state
C. To move the process from the main memory to the secondary memory
D. To delete the context of the process

3. What happens to the context of the process when it is deleted or killed?
A. It is moved to the secondary memory
B. It is moved to the ready queue
C. It is moved to the block or wait state
D. It is deleted"
"Process Scheduling in OS (Operating System):Operating system uses various schedulers for the process scheduling described below.1. Long term scheduler:Long term scheduler is also known as job scheduler. It chooses the processes from the pool (secondary memory) and keeps them in the ready queue maintained in the primary memory.Long Term scheduler mainly controls the degree of Multiprogramming. The purpose of long term scheduler is to choose a perfect mix of IO bound and CPU bound processes among the jobs present in the pool.If the job scheduler chooses more IO bound processes then all of the jobs may reside in the blocked state all the time and the CPU will remain idle most of the time. This will reduce the degree of Multiprogramming. Therefore, the Job of long term scheduler is very critical and may affect the system for a very long time.2. Short term scheduler:Short term scheduler is also known as CPU scheduler. It selects one of the Jobs from the ready queue and dispatch to the CPU for the execution.A scheduling algorithm is used to select which job is going to be dispatched for the execution. The Job of the short term scheduler can be very critical in the sense that if it selects job whose CPU burst time is very high then all the jobs after that, will have to wait in the ready queue for a very long time.This problem is called starvation which may arise if the short term scheduler makes some mistakes while selecting the job.3. Medium term scheduler:Medium term scheduler takes care of the swapped out processes.If the running state processes needs some IO time for the completion then there is a need to change its state from running to waiting.Medium term scheduler is used for this purpose. It removes the process from the running state to make room for the other processes. Such processes are the swapped out processes and this procedure is called swapping. The medium term scheduler is responsible for suspending and resuming the processes.It reduces the degree of multiprogramming. The swapping is necessary to have a perfect mix of processes in the ready queue.",os process schedulers,"

Easy Questions:
1. What is the purpose of the long term scheduler?
2. What is the purpose of the short term scheduler?
3. What is the purpose of the medium term scheduler?

Medium Questions:
1. What is the effect of the long term scheduler on the degree of multiprogramming?
2. What is the effect of the short term scheduler on the ready queue?
3. What is the effect of the medium term scheduler on the degree of multiprogramming?

Hard Questions:
1. What is the impact of the long term scheduler on the system performance?
2. What is the impact of the short term scheduler on the system performance?
3. What is the impact of the medium term scheduler on the system performance?

Multiple Choice Questions:
1. What is the purpose of the long term scheduler?
a. To select one of the jobs from the ready queue
b. To choose a perfect mix of IO bound and CPU bound processes
c. To reduce the degree of multiprogramming
d. To suspend and resume processes

2. What is the purpose of the short term scheduler?
a. To select one of the jobs from the ready queue
b. To choose a perfect mix of IO bound and CPU bound processes
c. To reduce the degree of multiprogramming
d. To suspend and resume processes

3. What is the purpose of the medium term scheduler?
a. To select one of the jobs from the ready queue
b. To choose a perfect mix of IO bound and CPU bound processes
c. To reduce the degree of multiprogramming
d. To suspend and resume processes"
"Process Queues:The Operating system manages various types of queues for each of the process states. The PCB related to the process is also stored in the queue of the same state. If the Process is moved from one state to another state then its PCB is also unlinked from the corresponding queue and added to the other state queue in which the transition is made.There are the following queues maintained by the Operating system.1. Job Queue:In starting, all the processes get stored in the job queue. It is maintained in the secondary memory. The long term scheduler (Job scheduler) picks some of the jobs and put them in the primary memory.2. Ready Queue:Ready queue is maintained in primary memory. The short term scheduler picks the job from the ready queue and dispatch to the CPU for the execution.3. Waiting Queue:When the process needs some IO operation in order to complete its execution, OS changes the state of the process from running to waiting. The context (PCB) associated with the process gets stored on the waiting queue which will be used by the Processor when the process finishes the IO.",os process queues,"

Easy Questions:
1. What is a Process Queue?
2. What is a PCB?
3. What is the purpose of the Job Queue?

Medium Questions:
1. What is the difference between the Job Queue and Ready Queue?
2. What is the role of the Long Term Scheduler?
3. What is the purpose of the Waiting Queue?

Hard Questions:
1. How does the Operating System manage the Process Queues?
2. How does the Operating System move a Process from one state to another?
3. What is the difference between the Long Term Scheduler and the Short Term Scheduler?

Multiple Choice Questions:
1. What is the purpose of the Job Queue?
A. To store processes in primary memory
B. To store processes in secondary memory
C. To pick processes from the ready queue
D. To dispatch processes to the CPU

Answer: B

2. What is the role of the Long Term Scheduler?
A. To store processes in primary memory
B. To store processes in secondary memory
C. To pick processes from the ready queue
D. To dispatch processes to the CPU

Answer: C

3. How does the Operating System move a Process from one state to another?
A. By unlink the PCB from the corresponding queue
B. By linking the PCB to the corresponding queue
C. By adding the PCB to the other state queue
D. By removing the PCB from the other state queue

Answer: A"
"Various Times related to the Process:1. Arrival Time:The time at which the process enters into the ready queue is called the arrival time.2. Burst Time:The total amount of time required by the CPU to execute the whole process is called the Burst Time. This does not include the waiting time. It is confusing to calculate the execution time for a process even before executing it hence the scheduling problems based on the burst time cannot be implemented in reality. 3. Completion Time:The Time at which the process enters into the completion state or the time at which the process completes its execution, is called completion time. 4. Turnaround time:The total amount of time spent by the process from its arrival to its completion, is called Turnaround time.5. Waiting Time:The Total amount of time for which the process waits for the CPU to be assigned is called waiting time.6. Response Time:The difference between the arrival time and the time at which the process first gets the CPU is called Response Time.",os various time related to the process,"

Easy Questions:
1. What is the Arrival Time?
2. What is the Burst Time?
3. What is the Completion Time?

Medium Questions:
1. What is the difference between Burst Time and Waiting Time?
2. How can scheduling problems based on Burst Time be implemented in reality?
3. What is the Turnaround Time?

Hard Questions:
1. How can the Response Time be calculated?
2. What is the relationship between Arrival Time and Response Time?
3. How can the Waiting Time be minimized?

Multiple Choice Questions:
1. What is the total amount of time spent by the process from its arrival to its completion?
A. Arrival Time
B. Burst Time
C. Completion Time
D. Turnaround Time

2. What is the difference between the arrival time and the time at which the process first gets the CPU?
A. Burst Time
B. Completion Time
C. Turnaround Time
D. Response Time

3. What is the total amount of time required by the CPU to execute the whole process?
A. Arrival Time
B. Burst Time
C. Completion Time
D. Waiting Time"
"CPU Scheduling:In the uniprogrammming systems like MS DOS, when a process waits for any I/O operation to be done, the CPU remains idol. This is an overhead since it wastes the time and causes the problem of starvation. However, In Multiprogramming systems, the CPU doesn't remain idle during the waiting time of the Process and it starts executing other processes. Operating System has to define which process the CPU will be given.In Multiprogramming systems, the Operating system schedules the processes on the CPU to have the maximum utilization of it and this procedure is called CPU scheduling. The Operating System uses various scheduling algorithm to schedule the processes. This is a task of the short term scheduler to schedule the CPU for the number of processes present in the Job Pool. Whenever the running process requests some IO operation then the short term scheduler saves the current context of the process (also called PCB) and changes its state from running to waiting. During the time, process is in waiting state; the Short term scheduler picks another process from the ready queue and assigns the CPU to this process. This procedure is called context switching. What is saved in the Process Control Block?:The Operating system maintains a process control block during the lifetime of the process. The Process control block is deleted when the process is terminated or killed. There is the following information which is saved in the process control block and is changing with the state of the process.Why do we need Scheduling?:In Multiprogramming, if the long term scheduler picks more I/O bound processes then most of the time, the CPU remains idol. The task of Operating system is to optimize the utilization of resources. If most of the running processes change their state from running to waiting then there may always be a possibility of deadlock in the system. Hence to reduce this overhead, the OS needs to schedule the jobs to get the optimal utilization of CPU and to avoid the possibility to deadlock.",os cpu scheduling,"

Easy Questions:
1. What is CPU scheduling?
2. What is the purpose of scheduling?
3. What is a Process Control Block?

Medium Questions:
1. What is the difference between uniprogramming and multiprogramming systems?
2. What is the role of the short term scheduler?
3. What is context switching?

Hard Questions:
1. What is the impact of scheduling on system performance?
2. How does the Operating System decide which process to schedule?
3. What are the different scheduling algorithms used by the Operating System?

Multiple Choice Questions:
1. What is the purpose of scheduling?
A. To optimize the utilization of resources
B. To reduce overhead
C. To avoid deadlock
D. All of the above

2. What is the role of the short term scheduler?
A. To pick more I/O bound processes
B. To save the current context of the process
C. To assign the CPU to a process
D. All of the above

3. What is saved in the Process Control Block?
A. The current context of the process
B. The state of the process
C. The lifetime of the process
D. All of the above"
"Scheduling Algorithms in OS (Operating System):There are various algorithms which are used by the Operating System to schedule the processes on the processor in an efficient way. The Purpose of a Scheduling algorithm :There are the following algorithms which can be used to schedule the jobs.1. First Come First Serve:It is the simplest algorithm to implement. The process with the minimal arrival time will get the CPU first. The lesser the arrival time, the sooner will the process gets the CPU. It is the non-preemptive type of scheduling.2. Round Robin:In the Round Robin scheduling algorithm, the OS defines a time quantum (slice). All the processes will get executed in the cyclic way. Each of the process will get the CPU for a small amount of time (called time quantum) and then get back to the ready queue to wait for its next turn. It is a preemptive type of scheduling. 3. Shortest Job First:The job with the shortest burst time will get the CPU first. The lesser the burst time, the sooner will the process get the CPU. It is the non-preemptive type of scheduling. 4. Shortest remaining time first:It is the preemptive form of SJF. In this algorithm, the OS schedules the Job according to the remaining time of the execution.5. Priority based scheduling:In this algorithm, the priority will be assigned to each of the processes. The higher the priority, the sooner will the process get the CPU. If the priority of the two processes is same then they will be scheduled according to their arrival time.6. Highest Response Ratio Next:In this scheduling Algorithm, the process with highest response ratio will be scheduled next. This reduces the starvation in the system. ",os scheduling algorithms,"

Easy Questions:
1. What is the purpose of scheduling algorithms in OS?
2. What is the First Come First Serve algorithm?
3. What is the time quantum in Round Robin scheduling?

Medium Questions:
1. What is the difference between preemptive and non-preemptive scheduling?
2. What is the difference between SJF and SRTF scheduling?
3. What is the Highest Response Ratio Next scheduling algorithm?

Hard Questions:
1. How does the Priority based scheduling algorithm work?
2. How does the Highest Response Ratio Next scheduling algorithm reduce starvation?
3. What are the advantages and disadvantages of the First Come First Serve scheduling algorithm?

Multiple Choice Questions:
1. What type of scheduling algorithm is First Come First Serve?
a. Preemptive
b. Non-preemptive
c. Priority based
d. Round Robin

2. What type of scheduling algorithm is Shortest Job First?
a. Preemptive
b. Non-preemptive
c. Priority based
d. Round Robin

3. What type of scheduling algorithm is Highest Response Ratio Next?
a. Preemptive
b. Non-preemptive
c. Priority based
d. Round Robin"
"First Come First Serve CPU Process Scheduling in Operating Systems:In this tutorial, we are going to learn an important concept in CPU Process Scheduling Algorithms. The important concept name is First Come First Serve. This is the basic algorithm which every student must learn to understand all the basics of CPU Process Scheduling Algorithms.First Come First Serve paves the way for understanding of other algorithms. This algorithm may have many disadvantages. But these disadvantages created very new and efficient algorithms. So, it is our responsibility to learn about First Come First Serve CPU Process Scheduling Algorithms.Important Abbreviations:First Come First Serve:First Come First Serve CPU Scheduling Algorithm shortly known as FCFS is the first algorithm of CPU Process Scheduling Algorithm. In First Come First Serve Algorithm what we do is to allow the process to execute in linear manner.This means that whichever process enters process enters the ready queue first is executed first. This shows that First Come First Serve Algorithm follows First In First Out (FIFO) principle.The First Come First Serve Algorithm can be executed in Pre Emptive and Non Pre Emptive manner. Before, going into examples, let us understand what is Pre Emptive and Non Pre Emptive Approach in CPU Process Scheduling.Pre Emptive Approach:In this instance of Pre Emptive Process Scheduling, the OS allots the resources to a Process for a predetermined period of time. The process transitions from running state to ready state or from waiting state to ready state during resource allocation. This switching happens because the CPU may assign other processes precedence and substitute the currently active process for the higher priority process.Non Pre Emptive Approach:In this case of Non Pre Emptive Process Scheduling, the resource cannot be withdrawn from a process before the process has finished running. When a running process finishes and transitions to the waiting state, resources are switched.Convoy Effect In First Come First Serve (FCFS ):Convoy Effect is a phenomenon which occurs in the Scheduling Algorithm named First Come First Serve (FCFS). The First Come First Serve Scheduling Algorithm occurs in a way of non preemptive way.The Non preemptive way means that if a process or job is started execution, then the operating system must complete its process or job. Until, the process or job is zero the new or next process or job does not start its execution. The definition of Non Preemptive Scheduling in terms of Operating System means that the Central Processing Unit (CPU) will be completely dedicated till the end of the process or job started first and the new process or job is executed only after finishing of the older process or job.There may be a few cases, which might cause the Central Processing Unit (CPU) to allot a too much time. This is because in the First Come First Serve Scheduling Algorithm Non Preemptive Approach, the Processes or the jobs are chosen in serial order. Due, to this shorter jobs or processes behind the larger processes or jobs takes too much time to complete its execution. Due, to this the Waiting Time, Turn Around Time, Completion Time is very high.So, here as the first process is large or completion time is too high, then this Convoy effect in the First Come First Serve Algorithm is occurred.Let us assume that Longer Job takes infinite time to complete. Then, the remaining processes have to wait for the same infinite time. Due to this Convoy Effect created by the Longer Job the Starvation of the waiting processes increases very rapidly. This is the biggest disadvantage of FCFS CPU Process Scheduling.Characteristics of FCFS CPU Process Scheduling:The characteristics of FCFS CPU Process Scheduling are:Advantages of FCFS CPU Process Scheduling:The advantages of FCFS CPU Process Scheduling are:Disadvantages of FCFS CPU Process Scheduling:The disadvantages of FCFS CPU Process Scheduling are:Problems in the First Come First Serve CPU Scheduling AlgorithmExample:Non Pre Emptive Approach:Now, let us solve this problem with the help of the Scheduling Algorithm named First Come First Serve in a Non Preemptive Approach.Gantt chart for the above Example 1 is:Turn Around Time = Completion Time - Arrival TimeWaiting Time = Turn Around Time - Burst TimeSolution to the Above Question Example 1:The Average Completion Time is:Average CT = ( 9 + 12 + 14 + 18 + 21 + 23 ) / 6Average CT = 97 / 6Average CT = 16.16667The Average Waiting Time is:Average WT = ( 0 + 8 + 11 + 13 + 16 + 18 ) /6Average WT = 66 / 6Average WT = 11The Average Turn Around Time is:Average TAT = ( 9 + 11 + 13 + 17 + 19 +20 ) / 6Average TAT = 89 / 6Average TAT = 14.83334This is how the FCFS is solved in Non Pre Emptive Approach.Now, let us understand how they can be solved in Pre Emptive ApproachPre Emptive Approach:Now, let us solve this problem with the help of the Scheduling Algorithm named First Come First Serve in a Pre Emptive Approach.In Pre Emptive Approach we search for the best process which is availableGantt chart for the above Example 1 is:Semaphores in OS (Operating System):To get rid of the problem of wasting the wake-up signals, Dijkstra proposed an approach which involves storing all the wake-up calls. Dijkstra states that, instead of giving the wake-up calls directly to the consumer, producer can store the wake-up call in a variable. Any of the consumers can read it whenever it needs to do so.Semaphore is the variables which storesthe entire wake up calls that are being transferred from producer to consumer. It is a variable on which read, modify and update happens automatically in kernel mode.Semaphore cannot be implemented in the user mode because race condition may always arise when two or more processes try to access the variable simultaneously. It always needs support from the operating system to be implemented.According to the demand of the situation, Semaphore can be divided into two categories.We will discuss each one in detail.The Average Completion Time is:Average CT = ( 23 + 8 + 3 + 15 + 11 + 5 ) / 6Average CT = 65 / 6Average CT = 10.83333The Average Waiting Time is:Average WT = ( 14 + 4 + 0 + 10 + 7 + 0 ) /6Average WT = 35 / 6Average WT = 5.83333The Average Turn Around Time is:Average TAT = ( 23 + 7 + 2 + 14 + 9 +2 ) / 6Average TAT = 57 / 6Average TAT = 9.5This is how the FCFS is solved in Pre Emptive Approach.",os fcfs scheduling,"

Easy Questions:
1. What is First Come First Serve CPU Scheduling Algorithm?
2. What is the principle followed by First Come First Serve Algorithm?
3. What are the two approaches of CPU Process Scheduling?

Medium Questions:
1. What is the Convoy Effect in First Come First Serve Algorithm?
2. What are the characteristics of FCFS CPU Process Scheduling?
3. What are the advantages and disadvantages of FCFS CPU Process Scheduling?

Hard Questions:
1. How can the First Come First Serve Algorithm be solved in Pre Emptive Approach?
2. What is the Average Completion Time, Waiting Time and Turn Around Time for the example given in the paragraph?
3. What is the purpose of Semaphores in Operating Systems?

Multiple Choice Questions:
1. What is the principle followed by First Come First Serve Algorithm?
A. First In First Out (FIFO)
B. Last In First Out (LIFO)
C. First Come First Serve (FCFS)
D. Last Come First Serve (LCFS)

2. What are the two approaches of CPU Process Scheduling?
A. Pre Emptive and Non Pre Emptive
B. Pre Emptive and Post Emptive
C. Pre Emptive and Non Emptive
D. Pre Emptive and Post Non Emptive

3. What is the Convoy Effect in First Come First Serve Algorithm?
A. The process or job is chosen in serial order
B. The process or job is chosen in parallel order
C. The process or job is chosen in random order
D. The process or job is chosen in sequential order"
"Convoy Effect in FCFS:FCFS may suffer from the convoy effect if the burst time of the first job is the highest among all. As in the real life, if a convoy is passing through the road then the other persons may get blocked until it passes completely. This can be simulated in the Operating System also. If the CPU gets the processes of the higher burst time at the front end of the ready queue then the processes of lower burst time may get blocked which means they may never get the CPU if the job in the execution has a very high burst time. This is called convoy effect or starvation. Example:In the Example, We have 3 processes named as P1, P2 and P3. The Burt Time of process P1 is highest. The Turnaround time and the waiting time in the following table, are calculated by the formula,In the First scenario, The Process P1 arrives at the first in the queue although; the burst time of the process is the highest among all. Since, the Scheduling algorithm, we are following is FCFS hence the CPU will execute the Process P1 first.In this schedule, the average waiting time of the system will be very high. That is because of the convoy effect. The other processes P2, P3 have to wait for their turn for 40 units of time although their burst time is very low. This schedule suffers from starvation.         Avg waiting Time = 81/3In the Second scenario, If Process P1 would have arrived at the last of the queue and the other processes P2 and P3 at earlier then the problem of starvation would not be there. Following example shows the deviation in the waiting times of both the scenarios. Although the length of the schedule is same that is 44 units but the waiting time will be lesser in this schedule.          Avg Waiting Time=6/3",os convoy effect in fcfs,"

Easy Questions:
1. What is the Convoy Effect?
2. What is the scheduling algorithm used in the example?
3. What is the average waiting time in the first scenario?

Medium Questions:
1. What is the difference between the waiting times of the two scenarios?
2. How does the convoy effect cause starvation?
3. What is the length of the schedule in the example?

Hard Questions:
1. How can the convoy effect be simulated in an operating system?
2. How can the convoy effect be avoided in FCFS scheduling?
3. What is the formula used to calculate the turnaround time and waiting time in the example?

Multiple Choice Questions:
1. What is the average waiting time in the first scenario?
A. 6/3
B. 81/3
C. 44
D. 40

2. What is the scheduling algorithm used in the example?
A. SJF
B. RR
C. FCFS
D. Priority

3. How does the convoy effect cause starvation?
A. Processes of lower burst time may never get the CPU
B. Processes of higher burst time may never get the CPU
C. Processes of higher burst time may get blocked
D. Processes of lower burst time may get blocked"
"FCFS with Overhead:In the above Examples, we are assuming that all the processes are the CPU bound processes only. We were also neglecting the context switching time. However if the time taken by the scheduler in context switching is considered then the average waiting time of the system will be increased which also affects the efficiency of the system.Context Switching is always an overhead. The Following Example describeshow the efficiency will be affected if the context switching time is considered in the system. Example:In the following Example, we are considering five processes P1, P2, P3, P4, P5 and P6. Their arrival time and Burst time are given below.If the context switching time of the system is 1 unit then the Gantt chart of the system will be prepared as follows.Given δ=1 unit;The system will take extra 1 unit of time (overhead) after the execution of every process to schedule the next process.",os fcfs with overhead,"

Easy Questions:
1. What is FCFS with Overhead?
2. What is the purpose of context switching?
3. What is the overhead time in the example given?

Medium Questions:
1. How does context switching time affect the average waiting time of the system?
2. What is the Gantt chart of the system in the example given?
3. How does the overhead time affect the efficiency of the system?

Hard Questions:
1. How can the overhead time be minimized in the system?
2. How can the context switching time be optimized in the system?
3. What are the different strategies to reduce the overhead time in the system?

Multiple Choice Questions:
1. What is the overhead time in the example given?
A. 0 unit
B. 1 unit
C. 2 unit
D. 5 unit

2. How does context switching time affect the average waiting time of the system?
A. Increases
B. Decreases
C. No effect
D. Cannot be determined

3. What is the Gantt chart of the system in the example given?
A. P1, P2, P3, P4, P5, P6
B. P1, P3, P5, P2, P4, P6
C. P1, P2, P3, P4, P5, P6, δ
D. P1, P2, P3, P4, P5, δ, P6"
"Shortest Job First (SJF) Scheduling:Till now, we were scheduling the processes according to their arrival time (in FCFS scheduling). However, SJF scheduling algorithm, schedules the processes according to their burst time. In SJF scheduling, the process with the lowest burst time, among the list of available processes in the ready queue, is going to be scheduled next.However, it is very difficult to predict the burst time needed for a process hence this algorithm is very difficult to implement in the system.Advantages of SJF:Disadvantages of SJF:There are different techniques available by which, the CPU burst time of the process can be determined. We will discuss them later in detail.Example:In the following example, there are five jobs named as P1, P2, P3, P4 and P5. Their arrival time and burst time are given in the table below.Since, No Process arrives at time 0 hence; there will be an empty slot in the Gantt chart from time 0 to 1 (the time at which the first process arrives).According to the algorithm, the OS schedules the process which is having the lowest burst time among the available processes in the ready queue. Till now, we have only one process in the ready queue hence the scheduler will schedule this to the processor no matter what is its burst time. This will be executed till 8 units of time. Till then we have three more processes arrived in the ready queue hence the scheduler will choose the process with the lowest burst time. Among the processes given in the table, P3 will be executed next since it is having the lowest burst time among all the available processes. So that's how the procedure will go on in shortest job first (SJF) scheduling algorithm.         Avg Waiting Time = 27/5",os sjf scheduling," = 5.4

Easy Questions:
1. What is SJF Scheduling?
2. What is the purpose of SJF Scheduling?
3. What is the average waiting time in the example given?

Medium Questions:
1. What are the advantages of SJF Scheduling?
2. What are the techniques used to determine the CPU burst time of a process?
3. How does the scheduler choose the process with the lowest burst time?

Hard Questions:
1. How does SJF Scheduling compare to FCFS Scheduling?
2. What are the challenges of implementing SJF Scheduling?
3. How can SJF Scheduling be improved?

Multiple Choice Questions:
1. What is the average waiting time in the example given?
A. 5.4
B. 8
C. 27
D. 5

2. What is the purpose of SJF Scheduling?
A. To schedule processes according to their arrival time
B. To schedule processes according to their burst time
C. To schedule processes according to their priority
D. To schedule processes according to their size

3. How does the scheduler choose the process with the lowest burst time?
A. By comparing the arrival times of the processes
B. By comparing the burst times of the processes
C. By comparing the priority of the processes
D. By comparing the size of the processes"
"Prediction of CPU Burst Time for a process in SJF:The SJF algorithm is one of the best scheduling algorithms since it provides the maximum throughput and minimal waiting time but the problem with the algorithm is, the CPU burst time can't be known in advance.We can approximate the CPU burst time for a process. There are various techniques which can be used to assume the CPU Burst time for a process. Our Assumption needs to be accurate in order to utilize the algorithm optimally.There are the following techniques used for the assumption of CPU burst time for a process. 1. Static Techniques:Process Size:We can predict the Burst Time of the process from its size. If we have two processes T_OLD and T_New and the actual burst time of the old process is known as 20 secs and the size of the process is 20 KB. We know that the size of P_NEW is 21 KB. Then the probability of P_New having the similar burst time as 20 secs is maximum.Hence, in this technique, we actually predict the burst time of a new process according to the burst time of an old process of similar size as of new process.Process Type:We can also predict the burst time of the process according to its type. A Process can be of various types defined as follows.A Process can be an Operating system process like schedulers, compilers, program managers and many more system processes. Their burst time is generally lower for example, 3 to 5 units of time.The Processes initiated by the users are called user processes. There can be three types of processes as follows.The Interactive processes are the one which interact with the user time to time or Execution of which totally depends upon the User inputs for example various games are such processes. There burst time needs to be lower since they don't need CPU for a large amount of time, they mainly depend upon the user's interactivity with the process hence they are mainly IO bound processes.Foreground processes are the processes which are used by the user to perform their needs such as MS office, Editors, utility software etc. These types of processes have a bit higher burst time since they are a perfect mix of CPU and IO bound processes.Background processes supports the execution of other processes. They work in hidden mode. For example, key logger is the process which records the keys pressed by the user and activities of the user on the system. They are mainly CPU bound processes and needs CPU for a higher amount of time. 2. Dynamic Techniques:Simple Averaging:In simple averaging, there are given list of n processes P(i).......P(n). Let T(i) denotes the burst time of the process P(i). Let τ(n) denotes the predicted burst time of Pth process. Then according to the simple averaging, the predicted burst time of process n+1 will be calculated as,Where, 0<=i<=n and ∑ T(i) is the summation of actual burst time of all the processes available till now.Exponential Averaging or Aging:Let, Tn be the actual burst time of nth process.τ(n) be the predicted burst time for nth process then the CPU burst time for the next process (n+1) will be calculated as,Where, α is the smoothing. Its value lies between 0 and 1.",os prediction of cpu burst time for a process in sjf,"

Easy Questions:
1. What is the SJF algorithm?
2. What is the purpose of the CPU burst time?
3. What is the purpose of the Static Techniques?

Medium Questions:
1. What is the difference between Static and Dynamic Techniques?
2. What is the formula for calculating the predicted burst time for the next process using Simple Averaging?
3. What is the formula for calculating the predicted burst time for the next process using Exponential Averaging?

Hard Questions:
1. What is the range of the smoothing factor in Exponential Averaging?
2. How can the CPU burst time of a process be accurately predicted?
3. What are the various techniques used to approximate the CPU burst time for a process?

Multiple Choice Questions:
1. What is the purpose of the CPU burst time?
a. To predict the burst time of a process
b. To measure the throughput of a process
c. To measure the waiting time of a process
d. To measure the size of a process

2. What is the range of the smoothing factor in Exponential Averaging?
a. 0-1
b. 0-10
c. 0-100
d. 0-1000

3. What are the various techniques used to approximate the CPU burst time for a process?
a. Static Techniques
b. Dynamic Techniques
c. Simple Averaging
d. All of the above"
"Shortest Remaining Time First (SRTF) Scheduling Algorithm:This Algorithm is the preemptive version of SJF scheduling. In SRTF, the execution of the process can be stopped after certain amount of time. At the arrival of every process, the short term scheduler schedules the process with the least remaining burst time among the list of available processes and the running process.Once all the processes are available in the ready queue, No preemption will be done and the algorithm will work as SJF scheduling. The context of the process is saved in the Process Control Block when the process is removed from the execution and the next process is scheduled. This PCB is accessed on the next execution of this process. Example:In this Example, there are five jobs P1, P2, P3, P4, P5 and P6. Their arrival time and burst time are given below in the table.                    Avg Waiting Time = 24/6The Gantt chart is prepared according to the arrival and burst time given in the table.Once all the processes arrive, No preemption is done and the algorithm will work as SJF.",os srtf scheduling algorithm,"

Easy Questions:
1. What is the Shortest Remaining Time First (SRTF) Scheduling Algorithm?
2. What is the purpose of the Process Control Block?
3. How is the Gantt chart prepared?

Medium Questions:
1. What is the difference between SRTF and SJF scheduling?
2. How is the average waiting time calculated?
3. What is the purpose of preemption in SRTF?

Hard Questions:
1. How can the SRTF algorithm be optimized?
2. How can the context switching overhead be minimized in SRTF?
3. How can the SRTF algorithm be implemented in a distributed system?

Multiple Choice Questions:
1. What is the purpose of preemption in SRTF?
A. To save the context of the process
B. To schedule the process with the least remaining burst time
C. To stop the execution of the process after certain amount of time
D. To reduce the context switching overhead

2. How is the Gantt chart prepared?
A. According to the arrival and burst time of the processes
B. According to the priority of the processes
C. According to the average waiting time
D. According to the context of the process

3. How is the average waiting time calculated?
A. By adding the arrival and burst time of the processes
B. By dividing the total waiting time by the number of processes
C. By subtracting the arrival time from the burst time of the processes
D. By multiplying the arrival and burst time of the processes"
"SRTF GATE 2011 Example:If we talk about scheduling algorithm from the GATE point of view, they generally ask simple numerical questions about finding the average waiting time and Turnaround Time. Let's discuss the question asked in GATE 2011 on SRTF.Q. Given the arrival time and burst time of 3 jobs in the table below. Calculate the Average waiting time of the system.:There are three jobs P1, P2 and P3. P1 arrives at time unit 0; it will be scheduled first for the time until the next process arrives. P2 arrives at 1 unit of time. Its burst time is 4 units which is least among the jobs in the queue. Hence it will be scheduled next.At time 2, P3 will arrive with burst time 9. Since remaining burst time of P2 is 3 units which are least among the available jobs. Hence the processor will continue its execution till its completion. Because all the jobs have been arrived so no preemption will be done now and all the jobs will be executed till the completion according to SJF.                    Avg Waiting Time = (4+0+11)/3 = 5 units",os srtf gate 2011 example,"

Easy Questions:
1. What is the arrival time of P1?
2. What is the burst time of P2?
3. What is the total number of jobs in the queue?

Medium Questions:
1. What is the turnaround time of P2?
2. What is the remaining burst time of P2 when P3 arrives?
3. What is the scheduling algorithm used in this example?

Hard Questions:
1. What is the total time taken to complete all the jobs?
2. What is the average turnaround time of the system?
3. What is the total time taken to complete P3?

Multiple Choice Questions:
1. What is the average waiting time of the system?
A. 4 units
B. 5 units
C. 6 units
D. 7 units

2. What is the burst time of P3?
A. 4 units
B. 5 units
C. 6 units
D. 9 units

3. What is the arrival time of P2?
A. 0 units
B. 1 unit
C. 2 units
D. 4 units"
"Round Robin Scheduling Algorithm:In this tutorial, we are going to learn about the most efficient CPU Process Scheduling Algorithm named Round Robin CPU Process Scheduling. This algorithm is very special because it is going to remove all the Flaws which we have detected in the previous CPU Process Scheduling Algorithms.There is a lot of popularity for this Round Robin CPU Scheduling is because Round Robin works only in Pre Emptive state. This makes it very reliable.Important Abbreviations:Round Robin CPU Scheduling:Round Robin CPU Scheduling is the most important CPU Scheduling Algorithm which is ever used in the history of CPU Scheduling Algorithms. Round Robin CPU Scheduling uses Time Quantum (TQ). The Time Quantum is something which is removed from the Burst Time and lets the chunk of process to be completed.Time Sharing is the main emphasis of the algorithm. Each step of this algorithm is carried out cyclically. The system defines a specific time slice, known as a time quantum.First, the processes which are eligible to enter the ready queue enter the ready queue. After entering the first process in Ready Queue is executed for a Time Quantum chunk of time. After execution is complete, the process is removed from the ready queue. Even now the process requires some time to complete its execution, then the process is added to Ready Queue.The Ready Queue does not hold processes which already present in the Ready Queue. The Ready Queue is designed in such a manner that it does not hold non unique processes. By holding same processes Redundancy of the processes increases.After, the process execution is complete, the Ready Queue does not take the completed process for holding.Advantages:The Advantages of Round Robin CPU Scheduling are:Disadvantages:The Disadvantages of Round Robin CPU Scheduling are:Examples:Assume Time Quantum TQ = 5Ready Queue:Gantt chart:Average Completion TimeAverage Waiting TimeAverage Turn Around Time",os round robin scheduling algorithm,"

Easy Questions:
1. What is Round Robin Scheduling Algorithm?
2. What is Time Quantum?
3. What is the main emphasis of the algorithm?

Medium Questions:
1. What are the advantages of Round Robin CPU Scheduling?
2. What are the disadvantages of Round Robin CPU Scheduling?
3. How does the Ready Queue work?

Hard Questions:
1. How does the system define a specific time slice?
2. How does the Ready Queue prevent redundancy of processes?
3. How is the Gantt chart used in Round Robin CPU Scheduling?

Multiple Choice Questions:
1. What is the main emphasis of the Round Robin CPU Scheduling algorithm?
A. Time Sharing
B. Preemptive State
C. Ready Queue
D. Time Quantum

2. What is the purpose of the Time Quantum?
A. To remove the Burst Time
B. To add processes to the Ready Queue
C. To prevent redundancy of processes
D. To define a specific time slice

3. What is the purpose of the Ready Queue?
A. To define a specific time slice
B. To add processes to the Ready Queue
C. To prevent redundancy of processes
D. To remove the Burst Time"
"RR Scheduling Example:In the following example, there are six processes named as P1, P2, P3, P4, P5 and P6. Their arrival time and burst time are given below in the table. The time quantum of the system is 4 units.According to the algorithm, we have to maintain the ready queue and the Gantt chart. The structure of both the data structures will be changed after every scheduling.Ready Queue::Initially, at time 0, process P1 arrives which will be scheduled for the time slice 4 units. Hence in the ready queue, there will be only one process P1 at starting with CPU burst time 5 units.GANTT chart:The P1 will be executed for 4 units first.Ready Queue:Meanwhile the execution of P1, four more processes P2, P3, P4 and P5 arrives in the ready queue. P1 has not completed yet, it needs another 1 unit of time hence it will also be added back to the ready queue. GANTT chart:After P1, P2 will be executed for 4 units of time which is shown in the Gantt chart.Ready Queue:During the execution of P2, one more process P6 is arrived in the ready queue. Since P2 has not completed yet hence, P2 will also be added back to the ready queue with the remaining burst time 2 units.GANTT chart:After P1 and P2, P3 will get executed for 3 units of time since its CPU burst time is only 3 seconds.Ready Queue:Since P3 has been completed, hence it will be terminated and not be added to the ready queue. The next process will be executed is P4.GANTT chart:After, P1, P2 and P3, P4 will get executed. Its burst time is only 1 unit which is lesser then the time quantum hence it will be completed.Ready Queue:The next process in the ready queue is P5 with 5 units of burst time. Since P4 is completed hence it will not be added back to the queue.GANTT chart:P5 will be executed for the whole time slice because it requires 5 units of burst time which is higher than the time slice.Ready Queue:P5 has not been completed yet; it will be added back to the queue with the remaining burst time of 1 unit.GANTT Chart:The process P1 will be given the next turn to complete its execution. Since it only requires 1 unit of burst time hence it will be completed.Ready Queue:P1 is completed and will not be added back to the ready queue. The next process P6 requires only 4 units of burst time and it will be executed next.GANTT chart:P6 will be executed for 4 units of time till completion.Ready Queue:Since P6 is completed, hence it will not be added again to the queue. There are only two processes present in the ready queue. The Next process P2 requires only 2 units of time.GANTT Chart:P2 will get executed again, since it only requires only 2 units of time hence this will be completed.Ready Queue:Now, the only available process in the queue is P5 which requires 1 unit of burst time. Since the time slice is of 4 units hence it will be completed in the next burst.GANTT chart:P5 will get executed till completion.The completion time, Turnaround time and waiting time will be calculated as shown in the table below.As, we know,                  Avg Waiting Time = (12+16+6+8+15+11)/6 = 76/6 units",os round robin scheduling example," = 12.67 units

Easy Questions:
1. What is the time quantum of the system?
2. How many processes are there in the example?
3. What is the average waiting time?

Medium Questions:
1. What is the structure of the ready queue after the execution of P3?
2. What is the turnaround time of P4?
3. What is the burst time of P5?

Hard Questions:
1. What is the total time taken to complete all the processes?
2. How many processes are added back to the ready queue after the execution of P2?
3. What is the completion time of P6?

Multiple Choice Questions:
1. What is the burst time of P1?
A. 4 units
B. 5 units
C. 6 units
D. 7 units

2. What is the turnaround time of P3?
A. 3 units
B. 4 units
C. 5 units
D. 6 units

3. How many processes are present in the ready queue after the execution of P6?
A. 0
B. 1
C. 2
D. 3"
"Highest Response Ratio Next (HRRN) Scheduling:Highest Response Ratio Next (HRNN) is one of the most optimal scheduling algorithms. This is a non-preemptive algorithm in which, the scheduling is done on the basis of an extra parameter called Response Ratio. A Response Ratio is calculated for each of the available jobs and the Job with the highest response ratio is given priority over the others. Response Ratio is calculated by the given formula. Where,If we look at the formula, we will notice that the job with the shorter burst time will be given priority but it is also including an extra factor called waiting time. Since,Hence,",os hrrn scheduling," this algorithm is considered to be one of the most optimal scheduling algorithms.

Easy Questions:
1. What is HRRN Scheduling?
2. What is the Response Ratio?
3. What is the formula for calculating Response Ratio?

Medium Questions:
1. What is the difference between HRRN and preemptive scheduling?
2. What is the advantage of HRRN Scheduling?
3. How does HRRN Scheduling prioritize jobs?

Hard Questions:
1. How does HRRN Scheduling handle jobs with the same Response Ratio?
2. How does HRRN Scheduling handle jobs with the same burst time?
3. How does HRRN Scheduling handle jobs with the same waiting time?

Multiple Choice Questions:
1. What is the main advantage of HRRN Scheduling?
A. It is a preemptive scheduling algorithm
B. It is a non-preemptive scheduling algorithm
C. It is an optimal scheduling algorithm
D. It is a fast scheduling algorithm

2. What is the formula for calculating Response Ratio?
A. Response Ratio = Burst Time / Waiting Time
B. Response Ratio = Waiting Time / Burst Time
C. Response Ratio = Burst Time + Waiting Time
D. Response Ratio = Waiting Time - Burst Time

3. How does HRRN Scheduling prioritize jobs?
A. By burst time
B. By waiting time
C. By Response Ratio
D. By priority"
"HRNN Example:In the following example, there are 5 processes given. Their arrival time and Burst Time are given in the table.At time 0, The Process P0 arrives with the CPU burst time of 3 units. Since it is the only process arrived till now hence this will get scheduled immediately.P0 is executed for 3 units, meanwhile, only one process P1 arrives at time 3. This will get scheduled immediately since the OS doesn't have a choice.P1 is executed for 5 units. Meanwhile, all the processes get available. We have to calculate the Response Ratio for all the remaining jobs.Since, the Response ratio of P3 is higher hence P3 will be scheduled first.P3 is scheduled for 1 unit. The next available processes are P2 and P4. Let's calculate their Response ratio.The response ratio of P2 is higher hence P2 will be scheduled.Now, the only available process is P4 with the burst time of 2 units, since there is no other process available hence this will be scheduled.                     Average Waiting Time = 13/5",os hrrn example," = 2.6

Easy Questions:
1. What is the average waiting time?
2. How many processes are given in the example?
3. What is the CPU burst time of Process P0?

Medium Questions:
1. What is the Response Ratio of Process P3?
2. What is the Response Ratio of Process P2?
3. What is the Burst Time of Process P4?

Hard Questions:
1. What is the total time taken to execute all the processes?
2. What is the total number of processes that have arrived before P3?
3. What is the total number of processes that have arrived before P4?

Multiple Choice Questions:
1. What is the Response Ratio of Process P2?
a. 0.5
b. 0.75
c. 1.0
d. 1.5

2. What is the Burst Time of Process P4?
a. 1 unit
b. 2 units
c. 3 units
d. 4 units

3. What is the total time taken to execute all the processes?
a. 8 units
b. 9 units
c. 10 units
d. 11 units"
"Priority Scheduling Algorithm in OS (Operating System):In Priority scheduling, there is a priority number assigned to each process. In some systems, the lower the number, the higher the priority. While, in the others, the higher the number, the higher will be the priority. The Process with the higher priority among the available processes is given the CPU. There are two types of priority scheduling algorithm exists. One is Preemptive priority scheduling while the other is Non Preemptive Priority scheduling.The priority number assigned to each of the process may or may not vary. If the priority number doesn't change itself throughout the process, it is called static priority, while if it keeps changing itself at the regular intervals, it is called dynamic priority.",os priority scheduling,"

Easy Questions:
1. What is Priority Scheduling Algorithm?
2. What are the two types of Priority Scheduling Algorithm?
3. What is the difference between static and dynamic priority?

Medium Questions:
1. What is the purpose of assigning priority numbers to processes?
2. How does the CPU decide which process to execute?
3. What are the advantages of using Priority Scheduling Algorithm?

Hard Questions:
1. How does the Priority Scheduling Algorithm handle processes with same priority?
2. How does the Priority Scheduling Algorithm handle processes with different priority?
3. How does the Priority Scheduling Algorithm handle processes with changing priority?

Multiple Choice Questions:
1. What is the purpose of assigning priority numbers to processes?
A. To decide which process to execute
B. To determine the order of execution
C. To assign resources to processes
D. To determine the priority of processes

2. What is the difference between static and dynamic priority?
A. Static priority is fixed while dynamic priority changes
B. Static priority changes while dynamic priority is fixed
C. Static priority is higher than dynamic priority
D. Dynamic priority is higher than static priority

3. How does the CPU decide which process to execute?
A. By the priority number assigned to each process
B. By the amount of resources assigned to each process
C. By the order of execution
D. By the amount of time each process has been waiting"
"Non Preemptive Priority Scheduling:In the Non Preemptive Priority scheduling, The Processes are scheduled according to the priority number assigned to them. Once the process gets scheduled, it will run till the completion. Generally, the lower the priority number, the higher is the priority of the process. The people might get confused with the priority numbers, hence in the GATE, there clearly mention which one is the highest priority and which one is the lowest one.Example:In the Example, there are 7 processes P1, P2, P3, P4, P5, P6 and P7. Their priorities, Arrival Time and burst time are given in the table.We can prepare the Gantt chart according to the Non Preemptive priority scheduling.The Process P1 arrives at time 0 with the burst time of 3 units and the priority number 2. Since No other process has arrived till now hence the OS will schedule it immediately.Meanwhile the execution of P1, two more Processes P2 and P3 are arrived. Since the priority of P3 is 3 hence the CPU will execute P3 over P2. Meanwhile the execution of P3, All the processes get available in the ready queue. The Process with the lowest priority number will be given the priority. Since P6 has priority number assigned as 4 hence it will be executed just after P3.After P6, P4 has the least priority number among the available processes; it will get executed for the whole burst time. Since all the jobs are available in the ready queue hence All the Jobs will get executed according to their priorities. If two jobs have similar priority number assigned to them, the one with the least arrival time will be executed. From the GANTT Chart prepared, we can determine the completion time of every process. The turnaround time, waiting time and response time will be determined.                       Avg Waiting Time = (0+11+2+7+12+2+18)/7 = 52/7 units",os non preemptive priority scheduling,"

Easy Questions:
1. What is Non Preemptive Priority Scheduling?
2. What is the purpose of assigning priority numbers to processes?
3. What is the average waiting time in the example given?

Medium Questions:
1. How is the Gantt chart prepared according to the Non Preemptive Priority Scheduling?
2. What is the turnaround time, waiting time and response time?
3. How is the process with the lowest priority number chosen?

Hard Questions:
1. How does the CPU decide which process to execute when two processes have the same priority number?
2. How does the CPU decide which process to execute when two processes have different priority numbers?
3. How does the CPU decide which process to execute when multiple processes arrive at the same time?

Multiple Choice Questions:
1. What is the average waiting time in the example given?
a. 0 units
b. 11 units
c. 2 units
d. 52/7 units

2. How is the process with the lowest priority number chosen?
a. By the CPU
b. By the OS
c. By the user
d. By the GANTT Chart

3. How does the CPU decide which process to execute when two processes have the same priority number?
a. By the CPU
b. By the OS
c. By the user
d. By the arrival time"
"Preemptive Priority Scheduling:In Preemptive Priority Scheduling, at the time of arrival of a process in the ready queue, its Priority is compared with the priority of the other processes present in the ready queue as well as with the one which is being executed by the CPU at that point of time. The One with the highest priority among all the available processes will be given the CPU next. The difference between preemptive priority scheduling and non preemptive priority scheduling is that, in the preemptive priority scheduling, the job which is being executed can be stopped at the arrival of a higher priority job. Once all the jobs get available in the ready queue, the algorithm will behave as non-preemptive priority scheduling, which means the job scheduled will run till the completion and no preemption will be done.Example:There are 7 processes P1, P2, P3, P4, P5, P6 and P7 given. Their respective priorities, Arrival Times and Burst times are given in the table below.GANTT chart Preparation:At time 0, P1 arrives with the burst time of 1 units and priority 2. Since no other process is available hence this will be scheduled till next job arrives or its completion (whichever is lesser).At time 1, P2 arrives. P1 has completed its execution and no other process is available at this time hence the Operating system has to schedule it regardless of the priority assigned to it.The Next process P3 arrives at time unit 2, the priority of P3 is higher to P2. Hence the execution of P2 will be stopped and P3 will be scheduled on the CPU.During the execution of P3, three more processes P4, P5 and P6 becomes available. Since, all these three have the priority lower to the process in execution so PS can't preempt the process. P3 will complete its execution and then P5 will be scheduled with the priority highest among the available processes.Meanwhile the execution of P5, all the processes got available in the ready queue. At this point, the algorithm will start behaving as Non Preemptive Priority Scheduling. Hence now, once all the processes get available in the ready queue, the OS just took the process with the highest priority and execute that process till completion. In this case, P4 will be scheduled and will be executed till the completion.Since P4 is completed, the other process with the highest priority available in the ready queue is P2. Hence P2 will be scheduled next.P2 is given the CPU till the completion. Since its remaining burst time is 6 units hence P7 will be scheduled after this.The only remaining process is P6 with the least priority, the Operating System has no choice unless of executing it. This will be executed at the last.The Completion Time of each process is determined with the help of GANTT chart. The turnaround time and the waiting time can be calculated by the following formula.                    Avg Waiting Time = (0+14+0+7+1+25+16)/7 = 63/7 = 9 units",os preemptive priority scheduling,"

Easy Questions:
1. What is Preemptive Priority Scheduling?
2. What is the difference between preemptive priority scheduling and non preemptive priority scheduling?
3. What is the formula for calculating the average waiting time?

Medium Questions:
1. How is the completion time of each process determined?
2. What is the turnaround time?
3. What is the priority of the process P3?

Hard Questions:
1. How does the algorithm behave when all the processes are available in the ready queue?
2. How does the Operating System decide which process to execute next?
3. What is the total number of processes given in the example?

Multiple Choice Questions:
1. What is the priority of the process P3?
A. 1
B. 2
C. 3
D. 4

2. How is the completion time of each process determined?
A. By GANTT chart
B. By the formula
C. By the priority
D. By the arrival time

3. What is the formula for calculating the average waiting time?
A. (0+14+0+7+1+25+16)/7
B. (0+14+0+7+1+25+16)/6
C. (0+14+0+7+1+25+16)/8
D. (0+14+0+7+1+25+16)/9"
"SRTF with Processes contains CPU and IO Time:Till now, we were considering the CPU bound jobs only. However, the process might need some IO operation or some resource to complete its execution. In this Example, we are considering, the IO bound processes. In the Example, there are four jobs with process ID P1, P2, P3 and P4 are available. Their Arrival Time, and the CPU Burst time are given in the table below. GANTT Chart Preparation:At time 0, the process P1 and P2 arrives. Since the algorithm we are using is SRTF hence, the process with the shortest burst time will be scheduled on the CPU. In this case, it is P2.From time 0 to time 1, P2 will be in running state.P2 also needs some IO time in order to complete its execution. After 1 unit of execution, P2 will change its state from running to waiting. The processor becomes free to execute other jobs. Since No other process is available at this point of time other than P1 so P1 will get executed.The following diagram illustrates the processes and states at Time 1. The process P2 went to waiting state and the CPU becomes idol at this time. From time 1 to 3, since P2 is being in waiting state, and no other process is available in ready queue, hence the only available process P1 will be executed in this period of time.At time 3, the process P3 arrived with the total CPU burst time of 5 units. Since the remaining burst time of P1 is lesser then P3 hence CPU will continue its execution.Hence, P1 will remain in the running state from time 3 to time 4.Since P1 is an IO bound process. At time unit 4, it will change its state from running to waiting. Processor becomes free for the execution of other jobs. Since P2 also becomes available at time 4 because it has completed the IO operation and now it needs another 1 unit of CPU burst time. P3 is also available and requires 5 units of total CPU burst time.The process with the least remaining CPU burst time among the available processes will get executed. In our case, such process is P2 which requires 1 unit of burst time hence it will be given the CPU.At time 5, P2 is finished. P1 is still in waiting state. At this point of time, the only available process is P3, hence it will be given the CPU.From Time 5 to time 6, P3 will be in the running state; meanwhile, P1 will still be in waiting state.At time 6, the Process P4 arrives in the ready queue. The P1 has also done with the IO and becomes available for the execution. P3 is not yet finished and still needs another 2 unit of CPU burst time.From time 6 to time 8, the reaming CPU burst time of Process P3 is least among the available processes, hence P3 will be given the CPU.P3 needs some IO operation in order to complete its execution. At time 8, P3 will change its state from running to waiting. The CPU becomes free to execute the other processes. Process P4 and P1 are available out of which, the process with the least remaining burst time will get executed.From time 8 to time 9, the process P1 will get executed.At time 9, the IO of process P3 is finished and it will now be available in the ready state along with P4 which is already waiting there for its turn. In order to complete its execution, it needs another 2 unit of burst time. P1 is in running state at this point of the time while no process is present in the waiting state.from time 9 to 10 , the process P1 will get executed since its remaining CPU burst time is lesser then the processes P4 and P3 available in the ready queue.At time 10, execution of P1 is finished, and now the CPU becomes idol. The process with the lesser CPU burst time among the ready processes will get the CPU turn.From time 10 to 12, the process P3 will get executed till its completion because of the fact that its remaining CPU burst time is the between the two available processes. It needs 2 units of more CPU burst time, since No other process will be arrived in the ready state hence No preemption will be done and it will be executed till the completion.At time 12, the process P3 will get completed, since there is only one process P4 available in the ready state hence P4 will be given the CPU. P4 needs 5 units of CPU burst time before IO, hence it will be executed till time 17 (for 5 units) and then it will change its state from running to waiting.At time 17, the Process P4 changes its state from running to waiting. Since this is the only process in the system hence the CPU will remain idol until P4 becomes available again.At time 21, P4 will be done with the IO operation and becomes available in the ready state.From time 21, the process P4 will get scheduled. Since No other process is in ready queue hence the processor don't have any choice. It will be executed till completion.Final Gantt chart::               Average waiting Time = (5+3+4+10)/4 = 22/4 units",os srtf with processes contains cpu and io time," = 5.5 units

Easy Questions:
1. What algorithm is being used in the example?
2. What is the total CPU burst time of process P3?
3. What is the average waiting time?

Medium Questions:
1. What is the state of the processor at time 0?
2. What is the state of the processor at time 4?
3. What is the state of the processor at time 10?

Hard Questions:
1. What is the total CPU burst time of process P1?
2. What is the total CPU burst time of process P4?
3. What is the total CPU burst time of process P2?

Multiple Choice Questions:
1. At time 0, what is the state of the processor?
A. Running
B. Waiting
C. Idol
D. None of the above

2. At time 4, what is the state of the processor?
A. Running
B. Waiting
C. Idol
D. None of the above

3. At time 10, what is the state of the processor?
A. Running
B. Waiting
C. Idol
D. None of the above"
"Process Synchronization in OS (Operating System):When two or more process cooperates with each other, their order of execution must be preserved otherwise there can be conflicts in their execution and inappropriate outputs can be produced. A cooperative process is the one which can affect the execution of other process or can be affected by the execution of other process. Such processes need to be synchronized so that their order of execution can be guaranteed. The procedure involved in preserving the appropriate order of execution of cooperative processes is known as Process Synchronization. There are various synchronization mechanisms that are used to synchronize the processes. Race Condition:A Race Condition typically occurs when two or more threads try to read, write and possibly make the decisions based on the memory that they are accessing concurrently.Critical Section:The regions of a program that try to access shared resources and may cause race conditions are called critical section. To avoid race condition among the processes, we need to assure that only one process at a time can execute within the critical section. ",os process synchronization introduction,"

Easy Questions:
1. What is process synchronization?
2. What is a cooperative process?
3. What is a race condition?

Medium Questions:
1. What are the various synchronization mechanisms used to synchronize processes?
2. What is a critical section?
3. How can race conditions be avoided?

Hard Questions:
1. What are the different types of synchronization techniques used in operating systems?
2. How can the critical section problem be solved?
3. What are the different methods of process synchronization?

Multiple Choice Questions:
1. What is the procedure involved in preserving the appropriate order of execution of cooperative processes?
A. Process Synchronization
B. Race Condition
C. Critical Section
D. Synchronization Mechanisms

Answer: A. Process Synchronization"
"Critical Section Problem in OS (Operating System):Critical Section is the part of a program which tries to access shared resources. That resource may be any resource in a computer like a memory location, Data structure, CPU or any IO device.The critical section cannot be executed by more than one process at the same time; operating system faces the difficulties in allowing and disallowing the processes from entering the critical section.The critical section problem is used to design a set of protocols which can ensure that the Race condition among the processes will never arise. In order to synchronize the cooperative processes, our main task is to solve the critical section problem. We need to provide a solution in such a way that the following conditions can be satisfied.Requirements of Synchronization mechanisms:Primary:Our solution must provide mutual exclusion. By Mutual Exclusion, we mean that if one process is executing inside critical section then the other process must not enter in the critical section.Progress means that if one process doesn't need to execute into critical section then it should not stop other processes to get into the critical section.Secondary:We should be able to predict the waiting time for every process to get into the critical section. The process must not be endlessly waiting for getting into the critical section.Our mechanism must be architectural natural. It means that if our solution is working fine on one architecture then it should also run on the other ones as well.",os critical section problem,"

Easy Questions:
1. What is a Critical Section?
2. What is the purpose of Critical Section Problem?
3. What are the requirements of synchronization mechanisms?

Medium Questions:
1. What is the Race Condition?
2. How can we ensure that Race Condition will never arise?
3. What is the main task to synchronize the cooperative processes?

Hard Questions:
1. How can we provide mutual exclusion in our solution?
2. How can we predict the waiting time for every process to get into the Critical Section?
3. How can we make sure that our solution is architectural natural?

Multiple Choice Questions:
1. What is the purpose of Critical Section Problem?
a. To access shared resources
b. To design a set of protocols
c. To synchronize the cooperative processes
d. To provide mutual exclusion

Answer: b. To design a set of protocols

2. What is the main task to synchronize the cooperative processes?
a. To access shared resources
b. To design a set of protocols
c. To solve the Critical Section Problem
d. To provide mutual exclusion

Answer: c. To solve the Critical Section Problem

3. How can we make sure that our solution is architectural natural?
a. By providing mutual exclusion
b. By predicting the waiting time
c. By ensuring that Race Condition will never arise
d. By running the solution on different architectures

Answer: d. By running the solution on different architectures"
"Lock Variable:This is the simplest synchronization mechanism. This is a Software Mechanism implemented in User mode. This is a busy waiting solution which can be used for more than two processes. In this mechanism, a Lock variable lockis used. Two values of lock can be possible, either 0 or 1. Lock value 0 means that the critical section is vacant while the lock value 1 means that it is occupied. A process which wants to get into the critical section first checks the value of the lock variable. If it is 0 then it sets the value of lock as 1 and enters into the critical section, otherwise it waits. The pseudo code of the mechanism looks like following.If we look at the Pseudo Code, we find that there are three sections in the code. Entry Section, Critical Section and the exit section. Initially the value of lock variable is 0. The process which needs to get into the critical section, enters into the entry section and checks the condition provided in the while loop.The process will wait infinitely until the value of lock is 1 (that is implied by while loop). Since, at the very first time critical section is vacant hence the process will enter the critical section by setting the lock variable as 1.When the process exits from the critical section, then in the exit section, it reassigns the value of lock as 0. Every Synchronization mechanism is judged on the basis of four conditions.Out of the four parameters, Mutual Exclusion and Progress must be provided by any solution. Let?s analyze this mechanism on the basis of the above mentioned conditions.Mutual Exclusion:The lock variable mechanism doesn't provide Mutual Exclusion in some of the cases. This can be better described by looking at the pseudo code by the Operating System point of view I.E. Assembly code of the program. Let's convert the Code into the assembly language.Let us consider that we have two processes P1 and P2. The process P1 wants to execute its critical section. P1 gets into the entry section. Since the value of lock is 0 hence P1 changes its value from 0 to 1 and enters into the critical section.Meanwhile, P1 is preempted by the CPU and P2 gets scheduled. Now there is no other process in the critical section and the value of lock variable is 0. P2 also wants to execute its critical section. It enters into the critical section by setting the lock variable to 1.Now, CPU changes P1's state from waiting to running. P1 is yet to finish its critical section. P1 has already checked the value of lock variable and remembers that its value was 0 when it previously checked it. Hence, it also enters into the critical section without checking the updated value of lock variable. Now, we got two processes in the critical section. According to the condition of mutual exclusion, morethan one process in the critical section must not be present at the same time. Hence, the lock variable mechanism doesn't guarantee the mutual exclusion. The problem with the lock variable mechanism is that, at the same time, more than one process can see the vacant tag and more than one process can enter in the critical section. Hence, the lock variable doesn't provide the mutual exclusion that's why it cannot be used in general. Since, this method is failed at the basic step; hence, there is no need to talk about the other conditions to be fulfilled.",os lock variable mechanism,"

Easy Questions:
1. What is a Lock Variable?
2. What is the value of the Lock Variable when the Critical Section is vacant?
3. What is the purpose of the Entry Section in the Pseudo Code?

Medium Questions:
1. What are the four conditions used to judge a Synchronization Mechanism?
2. What is the problem with the Lock Variable Mechanism?
3. What is the assembly code of the program?

Hard Questions:
1. How does the Lock Variable Mechanism fail to provide Mutual Exclusion?
2. What is the difference between the Lock Variable Mechanism and other Synchronization Mechanisms?
3. How can the Lock Variable Mechanism be improved to provide Mutual Exclusion?

Multiple Choice Questions:
1. What is the value of the Lock Variable when the Critical Section is vacant?
A. 0
B. 1
C. 2
D. 3

2. What is the problem with the Lock Variable Mechanism?
A. It does not provide Mutual Exclusion
B. It does not provide Progress
C. It does not provide Atomicity
D. It does not provide Fairness

3. What is the assembly code of the program?
A. Lock Variable
B. Critical Section
C. Entry Section
D. Exit Section"
"Test Set Lock Mechanism:Modification in the assembly code:In lock variable mechanism, Sometimes Process reads the old value of lock variable and enters the critical section. Due to this reason, more than one process might get into critical section. However, the code shown in the part one of the following section can be replaced with the code shown in the part two. This doesn't affect the algorithm but, by doing this, we can manage to provide the mutual exclusion to some extent but not completely.In the updated version of code, the value of Lock is loaded into the local register R0 and then value of lock is set to 1.However, in step 3, the previous value of lock (that is now stored into R0) is compared with 0. if this is 0 then the process will simply enter into the critical section otherwise will wait by executing continuously in the loop.The benefit of setting the lock immediately to 1 by the process itself is that, now the process which enters into the critical section carries the updated value of lock variable that is 1.In the case when it gets preempted and scheduled again then also it will not enter the critical section regardless of the current value of the lock variable as it already knows what the updated value of lock variable is.TSL Instruction:However, the solution provided in the above segment provides mutual exclusion to some extent but it doesn't make sure that the mutual exclusion will always be there. There is a possibility of having more than one process in the critical section. What if the process gets preempted just after executing the first instruction of the assembly code written in section 2? In that case, it will carry the old value of lock variable with it and it will enter into the critical section regardless of knowing the current value of lock variable. This may make the two processes present in the critical section at the same time.To get rid of this problem, we have to make sure that the preemption must not take place just after loading the previous value of lock variable and before setting it to 1. The problem can be solved if we can be able to merge the first two instructions. In order to address the problem, the operating system provides a special instruction called Test Set Lock (TSL) instruction which simply loads the value of lock variable into the local register R0 and sets it to 1 simultaneously The process which executes the TSL first will enter into the critical section and no other process after that can enter until the first process comes out. No process can execute the critical section even in the case of preemption of the first process.The assembly code of the solution will look like following.Let's examine TSL on the basis of the four conditions.Mutual Exclusion is guaranteed in TSL mechanism since a process can never be preempted just before setting the lock variable. Only one process can see the lock variable as 0 at a particular time and that's why, the mutual exclusion is guaranteed. According to the definition of the progress, a process which doesn't want to enter in the critical section should not stop other processes to get into it. In TSL mechanism, a process will execute the TSL instruction only when it wants to get into the critical section. The value of the lock will always be 0 if no process doesn't want to enter into the critical section hence the progress is always guaranteed in TSL.Bounded Waiting is not guaranteed in TSL. Some process might not get a chance for so long. We cannot predict for a process that it will definitely get a chance to enter in critical section after a certain time. TSL doesn't provide Architectural Neutrality. It depends on the hardware platform. The TSL instruction is provided by the operating system. Some platforms might not provide that. Hence it is not Architectural natural. ",os tsl mechanism,"

Easy Questions:
1. What is the purpose of the Test Set Lock (TSL) instruction?
2. What is the benefit of setting the lock immediately to 1 by the process itself?
3. What is the definition of progress?

Medium Questions:
1. What is the difference between the code shown in part one and part two of the following section?
2. What is the problem that the TSL instruction is designed to address?
3. How does the TSL mechanism guarantee mutual exclusion?

Hard Questions:
1. What are the implications of the TSL instruction on the four conditions of mutual exclusion?
2. How does the TSL instruction provide bounded waiting?
3. How does the TSL instruction provide architectural neutrality?

Multiple Choice Questions:
1. What is the purpose of the Test Set Lock (TSL) instruction?
A. To provide mutual exclusion
B. To provide bounded waiting
C. To provide architectural neutrality
D. To provide a way to enter the critical section

Answer: D. To provide a way to enter the critical section

2. What is the benefit of setting the lock immediately to 1 by the process itself?
A. It ensures mutual exclusion
B. It ensures bounded waiting
C. It ensures architectural neutrality
D. It ensures the process carries the updated value of the lock variable

Answer: D. It ensures the process carries the updated value of the lock variable

3. How does the TSL mechanism guarantee mutual exclusion?
A. By ensuring the process carries the updated value of the lock variable
B. By ensuring the preemption does not take place just after loading the previous value of lock variable
C. By ensuring only one process can see the lock variable as 0 at a particular time
D. By ensuring the process will execute the TSL instruction only when it wants to get into the critical section

Answer: C. By ensuring only one process can see the lock variable as 0 at a particular time"
"Priority Inversion:In TSL mechanism, there can be a problem of priority inversion. Let?s say that there are two cooperative processes, P1 and P2. The priority of P1 is 2 while that of P2 is 1. P1 arrives earlier and got scheduled by the CPU. Since it is a cooperative process and wants to execute in the critical section hence it will enter in the critical section by setting the lock variable to 1. Now, P2 arrives in the ready queue. The priority of P2 is higher than P1 hence according to priority scheduling, P2 is scheduled and P1 got preempted. P2 is also a cooperative process and wants to execute inside the critical section. Although, P1 got preempted but it the value of lock variable will be shown as 1 since P1 is not completed and it is yet to finish its critical section. P1 needs to finish the critical section but according to the scheduling algorithm, CPU is with P2. P2 wants to execute in the critical section, but according to the synchronization mechanism, critical section is with P1.This is a kind of lock where each of the process neither executes nor completes. Such kind of lock is called Spin Lock. This is different from deadlock since they are not in blocked state. One is in ready state and the other is in running state, but neither of the two is being executed.",os priority inversion in tsl,"

Easy Questions:
1. What is Priority Inversion?
2. What is the priority of P1?
3. What is the priority of P2?

Medium Questions:
1. What is the problem with the TSL mechanism?
2. What is the difference between Priority Inversion and Deadlock?
3. What is a Spin Lock?

Hard Questions:
1. How does Priority Inversion occur in the TSL mechanism?
2. How does the CPU decide which process to schedule?
3. How does the synchronization mechanism work in the TSL mechanism?

Multiple Choice Questions:
1. What is the priority of P1?
A. 1
B. 2
C. 3
D. 4

2. What is the problem with the TSL mechanism?
A. Deadlock
B. Priority Inversion
C. Spin Lock
D. Synchronization

3. What is the difference between Priority Inversion and Deadlock?
A. Priority Inversion is when two processes are in blocked state, while Deadlock is when two processes are in ready state.
B. Priority Inversion is when two processes are in ready state, while Deadlock is when two processes are in blocked state.
C. Priority Inversion is when two processes are in running state, while Deadlock is when two processes are in ready state.
D. Priority Inversion is when two processes are in running state, while Deadlock is when two processes are in blocked state."
"Turn Variable or Strict Alternation Approach:Turn Variable or Strict Alternation Approach is the software mechanism implemented at user mode. It is a busy waiting solution which can be implemented only for two processes. In this approach, A turn variable is used which is actually a lock.This approach can only be used for only two processes. In general, let the two processes be Pi and Pj. They share a variable called turn variable. The pseudo code of the program can be given as following. For Process PiFor Process PjThe actual problem of the lock variable approach was the fact that the process was entering in the critical section only when the lock variable is 1. More than one process could see the lock variable as 1 at the same time hence the mutual exclusion was not guaranteed there. This problem is addressed in the turn variable approach. Now, A process can enter in the critical section only in the case when the value of the turn variable equal to the PID of the process. There are only two values possible for turn variable, i or j. if its value is not i then it will definitely be j or vice versa. In the entry section, in general, the process Pi will not enter in the critical section until its value is j or the process Pj will not enter in the critical section until its value is i. Initially, two processes Pi and Pj are available and want to execute into critical section.The turn variable is equal to i hence Pi will get the chance to enter into the critical section. The value of Pi remains I until Pi finishes critical section.Pi finishes its critical section and assigns j to turn variable. Pj will get the chance to enter into the critical section. The value of turn remains j until Pj finishes its critical section.Analysis of Strict Alternation approach:Let's analyze Strict Alternation approach on the basis of four requirements.Mutual Exclusion:The strict alternation approach provides mutual exclusion in every case. This procedure works only for two processes. The pseudo code is different for both of the processes. The process will only enter when it sees that the turn variable is equal to its Process ID otherwise not Hence No process can enter in the critical section regardless of its turn. Progress:Progress is not guaranteed in this mechanism. If Pi doesn't want to get enter into the critical section on its turn then Pj got blocked for infinite time. Pj has to wait for so long for its turn since the turn variable will remain 0 until Pi assigns it to j. Portability:The solution provides portability. It is a pure software mechanism implemented at user mode and doesn't need any special instruction from the Operating System. ",os turn variable or strict alternation approach,"

Easy Questions:
1. What is the Turn Variable or Strict Alternation Approach?
2. What is the problem addressed in the Turn Variable approach?
3. How many processes can use the Turn Variable approach?

Medium Questions:
1. What is the pseudo code of the program for the Turn Variable approach?
2. How does the Turn Variable approach guarantee mutual exclusion?
3. What is the analysis of the Strict Alternation approach?

Hard Questions:
1. What are the advantages of the Turn Variable approach over the Lock Variable approach?
2. How does the Turn Variable approach ensure progress?
3. What are the limitations of the Turn Variable approach?

Multiple Choice Questions:
1. What is the problem addressed in the Turn Variable approach?
A. Mutual Exclusion
B. Progress
C. Portability
D. Race Condition

Answer: D. Race Condition

2. How does the Turn Variable approach guarantee mutual exclusion?
A. By using a lock variable
B. By using a turn variable
C. By using a PID
D. By using a special instruction from the Operating System

Answer: B. By using a turn variable

3. What are the limitations of the Turn Variable approach?
A. It can only be used for two processes
B. It does not guarantee progress
C. It requires a special instruction from the Operating System
D. It does not provide mutual exclusion

Answer: B. It does not guarantee progress"
"Interested Variable Mechanism:We have to make sure that the progress must be provided by our synchronization mechanism. In the turn variable mechanism, progress was not provided due to the fact that the process which doesn't want to enter in the critical section does not consider the other interested process as well.The other process will also have to wait regardless of the fact that there is no one inside the critical section. If the operating system can make use of an extra variable along with the turn variable then this problem can be solved and our problem can provide progress to most of the extent.Interested variable mechanism makes use of an extra Boolean variable to make sure that the progress is provided.For Process PiFor Process PjIn this mechanism, an extra variable interested is used. This is a Boolean variable used to store the interest of the processes to get enter inside the critical section. A process which wants to enter in the critical section first checks in the entry section whether the other process is interested to get inside. The process will wait for the time until the other process is interested. In exit section, the process makes the value of its interest variable false so that the other process can get into the critical section. The table shows the possible values of interest variable of both the processes and the process which get the chance in the scenario.Let's analyze the mechanism on the basis of the requirements.Mutual Exclusion:In interested variable mechanism, if one process is interested in getting into the CPU then the other process will wait until it becomes uninterested. Therefore, more than one process can never be present in the critical section at the same time hence the mechanism guarantees mutual exclusion.Progress:In this mechanism, if a process is not interested in getting into the critical section then it will not stop the other process from getting into the critical section. Therefore the progress will definitely be provided by this method.Bounded Waiting:To analyze bounded waiting, let us consider two processes Pi and Pj, are the cooperative processes wants to execute in the critical section. The instructions executed by the processes are shown below in relative manner.Initially, the interest variable of both the processes is false. The process Pi shows the interest to get inside the critical section. It sets its Interest Variable to true and check whether the Pj is also interested or not. Since the other process's interest variable is false hence Pi will get enter into the critical section. Meanwhile, the process Pi is preempted and Pj is scheduled. Pj is a cooperative process and therefore, it also wants to enter in the critical section. It shows its interest by setting the interest variable to true.It also checks whether the other process is also interested or not. We should notice that Pi is preempted but its interested variable is true that means it needs to further execute in the critical section. Therefore Pj will not get the chance and gets stuck in the while loop. Meanwhile, CPU changes Pi's state from blocked to running. Pi is yet to finish its critical section hence it finishes the critical section and makes an exit by setting the interest variable to False.Now, a case can be possible when Pi again wants to enter in the critical section and set its interested variable to true and checks whether the interested variable of Pj is true. Here, Pj's interest variable is True hence Pi will get stuck in the while loop and waits for Pj become uninterested.Since, Pj still stuck in the while loop waiting for the Pi' interested variable to become false. Therefore, both the processes are waiting for each other and none of them is getting into the critical section. This is a condition of deadlock and bounded waiting can never be provided in the case of deadlock.Therefore, we can say that the interested variable mechanism doesn't guarantee deadlock.Architectural Neutrality:The mechanism is a complete software mechanism executed in the user mode therefore it guarantees portability or architectural neutrality.",os interested variable mechanism,"

Easy Questions:
1. What is the purpose of the Interested Variable Mechanism?
2. What is the value of the Interest Variable initially?
3. What is the purpose of the extra Boolean variable?

Medium Questions:
1. What is the difference between the Turn Variable Mechanism and the Interested Variable Mechanism?
2. How does the Interested Variable Mechanism guarantee Mutual Exclusion?
3. How does the Interested Variable Mechanism guarantee Progress?

Hard Questions:
1. How does the Interested Variable Mechanism guarantee Bounded Waiting?
2. What is the difference between the Interested Variable Mechanism and the Deadlock Mechanism?
3. How does the Interested Variable Mechanism guarantee Architectural Neutrality?

Multiple Choice Questions:
1. What is the purpose of the Interested Variable Mechanism?
a. To provide progress
b. To provide mutual exclusion
c. To provide bounded waiting
d. To provide architectural neutrality

2. How does the Interested Variable Mechanism guarantee Mutual Exclusion?
a. By making sure that only one process can enter the critical section at a time
b. By making sure that all processes can enter the critical section at the same time
c. By making sure that all processes can enter the critical section at different times
d. By making sure that no process can enter the critical section

3. How does the Interested Variable Mechanism guarantee Bounded Waiting?
a. By making sure that only one process can enter the critical section at a time
b. By making sure that all processes can enter the critical section at the same time
c. By making sure that all processes can enter the critical section at different times
d. By making sure that no process can enter the critical section"
"Paterson Solution:This is a software mechanism implemented at user mode. It is a busy waiting solution can be implemented for only two processes. It uses two variables that are turn variable and interested variable.The Code of the solution is given belowTill now, each of our solution is affected by one or the other problem. However, the Peterson solution provides you all the necessary requirements such as Mutual Exclusion, Progress, Bounded Waiting and Portability.Analysis of Peterson Solution:This is a two process solution. Let us consider two cooperative processes P1 and P2. The entry section and exit section are shown below. Initially, the value of interested variables and turn variable is 0. Initially process P1 arrives and wants to enter into the critical section. It sets its interested variable to True (instruction line 3) and also sets turn to 1 (line number 4). Since the condition given in line number 5 is completely satisfied by P1 therefore it will enter in the critical section. Meanwhile, Process P1 got preempted and process P2 got scheduled. P2 also wants to enter in the critical section and executes instructions 1, 2, 3 and 4 of entry section. On instruction 5, it got stuck since it doesn't satisfy the condition (value of other interested variable is still true). Therefore it gets into the busy waiting. P1 again got scheduled and finish the critical section by executing the instruction no. 6 (setting interested variable to false). Now if P2 checks then it are going to satisfy the condition since other process's interested variable becomes false. P2 will also get enter the critical section.Any of the process may enter in the critical section for multiple numbers of times. Hence the procedure occurs in the cyclic order.Mutual Exclusion:The method provides mutual exclusion for sure. In entry section, the while condition involves the criteria for two variables therefore a process cannot enter in the critical section until the other process is interested and the process is the last one to update turn variable.Progress:An uninterested process will never stop the other interested process from entering in the critical section. If the other process is also interested then the process will wait.Bounded waiting:The interested variable mechanism failed because it was not providing bounded waiting. However, in Peterson solution, A deadlock can never happen because the process which first sets the turn variable will enter in the critical section for sure. Therefore, if a process is preempted after executing line number 4 of the entry section then it will definitely get into the critical section in its next chance. Portability:This is the complete software solution and therefore it is portable on every hardware. ",os paterson solution,"

Easy Questions:
1. What is the Peterson Solution?
2. What are the two variables used in the Peterson Solution?
3. What are the four requirements of the Peterson Solution?

Medium Questions:
1. What is the code of the Peterson Solution?
2. What is the analysis of the Peterson Solution?
3. What is the cyclic order of the Peterson Solution?

Hard Questions:
1. What is the mechanism of the Peterson Solution?
2. How does the Peterson Solution provide mutual exclusion?
3. How does the Peterson Solution provide bounded waiting?

Multiple Choice Questions:
1. What is the initial value of the two variables used in the Peterson Solution?
A. True and False
B. 0 and 1
C. 1 and 0
D. False and True

2. What is the portability of the Peterson Solution?
A. Hardware
B. Software
C. Both
D. Neither

3. What is the mechanism of the Peterson Solution?
A. Busy Waiting
B. Mutual Exclusion
C. Progress
D. Bounded Waiting"
"Synchronization Mechanism without busy waiting:All the solutions we have seen till now were intended to provide mutual exclusion with busy waiting. However, busy waiting is not the optimal allocation of resources because it keeps CPU busy all the time in checking the while loops condition continuously although the process is waiting for the critical section to become available.All the synchronization mechanism with busy waiting are also suffering from the priority inversion problem that is there is always a possibility of spin lock whenever there is a process with the higher priority has to wait outside the critical section since the mechanism intends to execute the lower priority process in the critical section. However these problems need a proper solution without busy waiting and priority inversion.",os synchronization mechanism without busy waiting,"

Easy Questions:
1. What is the purpose of synchronization mechanisms?
2. What is busy waiting?
3. What is the priority inversion problem?

Medium Questions:
1. What are the drawbacks of synchronization mechanisms with busy waiting?
2. How can the priority inversion problem be solved?
3. What are the alternatives to synchronization mechanisms with busy waiting?

Hard Questions:
1. How can synchronization mechanisms be optimized to reduce resource utilization?
2. What are the trade-offs between synchronization mechanisms with and without busy waiting?
3. How can synchronization mechanisms be designed to prevent deadlock?

Multiple Choice Questions:
1. What is the purpose of synchronization mechanisms?
A. To provide mutual exclusion
B. To reduce resource utilization
C. To prevent deadlock
D. All of the above

2. What is the priority inversion problem?
A. A problem where a process with a higher priority has to wait outside the critical section
B. A problem where a process with a lower priority has to wait outside the critical section
C. A problem where a process with a higher priority has to wait inside the critical section
D. A problem where a process with a lower priority has to wait inside the critical section

3. What are the alternatives to synchronization mechanisms with busy waiting?
A. Spin locks
B. Semaphores
C. Monitors
D. All of the above"
"Sleep and Wake:(Producer Consumer problem):Let's examine the basic model that is sleep and wake. Assume that we have two system calls as sleep and wake. The process which calls sleep will get blocked while the process which calls will get waked up.There is a popular example called producer consumer problem which is the most popular problem simulating sleep and wake mechanism.The concept of sleep and wake is very simple. If the critical section is not empty then the process will go and sleep. It will be waked up by the other process which is currently executing inside the critical section so that the process can get inside the critical section.In producer consumer problem, let us say there are two processes, one process writes something while the other process reads that. The process which is writing something is called producer while the process which is reading is called consumer. In order to read and write, both of them are usinga buffer. The code that simulates the sleep and wake mechanism in terms of providing the solution to producer consumer problem is shown below. The producer produces the item and inserts it into the buffer. The value of the global variable count got increased at each insertion. If the buffer is filled completely and no slot is available then the producer will sleep, otherwise it keep inserting. On the consumer's end, the value of count got decreased by 1 at each consumption. If the buffer is empty at any point of time then the consumer will sleep otherwise, it keeps consuming the items and decreasing the value of count by 1.The consumer will be waked up by the producer if there is at least 1 item available in the buffer which is to be consumed. The producer will be waked up by the consumer if there is at least one slot available in the buffer so that the producer can write that.Well, the problem arises in the case when the consumer got preempted just before it was about to sleep. Now the consumer is neither sleeping nor consuming. Since the producer is not aware of the fact that consumer is not actually sleeping therefore it keep waking the consumer while the consumer is not responding since it is not sleeping. This leads to the wastage of system calls. When the consumer get scheduled again, it will sleep because it was about to sleep when it was preempted.The producer keep writing in the buffer and it got filled after some time. The producer will also sleep at that time keeping in the mind that the consumer will wake him up when there is a slot available in the buffer. The consumer is also sleeping and not aware with the fact that the producer will wake him up.This is a kind of deadlock where neither producer nor consumer is active and waiting for each other to wake them up. This is a serious problem which needs to be addressed.Using a flag bit to get rid of this problem:A flag bit can be used in order to get rid of this problem. The producer can set the bit when it calls wake-up on the first time. When the consumer got scheduled, it checks the bit. The consumer will now get to know that the producer tried to wake him and therefore it will not sleep and get into the ready state to consume whatever produced by the producer. This solution works for only one pair of producer and consumer, what if there are n producers and n consumers. In that case, there is a need to maintain an integer which can record how many wake-up calls have been made and how many consumers need not sleep. This integer variable is called semaphore. We will discuss more about semaphore later in detail. ",os sleep and wake,"

Easy Questions:
1. What are the two system calls used in sleep and wake?
2. What is the most popular problem simulating sleep and wake mechanism?
3. What is the purpose of the flag bit?

Medium Questions:
1. What is the purpose of the global variable count in the producer consumer problem?
2. What is the problem that arises when the consumer gets preempted?
3. How can the problem of deadlock be solved?

Hard Questions:
1. How can the solution of one pair of producer and consumer be extended to n producers and n consumers?
2. What is the purpose of the integer variable called semaphore?
3. How does the producer and consumer know when to sleep and wake up?

Multiple Choice Questions:
1. What is the purpose of the global variable count in the producer consumer problem?
A. To keep track of the number of items in the buffer
B. To keep track of the number of wake-up calls
C. To keep track of the number of consumers
D. To keep track of the number of producers

2. What is the problem that arises when the consumer gets preempted?
A. The producer will not be aware of the fact that the consumer is not sleeping
B. The consumer will not be aware of the fact that the producer is not sleeping
C. The producer will not be able to write in the buffer
D. The consumer will not be able to read from the buffer

3. How can the problem of deadlock be solved?
A. By using a flag bit
B. By using a semaphore
C. By using a global variable
D. By using a buffer"
"Introduction to Semaphore in Operating Systems (OS):In thi tutorial, we are about to learn about the most important topic called Semaphores. There is a 100% surety that there is going to be a question about the topic named Semaphores in Viva, Interviews, Exams, and even Placement Exams. So, please understand this topic with utmost care and preference.In this topic, we are going to learn about Semaphore definition, Types of Semaphores, Operations of Semaphores, Advantages and Disadvantages in Semaphores, Process of Solving Classical Synchronization Problems using Semaphores and the usage of these types of Semaphores in solving these Classical Synchronization Problems.Semaphore Definition:Semaphore is a Hardware Solution. This Hardware solution is written or given to critical section problem.What is a Critical Section Problem?:The Critical Section Problem is a Code Snippet. This code snippet contains a few variables. These variables can be accessed by a few processes. There is a condition for these processes.The condition is that only one process can only enter the critical section. Remaining Processes which are interested to enter the critical section have to wait for the process to complete its work and then enter the critical section.Critical Section Representation:Problems in Critical Section Problems:There may be a state where one or more processes try to enter the critical state. After multiple processes enter the Critical Section, the second process try to access variable which already accessed by the first process.Explanation:Suppose there is a variable which is also known as shared variable. Let us define that shared variable.Here, x is the shared variable.Process 1Process 2If the process is accessed the x shared variable one after other, then we are going to be in a good position.If Process 1 is alone executed, then the value of x is denoted as x = 30;The shared variable x changes to 30 from 10If Process 2 is alone executed, then the value of x is denoted as x = -10;The shared variable x changes to -10 from 30If both the processes occur at the same time, then the compiler would be in a confusion to choose which variable value i.e. -10 or 30. This state faced by the variable x is Data Inconsistency. These problems can also be solved by Hardware LocksTo, prevent such kind of problems can also be solved by Hardware solutions named Semaphores.Semaphores:The Semaphore is just a normal integer. The Semaphore cannot be negative. The least value for a Semaphore is zero (0). The Maximum value of a Semaphore can be anything. The Semaphores usually have two operations. The two operations have the capability to decide the values of the semaphores.The two Semaphore Operations are:Wait Semaphore Operation:The Wait Operation is used for deciding the condition for the process to enter the critical state or wait for execution of process. Here, the wait operation has many different names. The different names are:The Wait Operation works on the basis of Semaphore or Mutex Value.Here, if the Semaphore value is greater than zero or positive then the Process can enter the Critical Section Area.If the Semaphore value is equal to zero then the Process has to wait for the Process to exit the Critical Section Area.This function is only present until the process enters the critical state. If the Processes enters the critical state, then the P Function or Wait Operation has no job to do.If the Process exits the Critical Section we have to reduce the value of SemaphoreBasic Algorithm of P Function or Wait OperationSignal Semaphore Operation:The Signal Semaphore Operation is used to update the value of Semaphore. The Semaphore value is updated when the new processes are ready to enter the Critical Section.The Signal Operation is also known as:We know that the semaphore value is decreased by one in the wait operation when the process left the critical state. So, to counter balance the decreased number 1 we use signal operation which increments the semaphore value. This induces the critical section to receive more and more processes into it.The most important part is that this Signal Operation or V Function is executed only when the process comes out of the critical section. The value of semaphore cannot be incremented before the exit of process from the critical sectionBasic Algorithm of V Function or Signal OperationTypes of Semaphores:There are two types of Semaphores.They are:1. Binary Semaphore:Here, there are only two values of Semaphore in Binary Semaphore Concept. The two values are 1 and 0.If the Value of Binary Semaphore is 1, then the process has the capability to enter the critical section area. If the value of Binary Semaphore is 0 then the process does not have the capability to enter the critical section area.2. Counting Semaphore:Here, there are two sets of values of Semaphore in Counting Semaphore Concept. The two types of values are values greater than and equal to one and other type is value equal to zero.If the Value of Binary Semaphore is greater than or equal to 1, then the process has the capability to enter the critical section area. If the value of Binary Semaphore is 0 then the process does not have the capability to enter the critical section area.This is the brief description about the Binary and Counting Semaphores. You will learn still more about them in next articles.Advantages of a Semaphore:Disadvantages of a Semaphore:Now, let us solve the Classical Synchronization Problems using the concept of Semaphore.Solving Classical Synchronization Problems using Semaphore Concept:1) Solving Readers - Writers Problem using Semaphore:Definition:First of all let us all know about the Readers - Writers Problem.In Readers - Writers Problems there are two Actors for the problem. They are Reader and Writer. The Reader - Writer Problem tries to access or change the value of the Shared Variable. Due, to this parallel execution of accessing and changing operation data faults take place. Due, to this expected outputs are not the actual outputs of the problem. This is the problem of Readers - WritersThe duties of Readers and Writers are different from each other.The Reader duty is to read the value of the Shared Variable.The Writer duty is to change the value of the Shared Variable.Now, let us solve this problem with the help of a Semaphore.Required DataFirst of all let us consider there is Critical Section which contains a shared variable. Let us consider that there are two processes. The first process is the one which always tries to access the shared variable. The second process is the one which always tries to change the value of the variable.SolutionNow, let us understand the solution to the Readers and Writers Problem usingNow, our task is to perform the operations of both Readers and Writers using Semaphores without any possibility of occurrence of Deadlock.Solving Readers - Writers Problem with the help of Binary SemaphoreWe know that the Binary Semaphore allows the process to enter the critical section if the value of Binary Semaphore is 1.Now, we are going to allow the writer to work or execute its task if the value of Binary Semaphore value is 1. This allows that the data changes are error free.We can allow the reader to work or execute its task if the value of Binary Semaphore is 1 or 0. This is because the Reader just reads the value of Shared Variable. But according to the rules laid by the Semaphore. We can only enter the critical section if the value of Semaphore is greater than or equal to 1.So, we can allow the reader if the value of semaphore 1.Algorithm for Solving Readers - Writers Problem with the help of Binary SemaphoreSolving Readers - Writers Problem with the help of Counting SemaphoreWe know that the Counting Semaphore allows the process to enter the critical section if the value of Counting Semaphore is greater than or equal to 1.Now, we are going to allow the writer to work or execute its task if the value of Counting Semaphore value is greater than or equal to 1. This allows that the data changes are error free.We can allow the reader to work or execute its task if the value of Counting Semaphore is greater than or equal to 1 or Counting Semaphore is equal to 0. This is because the Reader just reads the value of Shared Variable. But according to the rules laid by the Semaphore. We can only enter the critical section if the value of Counting Semaphore is greater than or equal to 1.So, we can allow the reader if the value of Counting Semaphore is greater than or equal to 1.Algorithm for Solving Readers - Writers Problem with the help of Binary Semaphore2. Solving Bound Buffer Problem using the concept of Semaphores:Definition:The Producer-Consumer problem is another name for Bound Buffer Problem. In this issue, there are n slots in a buffer, and each slot may hold one data unit. Producer and Consumer are the two operations that are using the buffer. Both the producer and the consumer attempt to enter and delete data.Solution:Now, we are going to solve the problem faced in the Bound Buffer or Producer Consumer Problem.We already know that the duty of the Producer is to enter data in which ever area possible.The duty of Consumer is to remove the data produced by the Producer or already present data where ever possible.Now, let us understand how these two Producers and Consumers are going to work along with the concepts of Binary and Counting Semaphore.Solving Bound Buffer or Producer Consumer Problem with the help of Binary SemaphoreWe know that the Binary Semaphore allows the process to enter the critical section if the value of Binary Semaphore is 1.Now, we are going to allow the writer to work or execute its task if the value of Binary Semaphore value is 1. This allows that the data changes are error free.Now, let us assume that the Producer is creating a few shared variables inside the critical section.Now, let us allow the process named Producer and produce new value inside the critical section after entering the Critical Section. The entry is accepted only if the value of Binary Semaphore is equal to 1.Now, let us allow the process named Consumer and delete the processes created after entering the Critical Section. The entry is accepted only if the value of Binary Semaphore is equal to 1.Now, it is our duty to prevent the Producer and Consumer gets executed at the same time. So, we give the user or the computer an option to choose which operation to be performed at the time of Entering the Entry Section of the Critical SectionSolving Bound Buffer or Producer Consumer Problem with the help of Counting SemaphoreWe know that the Binary Semaphore allows the process to enter the critical section if the value of Counting Semaphore is 1 or greater than 1.Now, we are going to allow the writer to work or execute its task if the value of Counting Semaphore is 1 or greater than 1.. This allows that the data changes are error free.Now, let us assume that the Producer is creating a few shared variables inside the critical section.Now, let us allow the process named Producer and produce new value inside the critical section after entering the Critical Section. The entry is accepted only if the value of Counting Semaphore is equal to or greater than 1.Now, let us allow the process named Consumer and delete the processes created after entering the Critical Section. The entry is accepted only if the value of Counting Semaphore is 1 or greater than 1.Now, it is our duty to prevent the Producer and Consumer gets executed at the same time. So, we give the user or the computer an option to choose which operation to be performed at the time of Entering the Entry Section of the Critical Section3. Solving Dining Philosopher's Problem using the concept of Semaphores:Definition:The dining philosopher's dilemma, also known as the classical synchronization issue, has five philosophers seated around a circular table who must alternate between thinking and eating. At the center of the table sits a bowl of noodles and five chopsticks, one for each of the philosophers. A philosopher must use both their right and left chopsticks in order to eat. A philosopher can only eat if both of their right and left chopsticks are within reach. If the philosopher's left and right chopsticks are not immediately available, the philosopher sets down one of the chopsticks (either the left or right) and resumes pondering.The Dining Philosopher is a classic synchronization problem because it illustrates a broad class of concurrency control problems.Now, you might have understood how Semaphores are useful in solving the problems created by the Synchronization Process.This is all about Semaphores in Operating Systems.",os semaphore introduction,"

Easy Questions:
1. What is a Semaphore?
2. What are the two operations of a Semaphore?
3. What is the purpose of Semaphores?

Medium Questions:
1. What are the two types of Semaphores?
2. What is the difference between Binary and Counting Semaphores?
3. How can Semaphores be used to solve the Readers-Writers Problem?

Hard Questions:
1. What is the Dining Philosopher's Problem and how can it be solved using Semaphores?
2. What is the Critical Section Problem and how can it be solved using Semaphores?
3. What are the advantages and disadvantages of using Semaphores?

Multiple Choice Questions:
1. What is the purpose of Semaphores?
a. To prevent data inconsistency
b. To prevent deadlock
c. To prevent race conditions
d. To prevent data corruption

2. What are the two types of Semaphores?
a. Binary and Counting
b. Mutex and Binary
c. Mutex and Counting
d. Binary and Mutex

3. What is the Dining Philosopher's Problem?
a. A synchronization problem involving five philosophers
b. A synchronization problem involving four philosophers
c. A synchronization problem involving three philosophers
d. A synchronization problem involving two philosophers"
"Counting Semaphore:There are the scenarios in which more than one processes need to execute in critical section simultaneously. However, counting semaphore can be used when we need to have more than one process in the critical section at the same time. The programming code of semaphore implementation is shown below which includes the structure of semaphore and the logic using which the entry and the exit can be performed in the critical section. In this mechanism, the entry and exit in the critical section are performed on the basis of the value of counting semaphore. The value of counting semaphore at any point of time indicates the maximum number of processes that can enter in the critical section at the same time. A process which wants to enter in the critical section first decrease the semaphore value by 1 and then check whether it gets negative or not. If it gets negative then the process is pushed in the list of blocked processes (i.e. q) otherwise it gets enter in the critical section. When a process exits from the critical section, it increases the counting semaphore by 1 and then checks whether it is negative or zero. If it is negative then that means that at least one process is waiting in the blocked state hence, to ensure bounded waiting, the first process among the list of blocked processes will wake up and gets enter in the critical section. The processes in the blocked list will get waked in the order in which they slept. If the value of counting semaphore is negative then it states the number of processes in the blocked state while if it is positive then it states the number of slots available in the critical section. ",os counting semaphore,"

Easy Questions:
1. What is a counting semaphore?
2. What is the purpose of a counting semaphore?
3. What is the value of the counting semaphore at any point of time?

Medium Questions:
1. What is the programming code of semaphore implementation?
2. How is the entry and exit in the critical section performed?
3. What happens when a process exits from the critical section?

Hard Questions:
1. How does the counting semaphore ensure bounded waiting?
2. How does the value of counting semaphore indicate the number of processes in the blocked state?
3. How does the value of counting semaphore indicate the number of slots available in the critical section?

Multiple Choice Questions:
1. What is the value of counting semaphore at any point of time?
A. The number of processes in the blocked state
B. The number of slots available in the critical section
C. The maximum number of processes that can enter in the critical section at the same time
D. The number of processes waiting in the blocked state

Answer: C. The maximum number of processes that can enter in the critical section at the same time

2. What happens when a process exits from the critical section?
A. The value of counting semaphore is increased by 1
B. The value of counting semaphore is decreased by 1
C. The first process among the list of blocked processes is waked up
D. The processes in the blocked list are waked in the order in which they slept

Answer: A. The value of counting semaphore is increased by 1

3. How does the counting semaphore ensure bounded waiting?
A. By decreasing the semaphore value by 1
B. By increasing the semaphore value by 1
C. By pushing the process in the list of blocked processes
D. By waking up the first process among the list of blocked processes

Answer: D. By waking up the first process among the list of blocked processes"
"Problem on Counting Semaphore:The questions are being asked on counting semaphore in GATE. Generally the questions are very simple that contains only subtraction and addition.The following type questions can be asked in GATE.A Counting Semaphore was initialized to 12. then 10P (wait) and 4V (Signal) operations were computed on this semaphore. What is the result?:Hence, the final value of counting semaphore is 6.",os problem on counting semaphore,"

Easy Questions:
1. What is a counting semaphore?
2. What operations are performed on a counting semaphore?
3. What is the initial value of the counting semaphore?

Medium Questions:
1. How many P and V operations were performed on the counting semaphore?
2. What is the final value of the counting semaphore?
3. What is the difference between a counting semaphore and a binary semaphore?

Hard Questions:
1. What is the mathematical formula used to calculate the final value of the counting semaphore?
2. How can the counting semaphore be used to solve critical section problems?
3. What are the different types of synchronization techniques used in operating systems?

Multiple Choice Questions:
1. What is the final value of the counting semaphore?
a. 12
b. 10
c. 4
d. 6

2. What operations are performed on a counting semaphore?
a. Addition and Subtraction
b. Multiplication and Division
c. Increment and Decrement
d. Logical AND and OR

3. What is the difference between a counting semaphore and a binary semaphore?
a. Counting semaphores can have multiple values while binary semaphores can only have two values
b. Counting semaphores can only have two values while binary semaphores can have multiple values
c. Counting semaphores are used for synchronization while binary semaphores are used for communication
d. Counting semaphores are used for communication while binary semaphores are used for synchronization"
"Binary Semaphore or Mutex:In counting semaphore, Mutual exclusion was not provided because we has the set of processes which required to execute in the critical section simultaneously.However, Binary Semaphore strictly provides mutual exclusion. Here, instead of having more than 1 slots available in the critical section, we can only have at most 1 process in the critical section. The semaphore can have only two values, 0 or 1.Let's see the programming implementation of Binary Semaphore.",os binary semaphore or mutex,"

Easy Questions:
1. What is a Binary Semaphore?
2. What is the purpose of a Binary Semaphore?
3. What are the two values a Binary Semaphore can have?

Medium Questions:
1. What is the difference between a Binary Semaphore and a Counting Semaphore?
2. How does a Binary Semaphore provide Mutual Exclusion?
3. What is the programming implementation of a Binary Semaphore?

Hard Questions:
1. What are the advantages of using a Binary Semaphore over a Counting Semaphore?
2. How can a Binary Semaphore be used to synchronize multiple processes?
3. What are the potential drawbacks of using a Binary Semaphore?

Multiple Choice Questions:
1. What is the purpose of a Binary Semaphore?
A. To provide mutual exclusion
B. To synchronize multiple processes
C. To provide counting capabilities
D. To provide access control

2. What are the two values a Binary Semaphore can have?
A. 0 and 1
B. True and False
C. On and Off
D. High and Low

3. What is the difference between a Binary Semaphore and a Counting Semaphore?
A. A Binary Semaphore provides mutual exclusion while a Counting Semaphore does not
B. A Binary Semaphore can have two values while a Counting Semaphore can have more than two
C. A Binary Semaphore is used for synchronization while a Counting Semaphore is used for access control
D. A Binary Semaphore is used for access control while a Counting Semaphore is used for synchronization"
"What is Deadlock in Operating System (OS)?:Every process needs some resources to complete its execution. However, the resource is granted in a sequential order.A Deadlock is a situation where each of the computer process waits for a resource which is being assigned to some another process. In this situation, none of the process gets executed since the resource it needs, is held by some other process which is also waiting for some other resource to be released.Let us assume that there are three processes P1, P2 and P3. There are three different resources R1, R2 and R3. R1 is assigned to P1, R2 is assigned to P2 and R3 is assigned to P3. After some time, P1 demands for R1 which is being used by P2. P1 halts its execution since it can't complete without R2. P2 also demands for R3 which is being used by P3. P2 also stops its execution because it can't continue without R3. P3 also demands for R1 which is being used by P1 therefore P3 also stops its execution. In this scenario, a cycle is being formed among the three processes. None of the process is progressing and they are all waiting. The computer becomes unresponsive since all the processes got blocked.Difference between Starvation and Deadlock:Necessary conditions for Deadlocks:A resource can only be shared in mutually exclusive manner. It implies, if two process cannot use the same resource at the same time.A process waits for some resources while holding another resource at the same time. The process which once scheduled will be executed till the completion. No other process can be scheduled by the scheduler meanwhile. All the processes must be waiting for the resources in a cyclic manner so that the last process is waiting for the resource which is being held by the first process.",os deadlocks introduction,"

Easy Questions:
1. What is Deadlock in Operating System?
2. What is the difference between Starvation and Deadlock?
3. What are the necessary conditions for Deadlocks?

Medium Questions:
1. What is the cycle formed among the three processes in a Deadlock situation?
2. How does the computer become unresponsive in a Deadlock situation?
3. What is the implication of a resource being shared in mutually exclusive manner?

Hard Questions:
1. How can a Deadlock situation be avoided?
2. What are the different strategies used to detect Deadlocks?
3. How can a Deadlock situation be resolved?

Multiple Choice Questions:
1. What is the implication of a resource being shared in mutually exclusive manner?
A. It implies that two process can use the same resource at the same time.
B. It implies that two process cannot use the same resource at the same time.
C. It implies that two process can use the same resource but not at the same time.
D. It implies that two process can use the same resource but only at the same time.

2. What is the cycle formed among the three processes in a Deadlock situation?
A. A cycle of waiting
B. A cycle of execution
C. A cycle of sharing
D. A cycle of blocking

3. How does the computer become unresponsive in a Deadlock situation?
A. All the processes get blocked.
B. All the processes get executed.
C. All the processes get shared.
D. All the processes get waiting."
"Strategies for handling Deadlock:1. Deadlock Ignorance:Deadlock Ignorance is the most widely used approach among all the mechanism. This is being used by many operating systems mainly for end user uses. In this approach, the Operating system assumes that deadlock never occurs. It simply ignores deadlock. This approach is best suitable for a single end user system where User uses the system only for browsing and all other normal stuff.There is always a tradeoff between Correctness and performance. The operating systems like Windows and Linux mainly focus upon performance. However, the performance of the system decreases if it uses deadlock handling mechanism all the time if deadlock happens 1 out of 100 times then it is completely unnecessary to use the deadlock handling mechanism all the time.In these types of systems, the user has to simply restart the computer in the case of deadlock. Windows and Linux are mainly using this approach. 2. Deadlock prevention :Deadlock happens only when Mutual Exclusion, hold and wait, No preemption and circular wait holds simultaneously. If it is possible to violate one of the four conditions at any time then the deadlock can never occur in the system.The idea behind the approach is very simple that we have to fail one of the four conditions but there can be a big argument on its physical implementation in the system.We will discuss it later in detail. 3. Deadlock avoidance:In deadlock avoidance, the operating system checks whether the system is in safe state or in unsafe state at every step which the operating system performs. The process continues until the system is in safe state. Once the system moves to unsafe state, the OS has to backtrack one step.In simple words, The OS reviews each allocation so that the allocation doesn't cause the deadlock in the system. We will discuss Deadlock avoidance later in detail. 4. Deadlock detection and recovery:This approach let the processes fall in deadlock and then periodically check whether deadlock occur in the system or not. If it occurs then it applies some of the recovery methods to the system to get rid of deadlock. We will discuss deadlock detection and recovery later in more detail since it is a matter of discussion. ",os strategies for handling deadlock,"

Easy Questions:
1. What is Deadlock Ignorance?
2. What is the main focus of Windows and Linux operating systems?
3. What is the main idea behind Deadlock Prevention?

Medium Questions:
1. What are the four conditions that must be met for a deadlock to occur?
2. What is the tradeoff between Correctness and performance?
3. How does the Operating System check for a safe state in Deadlock Avoidance?

Hard Questions:
1. What are the recovery methods used to get rid of deadlock in Deadlock Detection and Recovery?
2. How can the physical implementation of Deadlock Prevention be argued?
3. What are the advantages and disadvantages of Deadlock Ignorance?

Multiple Choice Questions:
1. What is the main focus of Windows and Linux operating systems?
a. Performance
b. Correctness
c. Efficiency
d. Reliability

2. What is the main idea behind Deadlock Prevention?
a. To fail one of the four conditions
b. To periodically check for deadlock
c. To review each allocation
d. To assume that deadlock never occurs

3. What are the four conditions that must be met for a deadlock to occur?
a. Mutual Exclusion, hold and wait, No preemption and circular wait
b. Mutual Exclusion, hold and wait, Preemption and circular wait
c. Mutual Exclusion, wait and hold, No preemption and circular wait
d. Mutual Exclusion, wait and hold, Preemption and circular wait"
"Deadlock Prevention:If we simulate deadlock with a table which is standing on its four legs then we can also simulate four legs with the four conditions which when occurs simultaneously, cause the deadlock. However, if we break one of the legs of the table then the table will fall definitely. The same happens with deadlock, if we can be able to violate one of the four necessary conditions and don't let them occur together then we can prevent the deadlock.Let's see how we can prevent each of the conditions.1. Mutual Exclusion:Mutual section from the resource point of view is the fact that a resource can never be used by more than one process simultaneously which is fair enough but that is the main reason behind the deadlock. If a resource could have been used by more than one process at the same time then the process would have never been waiting for any resource.However, if we can be able to violate resources behaving in the mutually exclusive manner then the deadlock can be prevented.Spooling:For a device like printer, spooling can work. There is a memory associated with the printer which stores jobs from each of the process into it. Later, Printer collects all the jobs and print each one of them according to FCFS. By using this mechanism, the process doesn't have to wait for the printer and it can continue whatever it was doing. Later, it collects the output when it is produced.Although, Spooling can be an effective approach to violate mutual exclusion but it suffers from two kinds of problems.We cannot force a resource to be used by more than one process at the same time since it will not be fair enough and some serious problems may arise in the performance. Therefore, we cannot violate mutual exclusion for a process practically.2. Hold and Wait:Hold and wait condition lies when a process holds a resource and waiting for some other resource to complete its task. Deadlock occurs because there can be more than one process which are holding one resource and waiting for other in the cyclic order. However, we have to find out some mechanism by which a process either doesn't hold any resource or doesn't wait. That means, a process must be assigned all the necessary resources before the execution starts. A process must not wait for any resource once the execution has been started. !(Hold and wait) = !hold or !wait (negation of hold and wait is, either you don't hold or you don't wait)This can be implemented practically if a process declares all the resources initially. However, this sounds very practical but can't be done in the computer system because a process can't determine necessary resources initially. Process is the set of instructions which are executed by the CPU. Each of the instruction may demand multiple resources at the multiple times. The need cannot be fixed by the OS. The problem with the approach is: 3. No Preemption:Deadlock arises due to the fact that a process can't be stopped once it starts. However, if we take the resource away from the process which is causing deadlock then we can prevent deadlock.This is not a good approach at all since if we take a resource away which is being used by the process then all the work which it has done till now can become inconsistent.Consider a printer is being used by any process. If we take the printer away from that process and assign it to some other process then all the data which has been printed can become inconsistent and ineffective and also the fact that the process can't start printing again from where it has left which causes performance inefficiency. 4. Circular Wait:To violate circular wait, we can assign a priority number to each of the resource. A process can't request for a lesser priority resource. This ensures that not a single process can request a resource which is being utilized by some other process and no cycle will be formed. Among all the methods, violating Circular wait is the only approach that can be implemented practically.",os deadlock prevention,"

Easy Questions:
1. What is deadlock prevention?
2. What is mutual exclusion?
3. What is spooling?

Medium Questions:
1. What is the negation of hold and wait?
2. What is the problem with the approach of no preemption?
3. How can circular wait be violated?

Hard Questions:
1. How can mutual exclusion be violated to prevent deadlock?
2. How can a process determine necessary resources initially?
3. What are the implications of taking a resource away from a process which is causing deadlock?

Multiple Choice Questions:
1. What is the main reason behind deadlock?
A. Mutual Exclusion
B. Hold and Wait
C. No Preemption
D. Circular Wait

2. What is the approach to violate circular wait?
A. Assign a priority number to each of the resource
B. Take a resource away from the process which is causing deadlock
C. Declare all the resources initially
D. Store jobs from each of the process into memory

3. What is the problem with the approach of no preemption?
A. Data printed can become inconsistent and ineffective
B. Process can't determine necessary resources initially
C. Process can't start printing again from where it has left
D. All of the above"
"Deadlock avoidance:In deadlock avoidance, the request for any resource will be granted if the resulting state of the system doesn't cause deadlock in the system. The state of the system will continuously be checked for safe and unsafe states. In order to avoid deadlocks, the process must tell OS, the maximum number of resources a process can request to complete its execution.The simplest and most useful approach states that the process should declare the maximum number of resources of each type it may ever need. The Deadlock avoidance algorithm examines the resource allocations so that there can never be a circular wait condition. :Safe and Unsafe StatesThe resource allocation state of a system can be defined by the instances of available and allocated resources, and the maximum instance of the resources demanded by the processes. A state of a system recorded at some random time is shown below.Above tables and vector E, P and A describes the resource allocation state of a system. There are 4 processes and 4 types of the resources in a system. Table 1 shows the instances of each resource assigned to each process.Table 2 shows the instances of the resources, each process still needs. Vector E is the representation of total instances of each resource in the system. Vector P represents the instances of resources that have been assigned to processes. Vector A represents the number of resources that are not in use.A state of the system is called safe if the system can allocate all the resources requested by all the processes without entering into deadlock. If the system cannot fulfill the request of all processes then the state of the system is called unsafe.The key of Deadlock avoidance approach is when the request is made for resources then the request must only be approved in the case if the resulting state is also a safe state. ",os deadlock avoidance,"

Easy Questions:
1. What is deadlock avoidance?
2. What is the simplest approach to deadlock avoidance?
3. What are the two states of a system?

Medium Questions:
1. What are the three vectors used to describe the resource allocation state of a system?
2. What is the key of the deadlock avoidance approach?
3. How does the deadlock avoidance algorithm work?

Hard Questions:
1. How can the system determine if a state is safe or unsafe?
2. How can the system ensure that a circular wait condition is avoided?
3. What is the difference between deadlock avoidance and deadlock prevention?

Multiple Choice Questions:
1. What is the key of the deadlock avoidance approach?
A. Allocating resources
B. Requesting resources
C. Releasing resources
D. Ensuring safe states

Answer: D. Ensuring safe states

2. What are the two states of a system?
A. Safe and Unstable
B. Unsafe and Unstable
C. Safe and Unsafe
D. Unsafe and Stable

Answer: C. Safe and Unsafe

3. What is the simplest approach to deadlock avoidance?
A. Allocating resources
B. Requesting resources
C. Releasing resources
D. Declaring maximum number of resources

Answer: D. Declaring maximum number of resources"
"Resource Allocation Graph:The resource allocation graph is the pictorial representation of the state of a system. As its name suggests, the resource allocation graph is the complete information about all the processes which are holding some resources or waiting for some resources. It also contains the information about all the instances of all the resources whether they are available or being used by the processes.In Resource allocation graph, the process is represented by a Circle while the Resource is represented by a rectangle. Let's see the types of vertices and edges in detail. Vertices are mainly of two types, Resource and process. Each of them will be represented by a different shape. Circle represents process while rectangle represents resource. A resource can have more than one instance. Each instance will be represented by a dot inside the rectangle. Edges in RAG are also of two types, one represents assignment and other represents the wait of a process for a resource. The above image shows each of them.A resource is shown as assigned to a process if the tail of the arrow is attached to an instance to the resource and the head is attached to a process.A process is shown as waiting for a resource if the tail of an arrow is attached to the process while the head is pointing towards the resource.Example:Let'sconsider 3 processes P1, P2 and P3, and two types of resources R1 and R2. The resources are having 1 instance each.According to the graph, R1 is being used by P1, P2 is holding R2 and waiting for R1, P3 is waiting for R1 as well as R2.
The graph is deadlock free since no cycle is being formed in the graph. ",os resource allocation graph,"

Easy Questions:
1. What is a Resource Allocation Graph?
2. What shapes are used to represent processes and resources in a Resource Allocation Graph?
3. What is an example of a deadlock free graph?

Medium Questions:
1. What are the two types of vertices in a Resource Allocation Graph?
2. What are the two types of edges in a Resource Allocation Graph?
3. How is a resource shown as assigned to a process in a Resource Allocation Graph?

Hard Questions:
1. How can a Resource Allocation Graph be used to determine if a system is in a deadlock state?
2. How can a Resource Allocation Graph be used to determine the order in which resources are allocated to processes?
3. What are the implications of a deadlock state in a Resource Allocation Graph?

Multiple Choice Questions:
1. What shapes are used to represent processes and resources in a Resource Allocation Graph?
A. Square and Triangle
B. Circle and Rectangle
C. Triangle and Circle
D. Rectangle and Square

2. How is a resource shown as assigned to a process in a Resource Allocation Graph?
A. The tail of the arrow is attached to the process and the head is pointing towards the resource
B. The tail of the arrow is attached to an instance of the resource and the head is attached to a process
C. The tail of the arrow is attached to the resource and the head is pointing towards the process
D. The tail of the arrow is attached to the process and the head is attached to the resource

3. What are the implications of a deadlock state in a Resource Allocation Graph?
A. The system is unable to allocate resources to processes
B. The system is unable to release resources
C. The system is unable to allocate resources to processes and release resources
D. The system is unable to process requests"
"Deadlock Detection using RAG:If a cycle is being formed in a Resource allocation graph where all the resources have the single instance then the system is deadlocked. In Case of Resource allocation graph with multi-instanced resource types, Cycle is a necessary condition of deadlock but not the sufficient condition.The following example contains three processes P1, P2, P3 and three resources R2, R2, R3. All the resources are having single instances each.If we analyze the graph then we can find out that there is a cycle formed in the graph since the system is satisfying all the four conditions of deadlock.Allocation Matrix:Allocation matrix can be formed by using the Resource allocation graph of a system. In Allocation matrix, an entry will be made for each of the resource assigned. For Example, in the following matrix, en entry is being made in front of P1 and below R3 since R3 is assigned to P1.Request Matrix:In request matrix, an entry will be made for each of the resource requested. As in the following example, P1 needs R1 therefore an entry is being made in front of P1 and below R1.Avial = (0,0,0):Neither we are having any resource available in the system nor a process going to release. Each of the process needs at least single resource to complete therefore they will continuously be holding each one of them. We cannot fulfill the demand of at least one process using the available resources therefore the system is deadlocked as determined earlier when we detected a cycle in the graph.",os deadlock detection using rag,"

Easy Questions:
1. What is the purpose of Resource Allocation Graph?
2. What is the purpose of Allocation Matrix?
3. What is the purpose of Request Matrix?

Medium Questions:
1. What are the conditions for a system to be deadlocked?
2. How can a cycle be detected in a Resource Allocation Graph?
3. What is the difference between single-instanced and multi-instanced resource types?

Hard Questions:
1. How can deadlock be prevented in a system?
2. How can the system be recovered from a deadlock?
3. What are the different algorithms used for deadlock detection?

Multiple Choice Questions:
1. What is the purpose of Resource Allocation Graph?
a. To detect deadlocks
b. To allocate resources
c. To detect cycles
d. To request resources

2. What is the difference between single-instanced and multi-instanced resource types?
a. Single-instanced resources can be used by multiple processes, while multi-instanced resources can only be used by one process
b. Single-instanced resources can only be used by one process, while multi-instanced resources can be used by multiple processes
c. Single-instanced resources are more expensive, while multi-instanced resources are cheaper
d. Single-instanced resources are more reliable, while multi-instanced resources are less reliable

3. How can the system be recovered from a deadlock?
a. By terminating one of the processes
b. By allocating more resources
c. By increasing the number of instances of resources
d. By increasing the number of processes"
"Deadlock Detection and Recovery:In this approach, The OS doesn't apply any mechanism to avoid or prevent the deadlocks. Therefore the system considers that the deadlock will definitely occur. In order to get rid of deadlocks, The OS periodically checks the system for any deadlock. In case, it finds any of the deadlock then the OS will recover the system using some recovery techniques.The main task of the OS is detecting the deadlocks. The OS can detect the deadlocks with the help of Resource allocation graph.In single instanced resource types, if a cycle is being formed in the system then there will definitely be a deadlock. On the other hand, in multiple instanced resource type graph, detecting a cycle is not just enough. We have to apply the safety algorithm on the system by converting the resource allocation graph into the allocation matrix and request matrix. In order to recover the system from deadlocks, either OS considers resources or processes.For Resource:Preempt the resource:We can snatch one of the resources from the owner of the resource (process) and give it to the other process with the expectation that it will complete the execution and will release this resource sooner. Well, choosing a resource which will be snatched is going to be a bit difficult.Rollback to a safe state:System passes through various states to get into the deadlock state. The operating system canrollback the system to the previous safe state. For this purpose, OS needs to implement check pointing at every state.The moment, we get into deadlock, we will rollback all the allocations to get into the previous safe state.For Process:Kill a process:Killing a process can solve our problem but the bigger concern is to decide which process to kill. Generally, Operating system kills a process which has done least amount of work until now.Kill all process:This is not a suggestible approach but can be implemented if the problem becomes very serious. Killing all process will lead to inefficiency in the system because all the processes will execute again from starting.",os deadlock detection and recovery,"

Easy Questions:
1. What is the main task of the OS in deadlock detection?
2. What is the resource allocation graph?
3. What is the purpose of the safety algorithm?

Medium Questions:
1. What are the two techniques used by the OS to recover from deadlock?
2. How does the OS decide which resource to preempt?
3. What is the disadvantage of killing all processes to recover from deadlock?

Hard Questions:
1. How does the OS detect a deadlock in multiple instanced resource type graph?
2. How does the OS rollback the system to a safe state?
3. What is the difference between single instanced resource type graph and multiple instanced resource type graph?

Multiple Choice Questions:
1. What is the main task of the OS in deadlock detection?
A. Preempt the resource
B. Kill a process
C. Detect the deadlocks
D. Rollback to a safe state

Answer: C. Detect the deadlocks

2. How does the OS decide which resource to preempt?
A. By killing a process
B. By killing all processes
C. By snatching one of the resources from the owner of the resource
D. By rolling back all the allocations to get into the previous safe state

Answer: C. By snatching one of the resources from the owner of the resource

3. What is the difference between single instanced resource type graph and multiple instanced resource type graph?
A. In single instanced resource type graph, detecting a cycle is not just enough.
B. In multiple instanced resource type graph, detecting a cycle is just enough.
C. In single instanced resource type graph, detecting a cycle is just enough.
D. In multiple instanced resource type graph, detecting a cycle is not just enough.

Answer: D. In multiple instanced resource type graph, detecting a cycle is not just enough."
"What is Memory?:Computer memory can be defined as a collection of some data represented in the binary format. On the basis of various functions, memory can be classified into various categories. We will discuss each one of them later in detail.A computer device that is capable to store any information or data temporally or permanently, is called storage device.How Data is being stored in a computer system?:In order to understand memory management, we have to make everything clear about how data is being stored in a computer system. Machine understands only binary language that is 0 or 1. Computer converts every data into binary language first and then stores it into the memory.That means if we have a program line written as int α = 10 then the computer converts it into the binary language and then store it into the memory blocks. The representation of inti = 10 is shown below.The binary representation of 10 is 1010. Here, we are considering 32 bit system therefore, the size of int is 2 bytes i.e. 16 bit. 1 memory block stores 1 bit. If we are using signed integer then the most significant bit in the memory array is always a signed bit. Signed bit value 0 represents positive integer while 1 represents negative integer. Here, the range of values that can be stored using the memory array is -32768 to +32767.well, we can enlarge this range by using unsigned int. in that case, the bit which is now storing the sign will also store the bit value and therefore the range will be 0 to 65,535.Need for Multi programming:However, The CPU can directly access the main memory, Registers and cache of the system. The program always executes in main memory. The size of main memory affects degree of Multi programming to most of the extant. If the size of the main memory is larger than CPU can load more processes in the main memory at the same time and therefore will increase degree of Multi programming as well as CPU utilization.Therefore, we can state that the CPU utilization will be increased if the memory size gets increased.",os memory management introduction,"

Easy Questions:
1. What is computer memory?
2. How does a computer store data?
3. What is a storage device?

Medium Questions:
1. What is the binary representation of 10?
2. How does the size of main memory affect degree of multiprogramming?
3. What is the range of values that can be stored using a memory array?

Hard Questions:
1. How does a computer convert data into binary language?
2. What is the difference between signed and unsigned integers?
3. How does the CPU access the main memory, registers and cache of the system?

Multiple Choice Questions:
1. What is the binary representation of 10?
a. 101
b. 0101
c. 1010
d. 0110

2. How does the size of main memory affect degree of multiprogramming?
a. It increases the CPU utilization
b. It decreases the CPU utilization
c. It has no effect on CPU utilization
d. It increases the speed of the CPU

3. What is the range of values that can be stored using a memory array?
a. 0 to 65,535
b. -32768 to +32767
c. -128 to +127
d. 0 to 255"
"Fixed Partitioning:The earliest and one of the simplest technique which can be used to load more than one processes into the main memory is Fixed partitioning or Contiguous memory allocation.In this technique, the main memory is divided into partitions of equal or different sizes. The operating system always resides in the first partition while the other partitions can be used to store user processes. The memory is assigned to the processes in contiguous way. In fixed partitioning,There are various cons of using this technique.1. Internal FragmentationIf the size of the process is lesser then the total size of the partition then some size of the partition get wasted and remain unused. This is wastage of the memory and called internal fragmentation. As shown in the image below, the 4 MB partition is used to load only 3 MB process and the remaining 1 MB got wasted.2. External FragmentationThe total unused space of various partitions cannot be used to load the processes even though there is space available but not in the contiguous form. As shown in the image below, the remaining 1 MB space of each partition cannot be used as a unit to store a 4 MB process. Despite of the fact that the sufficient space is available to load the process, process will not be loaded.3. Limitation on the size of the processIf the process size is larger than the size of maximum sized partition then that process cannot be loaded into the memory. Therefore, a limitation can be imposed on the process size that is it cannot be larger than the size of the largest partition.4. Degree of multiprogramming is lessBy Degree of multi programming, we simply mean the maximum number of processes that can be loaded into the memory at the same time. In fixed partitioning, the degree of multiprogramming is fixed and very less due to the fact that the size of the partition cannot be varied according to the size of processes.",os fixed partitioning,"

Easy Questions:
1. What is fixed partitioning?
2. What is internal fragmentation?
3. What is external fragmentation?

Medium Questions:
1. What is the limitation on the size of the process in fixed partitioning?
2. What is the operating system always stored in?
3. What is the degree of multiprogramming in fixed partitioning?

Hard Questions:
1. What are the advantages and disadvantages of fixed partitioning?
2. How does fixed partitioning compare to other memory allocation techniques?
3. How can the degree of multiprogramming be increased in fixed partitioning?

Multiple Choice Questions:
1. What is the operating system always stored in?
a. The first partition
b. The last partition
c. The middle partition
d. Any partition

2. What is the degree of multiprogramming in fixed partitioning?
a. High
b. Low
c. Variable
d. None of the above

3. What is external fragmentation?
a. Unused space of a partition
b. Space available but not in contiguous form
c. Wastage of memory
d. All of the above"
"Dynamic Partitioning:Dynamic partitioning tries to overcome the problems caused by fixed partitioning. In this technique, the partition size is not declared initially. It is declared at the time of process loading.The first partition is reserved for the operating system. The remaining space is divided into parts. The size of each partition will be equal to the size of the process. The partition size varies according to the need of the process so that the internal fragmentation can be avoided.Advantages of Dynamic Partitioning over fixed partitioning:1. No Internal Fragmentation:Given the fact that the partitions in dynamic partitioning are created according to the need of the process, It is clear that there will not be any internal fragmentation because there will not be any unused remaining space in the partition.2. No Limitation on the size of the process:In Fixed partitioning, the process with the size greater than the size of the largest partition could not be executed due to the lack of sufficient contiguous memory. Here, In Dynamic partitioning, the process size can't be restricted since the partition size is decided according to the process size.3. Degree of multiprogramming is dynamic:Due to the absence of internal fragmentation, there will not be any unused space in the partition hence more processes can be loaded in the memory at the same time.Disadvantages of dynamic partitioning:External Fragmentation:Absence of internal fragmentation doesn't mean that there will not be external fragmentation.Let's consider three processes P1 (1 MB) and P2 (3 MB) and P3 (1 MB) are being loaded in the respective partitions of the main memory. After some time P1 and P3 got completed and their assigned space is freed. Now there are two unused partitions (1 MB and 1 MB) available in the main memory but they cannot be used to load a 2 MB process in the memory since they are not contiguously located.The rule says that the process must be contiguously present in the main memory to get executed. We need to change this rule to avoid external fragmentation.Complex Memory Allocation:In Fixed partitioning, the list of partitions is made once and will never change but in dynamic partitioning, the allocation and deallocation is very complex since the partition size will be varied every time when it is assigned to a new process. OS has to keep track of all the partitions.Due to the fact that the allocation and deallocation are done very frequently in dynamic memory allocation and the partition size will be changed at each time, it is going to be very difficult for OS to manage everything. ",os dynamic partitioning,"

Easy Questions:
1. What is dynamic partitioning?
2. What are the advantages of dynamic partitioning?
3. What is the main disadvantage of dynamic partitioning?

Medium Questions:
1. What is the difference between fixed and dynamic partitioning?
2. How does dynamic partitioning avoid internal fragmentation?
3. What is the rule that must be changed to avoid external fragmentation?

Hard Questions:
1. How does the degree of multiprogramming change in dynamic partitioning?
2. How does the operating system keep track of all the partitions in dynamic partitioning?
3. What are the complexities involved in memory allocation in dynamic partitioning?

Multiple Choice Questions:
1. What is the main disadvantage of dynamic partitioning?
a. Internal Fragmentation
b. External Fragmentation
c. Complex Memory Allocation
d. Limitation on the size of the process

Answer: b. External Fragmentation

2. How does dynamic partitioning avoid internal fragmentation?
a. By changing the rule
b. By allocating the same size of partition for each process
c. By allocating the same size of memory for each process
d. By allocating the same size of partition for each memory

Answer: b. By allocating the same size of partition for each process

3. What is the rule that must be changed to avoid external fragmentation?
a. The process must be contiguously present in the main memory
b. The partition size must be equal to the size of the process
c. The partition size must be equal to the size of the memory
d. The process size must be equal to the size of the partition

Answer: a. The process must be contiguously present in the main memory"
"Compaction:We got to know that the dynamic partitioning suffers from external fragmentation. However, this can cause some serious problems. To avoid compaction, we need to change the rule which says that the process can't be stored in the different places in the memory. We can also use compaction to minimize the probability of external fragmentation. In compaction, all the free partitions are made contiguous and all the loaded partitions are brought together.By applying this technique, we can store the bigger processes in the memory. The free partitions are merged which can now be allocated according to the needs of new processes. This technique is also called defragmentation.As shown in the image above, the process P5, which could not be loaded into the memory due to the lack of contiguous space, can be loaded now in the memory since the free partitions are made contiguous.Problem with Compaction:The efficiency of the system is decreased in the case of compaction due to the fact that all the free spaces will be transferred from several places to a single place.Huge amount of time is invested for this procedure and the CPU will remain idle for all this time. Despite of the fact that the compaction avoids external fragmentation, it makes system inefficient.Let us consider that OS needs 6 NS to copy 1 byte from one place to another.hence, it is proved to some extent that the larger size memory transfer needs some huge amount of time that is in seconds.",os compaction,"

Easy Questions:
1. What is compaction?
2. What is the purpose of compaction?
3. What is the problem with compaction?

Medium Questions:
1. What is the rule that needs to be changed to avoid compaction?
2. What is the technique used to minimize the probability of external fragmentation?
3. How much time is invested for the compaction procedure?

Hard Questions:
1. How does compaction help in storing bigger processes in the memory?
2. How does compaction make the system inefficient?
3. How much time does OS need to copy 1 byte from one place to another?

Multiple Choice Questions:
1. What is the technique used to minimize the probability of external fragmentation?
A. Compaction
B. Defragmentation
C. Partitioning
D. Allocation

Answer: B. Defragmentation

2. How does compaction make the system inefficient?
A. It takes a lot of time
B. It requires a lot of memory
C. It increases the external fragmentation
D. It decreases the efficiency of the system

Answer: D. It decreases the efficiency of the system

3. How much time does OS need to copy 1 byte from one place to another?
A. 1 NS
B. 2 NS
C. 3 NS
D. 6 NS

Answer: D. 6 NS"
"Bit Map for Dynamic Partitioning:The Main concern for dynamic partitioning is keeping track of all the free and allocated partitions. However, the Operating system uses following data structures for this task.Bit Map is the least famous data structure to store the details. In this scheme, the main memory is divided into the collection of allocation units. One or more allocation units may be allocated to a process according to the need of that process. However, the size of the allocation unit is fixed that is defined by the Operating System and never changed. Although the partition size may vary but the allocation size is fixed. The main task of the operating system is to keep track of whether the partition is free or filled. For this purpose, the operating system also manages another data structure that is called bitmap. The process or the hole in Allocation units is represented by a flag bit of bitmap. In the image shown below, a flag bit is defined for every bit of allocation units. However, it is not the general case, it depends on the OS that, for how many bits of the allocation units, it wants to store the flag bit.The flag bit is set to 1 if there is a contiguously present process at the adjacent bit in allocation unit otherwise it is set to 0. A string of 0s in the bitmap shows that there is a hole in the relative Allocation unit while the string of 1s represents the process in the relative allocation unit.Disadvantages of using Bitmap:1. The OS has to assign some memory for bitmap as well since it stores the details about allocation units. That much amount of memory cannot be used to load any process therefore that decreases the degree of multiprogramming as well as throughput. In the above image,The allocation unit is of 4 bits that is 0.5 bits. Here, 1 bit of the bitmap is representing 1 bit of allocation unit.Therefore, in this bitmap configuration, 1/5 of total main memory is wasted.2. To identify any hole in the memory, the OS need to search the string of 0s in the bitmap. This searching takes a huge amount of time which makes the system inefficient to some extent",os bit map for dynamic partitioning,".

Easy Questions:
1. What is the main concern for dynamic partitioning?
2. What data structure does the Operating System use to keep track of free and allocated partitions?
3. What is the size of the allocation unit defined by the Operating System?

Medium Questions:
1. What is the purpose of the flag bit in the bitmap?
2. What does a string of 0s in the bitmap represent?
3. What is the disadvantage of using bitmap?

Hard Questions:
1. How does the bitmap configuration affect the degree of multiprogramming?
2. How does the bitmap configuration affect the throughput of the system?
3. How does the bitmap configuration affect the efficiency of the system?

Multiple Choice Questions:
1. What does a string of 0s in the bitmap represent?
A. Process
B. Flag bit
C. Hole
D. Allocation unit

2. What is the size of the allocation unit defined by the Operating System?
A. Variable
B. Fixed
C. Dynamic
D. None of the above

3. How does the bitmap configuration affect the degree of multiprogramming?
A. Increases
B. Decreases
C. No effect
D. None of the above"
"Linked List for Dynamic Partitioning:The better and the most popular approach to keep track the free or filled partitions is using Linked List. In this approach, the Operating system maintains a linked list where each node represents each partition. Every node has three fields.If a partition is freed at some point of time then that partition will be merged with its adjacent free partition without doing any extra effort. There are some points which need to be focused while using this approach.",os linked list for dynamic partitioning,"

Easy Questions:
1. What is the most popular approach to keep track of free or filled partitions?
2. What are the three fields in each node of the linked list?
3. What happens when a partition is freed?

Medium Questions:
1. What are the advantages of using the linked list approach for dynamic partitioning?
2. How does the operating system maintain the linked list?
3. What are the points to be focused while using this approach?

Hard Questions:
1. How does the linked list approach for dynamic partitioning compare to other approaches?
2. What are the challenges of using the linked list approach for dynamic partitioning?
3. How can the linked list approach for dynamic partitioning be improved?

Multiple Choice Questions:
1. What is the most popular approach to keep track of free or filled partitions?
A. Array
B. Linked List
C. Stack
D. Queue

2. What happens when a partition is freed?
A. It is merged with its adjacent free partition
B. It is deleted
C. It is moved to the end of the list
D. It is moved to the beginning of the list

3. What are the points to be focused while using this approach?
A. Efficiency
B. Memory usage
C. Speed
D. All of the above"
"Partitioning Algorithms:There are various algorithms which are implemented by the Operating System in order to find out the holes in the linked list and allocate them to the processes. The explanation about each of the algorithm is given below.1. First Fit AlgorithmFirst Fit algorithm scans the linked list and whenever it finds the first big enough hole to store a process, it stops scanning and load the process into that hole. This procedure produces two partitions. Out of them, one partition will be a hole while the other partition will store the process. First Fit algorithm maintains the linked list according to the increasing order of starting index. This is the simplest to implement among all the algorithms and produces bigger holes as compare to the other algorithms.2. Next Fit AlgorithmNext Fit algorithm is similar to First Fit algorithm except the fact that, Next fit scans the linked list from the node where it previously allocated a hole.Next fit doesn't scan the whole list, it starts scanning the list from the next node. The idea behind the next fit is the fact that the list has been scanned once therefore the probability of finding the hole is larger in the remaining part of the list.Experiments over the algorithm have shown that the next fit is not better then the first fit. So it is not being used these days in most of the cases.3. Best Fit AlgorithmThe Best Fit algorithm tries to find out the smallest hole possible in the list that can accommodate the size requirement of the process.Using Best Fit has some disadvantages.4. Worst Fit AlgorithmThe worst fit algorithm scans the entire list every time and tries to find out the biggest hole in the list which can fulfill the requirement of the process.Despite of the fact that this algorithm produces the larger holes to load the other processes, this is not the better approach due to the fact that it is slower because it searches the entire list every time again and again.5. Quick Fit AlgorithmThe quick fit algorithm suggestsmaintaining the different lists of frequently used sizes. Although, it is not practically suggestible because the procedure takes so much time to create the different lists and then expending the holes to load a process.The first fit algorithm is the best algorithm among all because",os partitioning algorithms," it is the simplest to implement and produces bigger holes as compare to the other algorithms.

Easy Questions:
1. What is the purpose of partitioning algorithms?
2. What is the First Fit algorithm?
3. What is the Quick Fit algorithm?

Medium Questions:
1. What are the advantages and disadvantages of the Best Fit algorithm?
2. What is the difference between First Fit and Next Fit algorithms?
3. How does the Worst Fit algorithm compare to the other algorithms?

Hard Questions:
1. How does the Quick Fit algorithm compare to the other algorithms?
2. What are the implications of using the Quick Fit algorithm?
3. How can the First Fit algorithm be improved?

Multiple Choice Questions:
1. What is the best algorithm among all the partitioning algorithms?
A. First Fit
B. Next Fit
C. Best Fit
D. Worst Fit

2. What is the main disadvantage of the Worst Fit algorithm?
A. It is slower than the other algorithms
B. It produces smaller holes
C. It is difficult to implement
D. It does not scan the entire list

3. What is the main advantage of the Quick Fit algorithm?
A. It produces bigger holes
B. It is faster than the other algorithms
C. It is simpler to implement
D. It maintains different lists of frequently used sizes"
"GATE question on best fit and first fit:From the GATE point of view, Numerical on best fit and first fit are being asked frequently in 1 mark. Let's have a look on the one given as below.Q. Process requests are given as;: 25 K , 50 K , 100 K , 75 K Determine the algorithm which can optimally satisfy this requirement. In the question, there are five partitions in the memory. 3 partitions are having processes inside them and two partitions are holes. Our task is to check the algorithm which can satisfy the request optimally.Using First Fit algorithm:Let's see, how first fit algorithm works on this problem.1. 25 K requirement:The algorithm scans the list until it gets first hole which should be big enough to satisfy the request of 25 K. it gets the space in the second partition which is free hence it allocates 25 K out of 75 K to the process and the remaining 50 K is produced as hole.2. 50 K requirement:The 50 K requirement can be fulfilled by allocating the third partition which is 50 K in size to the process. No free space is produced as free space.3. 100 K requirement:100 K requirement can be fulfilled by using the fifth partition of 175 K size. Out of 175 K, 100 K will be allocated and remaining 75 K will be there as a hole.4. 75 K requirement:Since we are having a 75 K free partition hence we can allocate that much space to the process which is demanding just 75 K space.Using first fit algorithm, we have fulfilled the entire request optimally and no useless space is remaining.Let's see, How Best Fit algorithm performs for the problem.Using Best Fit Algorithm:1. 25 K requirement:To allocate 25 K space using best fit approach, need to scan the whole list and then we find that a 75 K partition is free and the smallest among all, which can accommodate the need of the process. Therefore 25 K out of those 75 K free partition is allocated to the process and the remaining 5o K is produced as a hole.2. 50 K requirement:To satisfy this need, we will again scan the whole list and then find the 50 K space is free which the exact match of the need is. Therefore, it will be allocated for the process.3. 100 K requirement:100 K need is close enough to the 175 K space. The algorithm scans the whole list and then allocates 100 K out of 175 K from the 5th free partition.4. 75 K requirement:75 K requirement will get the space of 75 K from the 6th free partition but the algorithm will scan the whole list in the process of taking this decision.By following both of the algorithms, we have noticed that both the algorithms perform similar to most of the extant in this case.Both can satisfy the need of the processes but however, the best fit algorithm scans the list again and again which takes lot of time.Therefore, if you ask me that which algorithm performs in more optimal way then it will be First Fit algorithm for sure.Therefore, the answer in this case is A.",os gate question on best fit and first fit," First Fit Algorithm

Easy Questions:
1. What is the total amount of memory in the given problem?
2. What is the size of the fourth partition?
3. What is the size of the fifth partition?

Medium Questions:
1. What is the difference between First Fit and Best Fit algorithms?
2. What is the size of the largest hole produced by the First Fit algorithm?
3. What is the size of the largest hole produced by the Best Fit algorithm?

Hard Questions:
1. What is the time complexity of the First Fit algorithm?
2. What is the time complexity of the Best Fit algorithm?
3. How does the First Fit algorithm compare to the Best Fit algorithm in terms of optimal performance?

Multiple Choice Questions:
1. What algorithm is more optimal in this case?
A. First Fit Algorithm
B. Best Fit Algorithm
C. Both algorithms are equally optimal
D. Neither algorithm is optimal

2. How many partitions are there in the memory?
A. 3
B. 4
C. 5
D. 6

3. How much space is allocated to the process that requires 100K?
A. 25K
B. 50K
C. 75K
D. 100K"
"Need for Paging:Disadvantage of Dynamic Partitioning:The main disadvantage of Dynamic Partitioning is External fragmentation. Although, this can be removed by Compaction but as we have discussed earlier, the compaction makes the system inefficient.We need to find out a mechanism which can load the processes in the partitions in a more optimal way. Let us discuss a dynamic and flexible mechanism called paging.Need for Paging:Lets consider a process P1 of size 2 MB and the main memory which is divided into three partitions. Out of the three partitions, two partitions are holes of size 1 MB each.P1 needs 2 MB space in the main memory to be loaded. We have two holes of 1 MB each but they are not contiguous.Although, there is 2 MB space available in the main memory in the form of those holes but that remains useless until it become contiguous. This is a serious problem to address. We need to have some kind of mechanism which can store one process at different locations of the memory. The Idea behind paging is to divide the process in pages so that, we can store them in the memory at different holes. We will discuss paging with the examples in the next sections.",os need for paging,"

Easy Questions:
1. What is the main disadvantage of Dynamic Partitioning?
2. What is the need for paging?
3. What is the idea behind paging?

Medium Questions:
1. What is the size of the process P1?
2. How can compaction help in removing external fragmentation?
3. How can paging help in storing a process at different locations of the memory?

Hard Questions:
1. What are the different techniques used for dynamic partitioning?
2. How can paging help in improving the efficiency of the system?
3. What are the different types of paging techniques?

Multiple Choice Questions:
1. What is the main disadvantage of Dynamic Partitioning?
A. Internal fragmentation
B. External fragmentation
C. Compaction
D. Paging

2. What is the need for paging?
A. To store one process at different locations of the memory
B. To divide the process in pages
C. To improve the efficiency of the system
D. To remove external fragmentation

3. What is the idea behind paging?
A. To divide the process in pages
B. To store one process at different locations of the memory
C. To improve the efficiency of the system
D. To remove external fragmentation"
"Paging in OS (Operating System):In Operating Systems, Paging is a storage mechanism used to retrieve processes from the secondary storage into the main memory in the form of pages. The main idea behind the paging is to divide each process in the form of pages. The main memory will also be divided in the form of frames.One page of the process is to be stored in one of the frames of the memory. The pages can be stored at the different locations of the memory but the priority is always to find the contiguous frames or holes.Pages of the process are brought into the main memory only when they are required otherwise they reside in the secondary storage.Different operating system defines different frame sizes. The sizes of each frame must be equal. Considering the fact that the pages are mapped to the frames in Paging, page size needs to be as same as frame size.Example:Let us consider the main memory size 16 Kb and Frame size is 1 KB therefore the main memory will be divided into the collection of 16 frames of 1 KB each. There are 4 processes in the system that is P1, P2, P3 and P4 of 4 KB each. Each process is divided into pages of 1 KB each so that one page can be stored in one frame. Initially, all the frames are empty therefore pages of the processes will get stored in the contiguous way. Frames, pages and the mapping between the two is shown in the image below.Let us consider that, P2 and P4 are moved to waiting state after some time. Now, 8 frames become empty and therefore other pages can be loaded in that empty place. The process P5 of size 8 KB (8 pages) is waiting inside the ready queue.Given the fact that, we have 8 non contiguous frames available in the memory and paging provides the flexibility of storing the process at the different places. Therefore, we can load the pages of process P5 in the place of P2 and P4.Memory Management Unit:The purpose of Memory Management Unit (MMU) is to convert the logical address into the physical address. The logical address is the address generated by the CPU for every page while the physical address is the actual address of the frame where each page will be stored.When a page is to be accessed by the CPU by using the logical address, the operating system needs to obtain the physical address to access that page physically.The logical address has two parts.Memory management unit of OS needs to convert the page number to the frame number.ExampleConsidering the above image, let's say that the CPU demands 10th word of 4th page of process P3. Since the page number 4 of process P1 gets stored at frame number 9 therefore the 10th word of 9th frame will be returned as the physical address.",os paging with example,"

Easy Questions:
1. What is paging in OS?
2. What is the purpose of Memory Management Unit?
3. What is the size of each frame in the main memory?

Medium Questions:
1. What is the main idea behind the paging?
2. How does the operating system obtain the physical address to access a page?
3. What is the purpose of mapping pages to frames in paging?

Hard Questions:
1. How does the operating system decide which frames to use for storing the pages of a process?
2. How does the operating system manage the non-contiguous frames in the main memory?
3. How does the operating system handle the page faults in paging?

Multiple Choice Questions:
1. What is the purpose of paging in OS?
A. To store processes in the main memory
B. To divide each process into pages
C. To store pages in the secondary storage
D. To map pages to frames

Answer: B. To divide each process into pages

2. What is the purpose of Memory Management Unit?
A. To convert the logical address into the physical address
B. To store processes in the main memory
C. To divide each process into pages
D. To map pages to frames

Answer: A. To convert the logical address into the physical address

3. What is the size of each frame in the main memory?
A. 1 KB
B. 4 KB
C. 8 KB
D. 16 KB

Answer: A. 1 KB"
"Basics of Binary Addresses:Computer system assigns the binary addresses to the memory locations. However, The system uses amount of bits to address a memory location.Using 1 bit, we can address two memory locations. Using 2 bits we can address 4 and using 3 bits we can address 8 memory locations.A pattern can be identified in the mapping between the number of bits in the address and the range of the memory locations.We know,these n bits can be divided into two parts, that are, K bits and (n-k) bits.",os basics of binary addresses,"

Easy Questions:
1. How many memory locations can be addressed using 1 bit?
2. What is the pattern between the number of bits in the address and the range of the memory locations?
3. How many bits are used to address a memory location?

Medium Questions:
1. What is the relationship between the number of bits and the range of memory locations?
2. How can the n bits be divided?
3. What is the purpose of assigning binary addresses to memory locations?

Hard Questions:
1. How can the number of bits in the address be optimized to address a larger range of memory locations?
2. How can the mapping between the number of bits in the address and the range of the memory locations be improved?
3. What are the challenges associated with assigning binary addresses to memory locations?

Multiple Choice Questions:
1. How many memory locations can be addressed using 2 bits?
A. 2
B. 4
C. 8
D. 16

2. What is the purpose of assigning binary addresses to memory locations?
A. To store data
B. To identify memory locations
C. To optimize memory usage
D. To improve performance

3. How can the n bits be divided?
A. Into two parts
B. Into three parts
C. Into four parts
D. Into five parts"
"Physical and Logical Address Space:Physical Address Space:Physical address space in a system can be defined as the size of the main memory. It is really important to compare the process size with the physical address space. The process size must be less than the physical address space.Logical Address Space:Logical address space can be defined as the size of the process. The size of the process should be less enough so that it can reside in the main memory.Let's say,What is a Word?:The Word is the smallest unit of the memory. It is the collection of bytes. Every operating system defines different word sizes after analyzing the n-bit address that is inputted to the decoder and the 2 ^ n memory locations that are produced from the decoder.",os physical and logical address space,"

Easy Questions:
1. What is physical address space?
2. What is logical address space?
3. What is a word?

Medium Questions:
1. How do you compare the process size with the physical address space?
2. What is the importance of the word size in an operating system?
3. How does the decoder produce 2 ^ n memory locations?

Hard Questions:
1. How does the operating system determine the word size?
2. How does the decoder determine the n-bit address?
3. What is the relationship between the physical address space and the process size?

Multiple Choice Questions:
1. What is the smallest unit of the memory?
A. Byte
B. Word
C. Address
D. Process

2. How do you compare the process size with the physical address space?
A. By subtracting the process size from the physical address space
B. By adding the process size to the physical address space
C. By multiplying the process size with the physical address space
D. By dividing the process size with the physical address space

3. What is the importance of the word size in an operating system?
A. To determine the size of the main memory
B. To determine the size of the process
C. To determine the n-bit address
D. To determine the 2 ^ n memory locations"
"Page Table in OS:Page Table is a data structure used by the virtual memory system to store the mapping between logical addresses and physical addresses.Logical addresses are generated by the CPU for the pages of the processes therefore they are generally used by the processes.Physical addresses are the actual frame address of the memory. They are generally used by the hardware or more specifically by RAM subsystems.The image given below considers,The CPU always accesses the processes through their logical addresses. However, the main memory recognizes physical address only.In this situation, a unit named as Memory Management Unit comes into the picture. It converts the page number of the logical address to the frame number of the physical address. The offset remains same in both the addresses.To perform this task, Memory Management unit needs a special kind of mapping which is done by page table. The page table stores all the Frame numbers corresponding to the page numbers of the page table.In other words, the page table maps the page number to its actual location (frame number) in the memory.In the image given below shows, how the required word of the frame is accessed with the help of offset.",os page table,"

Easy Questions:
1. What is a page table?
2. What is the purpose of a Memory Management Unit?
3. What is the purpose of a page table?

Medium Questions:
1. How does the CPU access processes?
2. How does the page table map page numbers to frame numbers?
3. What is the relationship between logical and physical addresses?

Hard Questions:
1. How does the Memory Management Unit convert page numbers to frame numbers?
2. How does the offset help in accessing the required word of the frame?
3. How does the page table store frame numbers corresponding to page numbers?

Multiple Choice Questions:
1. What is the purpose of a page table?
A. To store the mapping between logical and physical addresses
B. To convert page numbers to frame numbers
C. To access the required word of the frame
D. To store frame numbers corresponding to page numbers

2. How does the CPU access processes?
A. Through their physical addresses
B. Through their logical addresses
C. Through their page numbers
D. Through their frame numbers

3. What is the relationship between logical and physical addresses?
A. They are the same
B. They are different
C. They are unrelated
D. They are interchangeable"
"Mapping from page table to main memory:In operating systems, there is always a requirement of mapping from logical address to the physical address. However, this process involves various steps which are defined as follows.1. Generation of logical address:CPU generates logical address for each page of the process. This contains two parts: page number and offset.2. Scaling:To determine the actual page number of the process, CPU stores the page table base in a special register. Each time the address is generated, the value of the page table base is added to the page number to get the actual location of the page entry in the table. This process is called scaling.3. Generation of physical Address:The frame number of the desired page is determined by its entry in the page table. A physical address is generated which also contains two parts : frame number and offset. The Offset will be similar to the offset of the logical address therefore it will be copied from the logical address.4. Getting Actual Frame Number:The frame number and the offset from the physical address is mapped to the main memory in order to get the actual word address.",os mapping from page table to main memory,"

Easy Questions:
1. What is the purpose of mapping from logical address to physical address?
2. What is the process of scaling?
3. What is the purpose of the page table base register?

Medium Questions:
1. What are the two parts of the logical address?
2. How is the frame number determined from the page table?
3. What is the purpose of the offset in the physical address?

Hard Questions:
1. How does the CPU determine the actual page number of the process?
2. How is the physical address generated from the logical address?
3. How is the actual word address obtained from the physical address?

Multiple Choice Questions:
1. What is the purpose of mapping from logical address to physical address?
a. To determine the actual page number of the process
b. To generate a physical address
c. To generate a logical address
d. To get the actual word address

2. What is the process of scaling?
a. Adding the page table base to the page number
b. Generating a physical address
c. Generating a logical address
d. Getting the actual word address

3. How is the frame number determined from the page table?
a. By adding the page table base to the page number
b. By generating a physical address
c. By generating a logical address
d. By getting the actual word address"
"Page Table Entry:Along with page frame number, the page table also contains some of the bits representing the extra information regarding the page. Let's see what the each bit represents about the page.1. Caching Disabled:Sometimes, there are differences between the information closest to the CPU and the information closest to the user. Operating system always wants CPU to access user's data as soon as possible. CPU accesses cache which can be inaccurate in some of the cases, therefore, OS can disable the cache for the required pages. This bit is set to 1 if the cache is disabled.2. Referenced:There are variouspage replacement algorithms which will be covered later in this tutorial. This bit is set to 1 if the page is referred in the last clock cycle otherwise it remains 0.3. Modified:This bit will be set if the page has been modified otherwise it remains 0.4. Protection:The protection field represents the protection level which is applied on the page. It can be read only or read & write or execute. We need to remember that it is not a bit rather it is a field which contains many bits.5. Present/Absent:In the concept of demand paging, all the pages doesn't need to be present in the main memory Therefore, for all the pages that are present in the main memory, this bit will be set to 1 and the bit will be 0 for all the pages which are absent.If some page is not present in the main memory then it is called page fault.",os page table entry,"

Easy Questions:
1. What is a page table?
2. What is the purpose of the caching disabled bit?
3. What is the purpose of the referenced bit?

Medium Questions:
1. What is the purpose of the modified bit?
2. What is the purpose of the protection field?
3. What is the purpose of the present/absent bit?

Hard Questions:
1. How does the operating system use the page table to access user data?
2. How does the operating system determine when to set the caching disabled bit?
3. How does the operating system determine when to set the referenced bit?

Multiple Choice Questions:
1. What is the purpose of the present/absent bit?
A. To indicate if the page is present in the main memory
B. To indicate if the page is absent in the main memory
C. To indicate if the page is modified
D. To indicate if the page is referenced

Answer: A"
"Size of the page table:However, the part of the process which is being executed by the CPU must be present in the main memory during that time period. The page table must also be present in the main memory all the time because it has the entry for all the pages.The size of the page table depends upon the number of entries in the table and the bytes stored in one entry.Let's consider,Here we are lucky enough to get the page table size equal to the frame size. Now, the page table will be simply stored in one of the frames of the main memory. The CPU maintains a register which contains the base address of that frame, every page number from the logical address will first be added to that base address so that we can access the actual location of the word being asked.However, in some cases, the page table size and the frame size might not be same. In those cases, the page table is considered as the collection of frames and will be stored in the different frames.",os page table size,"

Easy Questions:
1. What must be present in the main memory during the process execution?
2. What is the size of the page table determined by?
3. What is the register maintained by the CPU?

Medium Questions:
1. How is the page table stored in the main memory when the page table size is equal to the frame size?
2. How is the actual location of the word being asked accessed?
3. What happens when the page table size and the frame size are not the same?

Hard Questions:
1. How does the CPU determine the base address of the frame?
2. How is the page table stored in the main memory when the page table size is not equal to the frame size?
3. How does the CPU ensure that the page table is always present in the main memory?

Multiple Choice Questions:
1. What is the size of the page table determined by?
A. The number of entries in the table
B. The bytes stored in one entry
C. The frame size
D. All of the above

2. How is the page table stored in the main memory when the page table size is equal to the frame size?
A. In one of the frames
B. In multiple frames
C. In the CPU register
D. In the main memory

3. How is the actual location of the word being asked accessed?
A. By adding the page number to the base address
B. By subtracting the page number from the base address
C. By multiplying the page number with the base address
D. By dividing the page number with the base address"
"Finding Optimal Page Size:We have seen that the bigger page table size cause an extra overhead because we have to divide that table into the pages and then store that into the main memory. Our concern must be about executing processes not on the execution of page table. Page table provides a support for the execution of the process. The larger the page Table, the higher the overhead. We know that,There will be 1 million pages which is quite big number. However, try to make page size larger, say 2 MB.Then, Number of pages in page table = (2 X 2 ^ 30)/(2 X 2 ^ 20) = 1 K pages.
If we compare the two scenarios, we can find out that the page table size is anti proportional to Page Size.In Paging, there is always wastage on the last page. If the virtual address space is not a multiple of page size, then there will be some bytes remaining and we have to assign a full page to those many bytes. This is simply a overhead.Let's consider, The number of pages will be 9 although the 9th page will only contain 1 byte and the remaining page will be wasted. In general,On an average, the wasted number of pages in a virtual space is p/2(the half of total number of pages).For, the minimal overhead,Hence, if the page size √(2.S.e) bytes then the overhead will be minimal.",os finding optimal page size,"

Easy Questions:
1. What is the purpose of a page table?
2. What is the size of a page in the example given?
3. What is the average number of wasted pages in a virtual space?

Medium Questions:
1. What is the relationship between page table size and page size?
2. How can we minimize the overhead of a page table?
3. What is the formula for calculating the optimal page size?

Hard Questions:
1. How does the size of the virtual address space affect the number of wasted pages?
2. How does the size of the page table affect the execution of processes?
3. How does the size of the page table affect the amount of memory used?

Multiple Choice Questions:
1. What is the relationship between page table size and page size?
A. Proportional
B. Inversely Proportional
C. No Relationship
D. Linear

2. How can we minimize the overhead of a page table?
A. Increase the page size
B. Decrease the page size
C. Increase the page table size
D. Decrease the page table size

3. What is the formula for calculating the optimal page size?
A. √(2.S.e) bytes
B. 2 MB
C. 1 K pages
D. p/2"
"What is Virtual Memory in OS (Operating System)?:Virtual Memory is a storage scheme that provides user an illusion of having a very big main memory. This is done by treating a part of secondary memory as the main memory.In this scheme, User can load the bigger size processes than the available main memory by having the illusion that the memory is available to load the process.Instead of loading one big process in the main memory, the Operating System loads the different parts of more than one process in the main memory.By doing this, the degree of multiprogramming will be increased and therefore, the CPU utilization will also be increased. How Virtual Memory Works?:In modern word, virtual memory has become quite common these days. In this scheme, whenever some pages needs to be loaded in the main memory for the execution and the memory is not available for those many pages, then in that case, instead of stopping the pages from entering in the main memory, the OS search for the RAM area that are least used in the recent times or that are not referenced and copy that into the secondary memory to make the space for the new pages in the main memory.Since all this procedure happens automatically, therefore it makes the computer feel like it is having the unlimited RAM.Demand Paging:Demand Paging is a popular method of virtual memory management. In demand paging, the pages of a process which are least used, get stored in the secondary memory.A page is copied to the main memory when its demand is made or page fault occurs. There are various page replacement algorithms which are used to determine the pages which will be replaced. We will discuss each one of them later in detail. Snapshot of a virtual memory management system:Let us assume 2 processes, P1 and P2, contains 4 pages each. Each page size is 1 KB. The main memory contains 8 frame of 1 KB each. The OS resides in the first two partitions. In the third partition, 1st page of P1 is stored and the other frames are also shown as filled with the different pages of processes in the main memory. The page tables of both the pages are 1 KB size each and therefore they can be fit in one frame each. The page tables of both the processes contain various information that is also shown in the image.The CPU contains a register which contains the base address of page table that is 5 in the case of P1 and 7 in the case of P2. This page table base address will be added to the page number of the Logical address when it comes to accessing the actual corresponding entry.Advantages of Virtual Memory:Disadvantages of Virtual Memory:",os virtual memory,"

Easy Questions:
1. What is Virtual Memory?
2. What is the purpose of Virtual Memory?
3. How does Virtual Memory work?

Medium Questions:
1. What is the size of a page in Virtual Memory?
2. What is the purpose of the page table in Virtual Memory?
3. What is Demand Paging?

Hard Questions:
1. What is the purpose of the page table base address in Virtual Memory?
2. How does the CPU access the actual corresponding entry in Virtual Memory?
3. What are the advantages and disadvantages of Virtual Memory?

Multiple Choice Questions:
1. What is the purpose of Virtual Memory?
A. To provide an illusion of having a very big main memory
B. To store the least used pages in the secondary memory
C. To increase the degree of multiprogramming
D. All of the above

2. What is Demand Paging?
A. A method of virtual memory management
B. A page replacement algorithm
C. A register that contains the base address of page table
D. A page that is copied to the main memory when its demand is made

3. What is the size of a page in Virtual Memory?
A. 1 KB
B. 2 KB
C. 4 KB
D. 8 KB"
"Translation Look aside buffer:Drawbacks of Paging:How to decrease the page table size:How to decrease the effective access time:Locality of reference:In operating systems, the concept of locality of reference states that, instead of loading the entire process in the main memory, OS can load only those number of pages in the main memory that are frequently accessed by the CPU and along with that, the OS can also load only those page table entries which are corresponding to those many pages.Translation look aside buffer (TLB):A Translation look aside buffer can be defined as a memory cache which can be used to reduce the time taken to access the page table again and again.It is a memory cache which is closer to the CPU and the time taken by CPU to access TLB is lesser then that taken to access main memory.In other words, we can say that TLB is faster and smaller than the main memory but cheaper and bigger than the register.TLB follows the concept of locality of reference which means that it contains only the entries of those many pages that are frequently accessed by the CPU. In translation look aside buffers, there are tags and keys with the help of which, the mapping is done. TLB hit is a condition where the desired entry is found in translation look aside buffer. If this happens then the CPU simply access the actual location in the main memory.However, if the entry is not found in TLB (TLB miss) then CPU has to access page table in the main memory and then access the actual frame in the main memory. Therefore, in the case of TLB hit, the effective access time will be lesser as compare to the case of TLB miss. If the probability of TLB hit is P% (TLB hit rate) then the probability of TLB miss (TLB miss rate) will be (1-P) %. Therefore, the effective access time can be defined as;Where, p → TLB hit rate, t → time taken to access TLB, m → time taken to access main memory k = 1, if the single level paging has been implemented. By the formula, we come to know that",os translation look aside buffer," the effective access time can be decreased by increasing the TLB hit rate.

Easy Questions:
1. What is a Translation Look Aside Buffer?
2. What is the concept of locality of reference?
3. What is a TLB hit?

Medium Questions:
1. What is the formula for calculating effective access time?
2. How can the effective access time be decreased?
3. What is the difference between TLB and main memory?

Hard Questions:
1. How does the TLB follow the concept of locality of reference?
2. What is the difference between TLB hit and TLB miss?
3. How does the TLB reduce the time taken to access the page table?

Multiple Choice Questions:
1. What is the TLB hit rate?
a. The probability of TLB hit
b. The probability of TLB miss
c. The time taken to access TLB
d. The time taken to access main memory

2. How can the effective access time be decreased?
a. Increasing the TLB hit rate
b. Decreasing the TLB hit rate
c. Increasing the TLB miss rate
d. Decreasing the TLB miss rate

3. What is the difference between TLB and main memory?
a. TLB is faster and smaller than main memory
b. TLB is slower and bigger than main memory
c. TLB is faster and bigger than main memory
d. TLB is slower and smaller than main memory"
"GATE Question on TLB:GATE | GATE-CS-2014-(Set-3):Consider a paging hardware with a TLB. Assume that the entire page table and all the pages are in the physical memory. It takes 10 milliseconds to search the TLB and 80 milliseconds to access the physical memory. If the TLB hit ratio is 0.6, the effective memory access time (in milliseconds) is _________.A. 120
B. 122
C. 124
D. 118Given,Effective Access Time (EAT) = 0.6 ( 10 + 80 ) + 0.4 ( 10 + 80 + 80 ) = 90 X 0.6 + 0.4 X 170 = 122 Hence, the right answer is option B.",os gate 2014 question on tlb,"

Easy Questions:
1. What is a TLB?
2. What is the TLB hit ratio?
3. How long does it take to search the TLB?

Medium Questions:
1. What is the effective memory access time?
2. How long does it take to access the physical memory?
3. What is the formula for calculating the effective memory access time?

Hard Questions:
1. How does the TLB improve the performance of a paging hardware?
2. What are the advantages of using a TLB?
3. How does the TLB hit ratio affect the effective memory access time?

Multiple Choice Questions:
1. What is the effective memory access time (in milliseconds) given the TLB hit ratio is 0.6?
A. 120
B. 122
C. 124
D. 118

2. How long does it take to search the TLB?
A. 10 milliseconds
B. 20 milliseconds
C. 30 milliseconds
D. 40 milliseconds

3. What is the formula for calculating the effective memory access time?
A. EAT = 0.6 ( 10 + 80 ) + 0.4 ( 10 + 80 + 80 )
B. EAT = 0.6 ( 10 + 80 ) + 0.4 ( 10 + 80 - 80 )
C. EAT = 0.6 ( 10 - 80 ) + 0.4 ( 10 + 80 + 80 )
D. EAT = 0.6 ( 10 - 80 ) + 0.4 ( 10 - 80 - 80 )"
"What is Demand Paging in OS (Operating System)?:According to the concept of Virtual Memory, in order to execute some process, only a part of the process needs to be present in the main memory which means that only a few pages will only be present in the main memory at any time. However, deciding, which pages need to be kept in the main memory and which need to be kept in the secondary memory, is going to be difficult because we cannot say in advance that a process will require a particular page at particular time.Therefore, to overcome this problem, there is a concept called Demand Paging is introduced. It suggests keeping all pages of the frames in the secondary memory until they are required. In other words, it says that do not load any page in the main memory until it is required. Whenever any page is referred for the first time in the main memory, then that page will be found in the secondary memory.After that, it may or may not be present in the main memory depending upon the page replacement algorithm which will be covered later in this tutorial.What is a Page Fault?:If the referred page is not present in the main memory then there will be a miss and the concept is called Page miss or page fault.The CPU has to access the missed page from the secondary memory. If the number of page fault is very high then the effective access time of the system will become very high.What is Thrashing?:If the number of page faults is equal to the number of referred pages or the number of page faults are so high so that the CPU remains busy in just reading the pages from the secondary memory then the effective access time will be the time taken by the CPU to read one word from the secondary memory and it will be so high. The concept is called thrashing.If the page fault rate is PF %, the time taken in getting a page from the secondary memory and again restarting is S (service time) and the memory access time is ma then the effective access time can be given as; ",os demand paging,"

Easy Questions:
1. What is Demand Paging?
2. What is a Page Fault?
3. What is Thrashing?

Medium Questions:
1. What is the concept of Virtual Memory?
2. What is the page replacement algorithm?
3. How is the effective access time of the system affected by the number of page faults?

Hard Questions:
1. How is the page fault rate calculated?
2. How is the effective access time affected by the memory access time?
3. What is the relationship between the service time and the effective access time?

Multiple Choice Questions:
1. What is the concept of Virtual Memory?
A. Keeping all pages of the frames in the main memory
B. Keeping all pages of the frames in the secondary memory
C. Keeping only a few pages in the main memory
D. Keeping only a few pages in the secondary memory

2. What is Thrashing?
A. High number of page faults
B. Low number of page faults
C. High number of referred pages
D. Low number of referred pages

3. How is the effective access time of the system affected by the number of page faults?
A. It increases
B. It decreases
C. It remains the same
D. It fluctuates"
"Inverted Page Table:Inverted Page Table is the global page table which is maintained by the Operating System for all the processes. In inverted page table, the number of entries is equal to the number of frames in the main memory. It can be used to overcome the drawbacks of page table. There is always a space reserved for the page regardless of the fact that whether it is present in the main memory or not. However, this is simply the wastage of the memory if the page is not present.We can save this wastage by just inverting the page table. We can save the details only for the pages which are present in the main memory. Frames are the indices and the information saved inside the block will be Process ID and page number.",os inverted page table,"

Easy Questions:
1. What is an Inverted Page Table?
2. What is the purpose of an Inverted Page Table?
3. How many entries are in an Inverted Page Table?

Medium Questions:
1. What are the drawbacks of a page table that an Inverted Page Table can overcome?
2. How does an Inverted Page Table save memory?
3. What information is stored in each block of an Inverted Page Table?

Hard Questions:
1. How does an Inverted Page Table ensure that the page is always reserved?
2. How does an Inverted Page Table ensure that the information is only saved for pages that are present in the main memory?
3. What is the relationship between the number of entries in an Inverted Page Table and the number of frames in the main memory?

Multiple Choice Questions:
1. What is the purpose of an Inverted Page Table?
A. To save memory
B. To store information
C. To overcome the drawbacks of a page table
D. To reserve space for the page

2. How many entries are in an Inverted Page Table?
A. The same as the number of frames in the main memory
B. The same as the number of pages in the main memory
C. The same as the number of processes in the main memory
D. The same as the number of entries in a page table

3. What information is stored in each block of an Inverted Page Table?
A. Process ID and page number
B. Frame number and page number
C. Process ID and frame number
D. Page number and memory address"
"Page Replacement Algorithms in Operating Systems (OS):Today we are going to learn about Page Replacement Algorithms in Operating Systems (OS). Before knowing about Page Replacement Algorithms in Operating Systems let us learn about Paging in Operating Systems and also a little about Virtual Memory.Only after understanding the concept of Paging we will understand about Page Replacement Algorithms.Paging in Operating Systems (OS):Paging is a storage mechanism. Paging is used to retrieve processes from secondary memory to primary memory.The main memory is divided into small blocks called pages. Now, each of the pages contains the process which is retrieved into main memory and it is stored in one frame of memory.It is very important to have pages and frames which are of equal sizes which are very useful for mapping and complete utilization of memory.Virtual Memory in Operating Systems (OS):A storage method known as virtual memory gives the user the impression that their main memory is quite large. By considering a portion of secondary memory as the main memory, this is accomplished.By giving the user the impression that there is memory available to load the process, this approach allows them to load larger size programs than the primary memory that is accessible.The Operating System loads the many components of several processes in the main memory as opposed to loading a single large process there.By doing this, the level of multiprogramming will be enhanced, which will increase CPU consumption.Demand Paging:The Demand Paging is a condition which is occurred in the Virtual Memory. We know that the pages of the process are stored in secondary memory. The page is brought to the main memory when required. We do not know when this requirement is going to occur. So, the pages are brought to the main memory when required by the Page Replacement Algorithms.So, the process of calling the pages to main memory to secondary memory upon demand is known as Demand Paging.The important jobs of virtual memory in Operating Systems are two. They are:Frame Allocation in Virtual Memory:Demand paging is used to implement virtual memory, an essential component of operating systems. A page-replacement mechanism and a frame allocation algorithm must be created for demand paging. If you have numerous processes, frame allocation techniques are utilized to determine how many frames to provide to each process.A Physical Address is required by the Central Processing Unit (CPU) for the frame creation and the physical Addressing provides the actual address to the frame created. For each page a frame must be created.Frame Allocation Constraints:Frame Allocation Algorithms:There are three types of Frame Allocation Algorithms in Operating Systems. They are:1) Equal Frame Allocation AlgorithmsHere, in this Frame Allocation Algorithm we take number of frames and number of processes at once. We divide the number of frames by number of processes. We get the number of frames we must provide for each process.This means if we have 36 frames and 6 processes. For each process 6 frames are allocated.It is not very logical to assign equal frames to all processes in systems with processes of different sizes. A lot of allocated but unused frames will eventually be wasted if a lot of frames are given to a little operation.2) Proportionate Frame Allocation AlgorithmsHere, in this Frame Allocation Algorithms we take number of frames based on the process size. For big process more number of frames is allocated. For small processes less number of frames is allocated by the operating system.The problem in the Proportionate Frame Allocation Algorithm is number of frames are wasted in some rare cases.The advantage in Proportionate Frame Allocation Algorithm is that instead of equally, each operation divides the available frames according to its demands.3) Priority Frame Allocation AlgorithmsAccording to the quantity of frame allocations and the processes, priority frame allocation distributes frames. Let's say a process has a high priority and needs more frames; in such case, additional frames will be given to the process. Processes with lower priorities are then later executed in future and first only high priority processes are executed first.Page Replacement Algorithms:There are three types of Page Replacement Algorithms. They are:First in First out Page Replacement Algorithm:This is the first basic algorithm of Page Replacement Algorithms. This algorithm is basically dependent on the number of frames used. Then each frame takes up the certain page and tries to access it. When the frames are filled then the actual problem starts. The fixed number of frames is filled up with the help of first frames present. This concept is fulfilled with the help of Demand PagingAfter filling up of the frames, the next page in the waiting queue tries to enter the frame. If the frame is present then, no problem is occurred. Because of the page which is to be searched is already present in the allocated frames.If the page to be searched is found among the frames then, this process is known as Page Hit.If the page to be searched is not found among the frames then, this process is known as Page Fault.When Page Fault occurs this problem arises, then the First In First Out Page Replacement Algorithm comes into picture.The First In First Out (FIFO) Page Replacement Algorithm removes the Page in the frame which is allotted long back. This means the useless page which is in the frame for a longer time is removed and the new page which is in the ready queue and is ready to occupy the frame is allowed by the First In First Out Page Replacement.Let us understand this First In First Out Page Replacement Algorithm working with the help of an example.Example:Consider the reference string 6, 1, 1, 2, 0, 3, 4, 6, 0, 2, 1, 2, 1, 2, 0, 3, 2, 1, 2, 0 for a memory with three frames and calculate number of page faults by using FIFO (First In First Out) Page replacement algorithms.Points to RememberPage Not Found - - - > Page FaultPage Found - - - > Page HitReference String:Number of Page Hits = 8Number of Page Faults = 12The Ratio of Page Hit to the Page Fault = 8 : 12 - - - > 2 : 3 - - - > 0.66The Page Hit Percentage = 8 *100 / 20 = 40%The Page Fault Percentage = 100 - Page Hit Percentage = 100 - 40 = 60%ExplanationFirst, fill the frames with the initial pages. Then, after the frames are filled we need to create a space in the frames for the new page to occupy. So, with the help of First in First Out Page Replacement Algorithm we remove the frame which contains the page is older among the pages. By removing the older page we give access for the new frame to occupy the empty space created by the First in First out Page Replacement Algorithm.OPTIMAL Page Replacement Algorithm:This is the second basic algorithm of Page Replacement Algorithms. This algorithm is basically dependent on the number of frames used. Then each frame takes up the certain page and tries to access it. When the frames are filled then the actual problem starts. The fixed number of frames is filled up with the help of first frames present. This concept is fulfilled with the help of Demand PagingAfter filling up of the frames, the next page in the waiting queue tries to enter the frame. If the frame is present then, no problem is occurred. Because of the page which is to be searched is already present in the allocated frames.If the page to be searched is found among the frames then, this process is known as Page Hit.If the page to be searched is not found among the frames then, this process is known as Page Fault.When Page Fault occurs this problem arises, then the OPTIMAL Page Replacement Algorithm comes into picture.The OPTIMAL Page Replacement Algorithms works on a certain principle. The principle is:Replace the Page which is not used in the Longest Dimension of time in futureThis principle means that after all the frames are filled then, see the future pages which are to occupy the frames. Go on checking for the pages which are already available in the frames. Choose the page which is at last.Example:Suppose the Reference String is:0, 3, 4, 6, 0, 2, 1, 2, 1, 2, 0, 3, 2, 1, 2, 06, 1, 2 are in the frames occupying the frames.Now we need to enter 0 into the frame by removing one page from the pageSo, let us check which page number occurs lastFrom the sub sequence 0, 3, 4, 6, 0, 2, 1 we can say that 1 is the last occurring page number. So we can say that 0 can be placed in the frame body by removing 1 from the frame.Let us understand this OPTIMAL Page Replacement Algorithm working with the help of an example.Example:Consider the reference string 6, 1, 1, 2, 0, 3, 4, 6, 0, 2, 1, 2, 1, 2, 0, 3, 2, 1, 4, 0 for a memory with three frames and calculate number of page faults by using OPTIMAL Page replacement algorithms.Points to RememberPage Not Found - - - > Page FaultPage Found - - - > Page HitReference String:Number of Page Hits = 8Number of Page Faults = 12The Ratio of Page Hit to the Page Fault = 8 : 12 - - - > 2 : 3 - - - > 0.66The Page Hit Percentage = 8 *100 / 20 = 40%The Page Fault Percentage = 100 - Page Hit Percentage = 100 - 40 = 60%ExplanationFirst, fill the frames with the initial pages. Then, after the frames are filled we need to create a space in the frames for the new page to occupy.Here, we would fill the empty spaces with the pages we and the empty frames we have. The problem occurs when there is no space for occupying of pages. We have already known that we would replace the Page which is not used in the Longest Dimension of time in future.There comes a question what if there is absence of page which is in the frame.Suppose the Reference String is:0, 2, 4, 6, 0, 2, 1, 2, 1, 2, 0, 3, 2, 1, 2, 06, 1, 5 are in the frames occupying the frames.Here, we can see that page number 5 is not present in the Reference String. But the number 5 is present in the Frame. So, as the page number 5 is absent we remove it when required and other page can occupy that position.Least Recently Used (LRU) Replacement Algorithm:This is the last basic algorithm of Page Replacement Algorithms. This algorithm is basically dependent on the number of frames used. Then each frame takes up the certain page and tries to access it. When the frames are filled then the actual problem starts. The fixed number of frames is filled up with the help of first frames present. This concept is fulfilled with the help of Demand PagingAfter filling up of the frames, the next page in the waiting queue tries to enter the frame. If the frame is present then, no problem is occurred. Because of the page which is to be searched is already present in the allocated frames.If the page to be searched is found among the frames then, this process is known as Page Hit.If the page to be searched is not found among the frames then, this process is known as Page Fault.When Page Fault occurs this problem arises, then the Least Recently Used (LRU) Page Replacement Algorithm comes into picture.The Least Recently Used (LRU) Page Replacement Algorithms works on a certain principle. The principle is:Replace the page with the page which is less dimension of time recently used page in the past.Example:Suppose the Reference String is:6, 1, 1, 2, 0, 3, 4, 6, 0The pages with page numbers 6, 1, 2 are in the frames occupying the frames.Now, we need to allot a space for the page numbered 0.Now, we need to travel back into the past to check which page can be replaced.6 is the oldest page which is available in the Frame.So, replace 6 with the page numbered 0.Let us understand this Least Recently Used (LRU) Page Replacement Algorithm working with the help of an example.Example:Consider the reference string 6, 1, 1, 2, 0, 3, 4, 6, 0, 2, 1, 2, 1, 2, 0, 3, 2, 1, 2, 0 for a memory with three frames and calculate number of page faults by using Least Recently Used (LRU) Page replacement algorithms.Points to RememberPage Not Found - - - > Page FaultPage Found - - - > Page HitReference String:Number of Page Hits = 7Number of Page Faults = 13The Ratio of Page Hit to the Page Fault = 7 : 12 - - - > 0.5833 : 1The Page Hit Percentage = 7 * 100 / 20 = 35%The Page Fault Percentage = 100 - Page Hit Percentage = 100 - 35 = 65%ExplanationFirst, fill the frames with the initial pages. Then, after the frames are filled we need to create a space in the frames for the new page to occupy.Here, we would fill the empty spaces with the pages we and the empty frames we have. The problem occurs when there is no space for occupying of pages. We have already known that we would replace the Page which is not used in the Longest Dimension of time in past or can be said as the Page which is very far away in the past.",os page replacement algorithms,"There comes a question what if there is absence of page which is in the frame.Suppose the Reference String is:0, 2, 4, 6, 0, 2, 1, 2, 1, 2, 0, 3, 2, 1, 2, 06, 1, 5 are in the frames occupying the frames.Here, we can see that page number 5 is not present in the Reference String. But the number 5 is present in the Frame. So, as the page number 5 is absent we remove it when required and other page can occupy that position.

Easy Questions:
1. What is paging in operating systems?
2. What is virtual memory?
3. What is demand paging?

Medium Questions:
1. What are the three types of frame allocation algorithms?
2. What are the three types of page replacement algorithms?
3. What is the principle of the Optimal page replacement algorithm?

Hard Questions:
1. How does the First In First Out page replacement algorithm work?
2. How does the Least Recently Used page replacement algorithm work?
3. How does the Optimal page replacement algorithm work?

Multiple Choice Questions:
1. What is the principle of the Least Recently Used page replacement algorithm?
a. Replace the page with the page which is most recently used page in the past
b. Replace the page with the page which is least recently used page in the past
c. Replace the page with the page which is most dimension of time recently used page in the past
d. Replace the page with the page which is less dimension of time recently used page in the past

2. What is the ratio of page hit to page fault in the example given in the paragraph?
a. 2:3
b. 3:2
c. 4:3
d. 3:4

3. What is the page hit percentage in the example given in the paragraph?
a. 40%
b. 50%
c. 60%
d. 70%"
"GATE 2015 question on LRU and FIFO:Q. Consider a main memory with five page frames and the following sequence of page references: 3, 8, 2, 3, 9, 1, 6, 3, 8, 9, 3, 6, 2, 1, 3. which one of the following is true with respect to page replacement policies First-In-First-out (FIFO) and Least Recently Used (LRU)?:A. Both incur the same number of page faults 
B. FIFO incurs 2 more page faults than LRU
C. LRU incurs 2 more page faults than FIFO
D. FIFO incurs 1 more page faults than LRUSolution:Number of frames = 5FIFO:According to FIFO, the page which first comes in the memory will first goes out.Number of Page Faults = 9 
Number of hits = 6LRU:According to LRU, the page which has not been requested for a long time will get replaced with the new one.Number of Page Faults = 9
Number of Hits = 6 The Number of page faults in both the cases is equal therefore the Answer is option (A).",os gate 2015 question on lru and fifo,"

Easy Questions:
1. What is the main memory size in the given question?
2. What is the page replacement policy for FIFO?
3. How many page faults occur in FIFO?

Medium Questions:
1. What is the page replacement policy for LRU?
2. How many page hits occur in LRU?
3. What is the total number of page frames in the given question?

Hard Questions:
1. What is the difference between FIFO and LRU page replacement policies?
2. How can the number of page faults be minimized?
3. What is the optimal page replacement algorithm?

Multiple Choice Questions:
1. What is the total number of page faults in the given question?
A. 8
B. 9
C. 10
D. 11

2. How many page hits occur in FIFO?
A. 5
B. 6
C. 7
D. 8

3. What is the page replacement policy for LRU?
A. First-In-First-Out
B. Last-In-First-Out
C. Least Recently Used
D. Most Recently Used"
"Numerical on Optimal, LRU and FIFO:Q. Consider a reference string: 4, 7, 6, 1, 7, 6, 1, 2, 7, 2. the number of frames in the memory is 3. Find out the number of page faults respective to::Optimal Page Replacement Algorithm:Number of Page Faults in Optimal Page Replacement Algorithm = 5LRU Page Replacement Algorithm:Number of Page Faults in LRU = 6FIFO Page Replacement Algorithm:Number of Page Faults in FIFO = 6",os numerical on lru fifo and optimal,"

Easy Questions:
1. What is the reference string in the question?
2. How many frames are in the memory?
3. What is the page replacement algorithm?

Medium Questions:
1. What is the difference between LRU and FIFO page replacement algorithms?
2. What is the purpose of the optimal page replacement algorithm?
3. How does the number of frames in the memory affect the number of page faults?

Hard Questions:
1. How can the page replacement algorithm be optimized to reduce the number of page faults?
2. What are the trade-offs between the different page replacement algorithms?
3. How can the page replacement algorithm be adapted to different memory sizes?

Multiple Choice Questions:
1. What is the number of page faults in the Optimal Page Replacement Algorithm?
A. 4
B. 5
C. 6
D. 7

2. What is the number of page faults in the LRU Page Replacement Algorithm?
A. 4
B. 5
C. 6
D. 7

3. What is the number of page faults in the FIFO Page Replacement Algorithm?
A. 4
B. 5
C. 6
D. 7"
"Belady'sAnomaly:In the case of LRU and optimal page replacement algorithms, it is seen that the number of page faults will be reduced if we increase the number of frames. However, Balady found that, In FIFO page replacement algorithm, the number of page faults will get increased with the increment in number of frames.This is the strange behavior shown by FIFO algorithm in some of the cases. This is an Anomaly called as Belady'sAnomaly.Let's examine such example :The reference String is given as 0 1 5 3 0 1 4 0 1 5 3 4. Let's analyze the behavior of FIFO algorithm in two cases.Case 1: Number of frames = 3:Number of Page Faults = 9Case 2: Number of frames = 4:Number of Page Faults = 10Therefore, in this example, the number of page faults is increasing by increasing the number of frames hence this suffers from Belady'sAnomaly.",os beladys anamoly,"

Easy Questions:
1. What is Belady's Anomaly?
2. What is the reference string in the example?
3. What page replacement algorithm is being discussed?

Medium Questions:
1. What is the difference between the number of page faults in Case 1 and Case 2?
2. What is the behavior of FIFO algorithm in the example?
3. What is the strange behavior shown by FIFO algorithm?

Hard Questions:
1. What is the significance of Belady's Anomaly?
2. How does the number of page faults change with the number of frames in LRU and optimal page replacement algorithms?
3. How can Belady's Anomaly be avoided?

Multiple Choice Questions:
1. What is the number of page faults in Case 1?
A. 8
B. 9
C. 10
D. 11

2. What is the reference string in the example?
A. 0 1 5 3 0 1 4 0 1 5 3 4
B. 0 1 2 3 4 5 6 7 8 9 10
C. 0 1 2 3 4 5 6 7 8 9
D. 0 1 2 3 4 5 6

3. What page replacement algorithm is being discussed?
A. LRU
B. FIFO
C. Optimal
D. Belady's Anomaly"
"Segmentation in OS (Operating System):In Operating Systems, Segmentation is a memory management technique in which the memory is divided into the variable size parts. Each part is known as a segment which can be allocated to a process.The details about each segment are stored in a table called a segment table. Segment table is stored in one (or many) of the segments.Segment table contains mainly two information about segment:Why Segmentation is required?:Till now, we were using Paging as our main memory management technique. Paging is more close to the Operating system rather than the User. It divides all the processes into the form of pages regardless of the fact that a process can have some relative parts of functions which need to be loaded in the same page.Operating system doesn't care about the User's view of the process. It may divide the same function into different pages and those pages may or may not be loaded at the same time into the memory. It decreases the efficiency of the system.It is better to have segmentation which divides the process into the segments. Each segment contains the same type of functions such as the main function can be included in one segment and the library functions can be included in the other segment.Translation of Logical address into physical address by segment table:CPU generates a logical address which contains two parts:For Example:Suppose a 16 bit address is used with 4 bits for the segment number and 12 bits for the segment offset so the maximum segment size is 4096 and the maximum number of segments that can be refereed is 16.When a program is loaded into memory, the segmentation system tries to locate space that is large enough to hold the first segment of the process, space information is obtained from the free list maintained by memory manager. Then it tries to locate space for other segments. Once adequate space is located for all the segments, it loads them into their respective areas.The operating system also generates a segment map table for each program.With the help of segment map tables and hardware assistance, the operating system can easily translate a logical address into physical address on execution of a program.The Segment number is mapped to the segment table. The limit of the respective segment is compared with the offset. If the offset is less than the limit then the address is valid otherwise it throws an error as the address is invalid.In the case of valid addresses, the base address of the segment is added to the offset to get the physical address of the actual word in the main memory.The above figure shows how address translation is done in case of segmentation.Advantages of Segmentation:Disadvantages:",os segmentation,"

Easy Questions:
1. What is segmentation in OS?
2. What is a segment table?
3. What is the purpose of segmentation?

Medium Questions:
1. What are the advantages of segmentation?
2. How does the operating system translate a logical address into a physical address?
3. What is the difference between segmentation and paging?

Hard Questions:
1. How does the operating system decide the size of each segment?
2. How does the operating system decide the number of segments that can be referred?
3. How does the operating system decide the base address of each segment?

Multiple Choice Questions:
1. What is the purpose of segmentation?
A. To divide memory into fixed size parts
B. To divide memory into variable size parts
C. To divide memory into equal size parts
D. To divide memory into unequal size parts

2. What is the maximum segment size?
A. 4096
B. 16
C. 32
D. 64

3. What is the maximum number of segments that can be referred?
A. 4096
B. 16
C. 32
D. 64"
Difference between Paging and Segmentation:,os paging vs segmentation,"

Easy Questions:
1. What is the difference between paging and segmentation?
2. What is paging?
3. What is segmentation?

Medium Questions:
1. What are the advantages of paging over segmentation?
2. How does segmentation improve memory management?
3. What are the disadvantages of segmentation?

Hard Questions:
1. How does segmentation help in reducing the external fragmentation?
2. How does segmentation help in reducing the internal fragmentation?
3. How does segmentation help in improving the performance of the system?

Multiple Choice Questions:
1. What is the main difference between paging and segmentation?
a. Paging divides memory into fixed-sized blocks while segmentation divides memory into variable-sized blocks
b. Paging divides memory into variable-sized blocks while segmentation divides memory into fixed-sized blocks
c. Paging divides memory into multiple blocks while segmentation divides memory into single block
d. Paging divides memory into single block while segmentation divides memory into multiple blocks

2. What is the main advantage of segmentation over paging?
a. Segmentation reduces external fragmentation
b. Segmentation reduces internal fragmentation
c. Segmentation improves memory management
d. Segmentation improves performance of the system

3. What is the main disadvantage of segmentation?
a. Segmentation increases external fragmentation
b. Segmentation increases internal fragmentation
c. Segmentation reduces memory management
d. Segmentation reduces performance of the system"
"Segmented Paging:Pure segmentation is not very popular and not being used in many of the operating systems. However, Segmentation can be combined with Paging to get the best features out of both the techniques.In Segmented Paging, the main memory is divided into variable size segments which are further divided into fixed size pages.Segment Number → It points to the appropriate Segment Number.Page Number → It Points to the exact page within the segmentPage Offset → Used as an offset within the page frameEach Page table contains the various information about every page of the segment. The Segment Table contains the information about every segment. Each segment table entry points to a page table entry and every page table entry is mapped to one of the page within a segment.Translation of logical address to physical address:The CPU generates a logical address which is divided into two parts: Segment Number and Segment Offset. The Segment Offset must be less than the segment limit. Offset is further divided into Page number and Page Offset. To map the exact page number in the page table, the page number is added into the page table base.The actual frame number with the page offset is mapped to the main memory to get the desired word in the page of the certain segment of the process.Advantages of Segmented Paging:Disadvantages of Segmented Paging:",os segmented paging,"

Easy Questions:
1. What is Segmented Paging?
2. What is the purpose of the Segment Number?
3. What is the purpose of the Page Offset?

Medium Questions:
1. How is a logical address translated to a physical address?
2. What are the advantages of Segmented Paging?
3. What are the disadvantages of Segmented Paging?

Hard Questions:
1. How is the page number mapped to the page table?
2. How is the page offset used to map the exact page number in the page table?
3. How is the actual frame number with the page offset mapped to the main memory?

Multiple Choice Questions:
1. What is the purpose of the Segment Number?
A. To point to the appropriate Segment Number
B. To point to the exact page within the segment
C. To be used as an offset within the page frame
D. To map the exact page number in the page table

2. How is a logical address translated to a physical address?
A. By adding the page number to the page table base
B. By dividing the logical address into two parts
C. By mapping the page offset to the main memory
D. By mapping the actual frame number with the page offset

3. What are the advantages of Segmented Paging?
A. Variable size segments
B. Fixed size pages
C. Easier memory management
D. Faster access to data"
"What is a File ?:A file can be defined as a data structure which stores the sequence of records. Files are stored in a file system, which may exist on a disk or in the main memory. Files can be simple (plain text) or complex (specially-formatted).The collection of files is known as Directory. The collection of directories at the different levels, is known as File System.Attributes of the File:1.NameEvery file carries a name by which the file is recognized in the file system. One directory cannot have two files with the same name.2.IdentifierAlong with the name, Each File has its own extension which identifies the type of the file. For example, a text file has the extension .txt, A video file can have the extension .mp4.3.TypeIn a File System, the Files are classified in different types such as video files, audio files, text files, executable files, etc.4.LocationIn the File System, there are several locations on which, the files can be stored. Each file carries its location as its attribute.5.SizeThe Size of the File is one of its most important attribute. By size of the file, we mean the number of bytes acquired by the file in the memory.6.ProtectionThe Admin of the computer may want the different protections for the different files. Therefore each file carries its own set of permissions to the different group of Users.7.Time and DateEvery file carries a time stamp which contains the time and date on which the file is last modified.",os attributes of the file,"

Easy Questions:
1. What is a file?
2. What is a directory?
3. What is a file system?

Medium Questions:
1. What are the attributes of a file?
2. What is the purpose of a file extension?
3. What is the purpose of a time stamp?

Hard Questions:
1. How is a file system organized?
2. How is access to files controlled?
3. How is the size of a file determined?

Multiple Choice Questions:
1. What is the purpose of a file extension?
A. To identify the type of file
B. To identify the location of the file
C. To identify the size of the file
D. To identify the time and date of the file

2. What is the collection of files known as?
A. File System
B. Directory
C. Time Stamp
D. Extension

3. How is access to files controlled?
A. By the size of the file
B. By the type of the file
C. By the permissions of the file
D. By the name of the file"
"Operations on the File:A file is a collection of logically related data that is recorded on the secondary storage in the form of sequence of operations. The content of the files are defined by its creator who is creating the file. The various operations which can be implemented on a file such as read, write, open and close etc. are called file operations. These operations are performed by the user by using the commands provided by the operating system. Some common operations are as follows:1.Create operation:This operation is used to create a file in the file system. It is the most widely used operation performed on the file system. To create a new file of a particular type the associated application program calls the file system. This file system allocates space to the file. As the file system knows the format of directory structure, so entry of this new file is made into the appropriate directory. 2. Open operation:This operation is the common operation performed on the file. Once the file is created, it must be opened before performing the file processing operations. When the user wants to open a file, it provides a file name to open the particular file in the file system. It tells the operating system to invoke the open system call and passes the file name to the file system. 3. Write operation:This operation is used to write the information into a file. A system call write is issued that specifies the name of the file and the length of the data has to be written to the file. Whenever the file length is increased by specified value and the file pointer is repositioned after the last byte written.4. Read operation:This operation reads the contents from a file. A Read pointer is maintained by the OS, pointing to the position up to which the data has been read.5. Re-position or Seek operation:The seek system call re-positions the file pointers from the current position to a specific place in the file i.e. forward or backward depending upon the user's requirement. This operation is generally performed with those file management systems that support direct access files.6. Delete operation:Deleting the file will not only delete all the data stored inside the file it is also used so that disk space occupied by it is freed. In order to delete the specified file the directory is searched. When the directory entry is located, all the associated file space and the directory entry is released.7. Truncate operation:Truncating is simply deleting the file except deleting attributes. The file is not completely deleted although the information stored inside the file gets replaced.8. Close operation:When the processing of the file is complete, it should be closed so that all the changes made permanent and all the resources occupied should be released. On closing it deallocates all the internal descriptors that were created when the file was opened.9. Append operation:This operation adds data to the end of the file.10. Rename operation:This operation is used to rename the existing file.",os operations on the file,"

Easy Questions:
1. What is a file?
2. What are the operations that can be performed on a file?
3. What is the Create operation?

Medium Questions:
1. What is the purpose of the Open operation?
2. What is the difference between the Delete and Truncate operations?
3. How does the Close operation work?

Hard Questions:
1. How does the operating system know the format of the directory structure?
2. How does the Read operation work?
3. What is the purpose of the Append operation?

Multiple Choice Questions:
1. What is the purpose of the Rename operation?
A. To delete the file
B. To rename the existing file
C. To add data to the end of the file
D. To re-position the file pointer

2. What is the purpose of the Write operation?
A. To delete the file
B. To rename the existing file
C. To add data to the end of the file
D. To write the information into a file

3. What is the purpose of the Seek operation?
A. To delete the file
B. To rename the existing file
C. To add data to the end of the file
D. To re-position the file pointer"
"File Access Methods:Let's look at various ways to access files stored in secondary memory.Sequential Access:Most of the operating systems access the file sequentially. In other words, we can say that most of the files need to be accessed sequentially by the operating system.In sequential access, the OS read the file word by word. A pointer is maintained which initially points to the base address of the file. If the user wants to read first word of the file then the pointer provides that word to the user and increases its value by 1 word. This process continues till the end of the file.Modern word systems do provide the concept of direct access and indexed access but the most used method is sequential access due to the fact that most of the files such as text files, audio files, video files, etc need to be sequentially accessed.Direct Access:The Direct Access is mostly required in the case of database systems. In most of the cases, we need filtered information from the database. The sequential access can be very slow and inefficient in such cases.Suppose every block of the storage stores 4 records and we know that the record we needed is stored in 10th block. In that case, the sequential access will not be implemented because it will traverse all the blocks in order to access the needed record.Indexed Access:If a file can be sorted on any of the filed then an index can be assigned to a group of certain records. However, A particular record can be accessed by its index. The index is nothing but the address of a record in the file.In index accessing, searching in a large database became very quick and easy but we need to have some extra space in the memory to store the index value.",os file access methods,"

Easy Questions:
1. What is the most used method of file access?
2. What is the purpose of a pointer in sequential access?
3. What is the purpose of indexed access?

Medium Questions:
1. What are the advantages of direct access over sequential access?
2. How does indexed access improve searching in a large database?
3. What is the difference between direct access and indexed access?

Hard Questions:
1. How does the operating system determine the base address of a file?
2. How does the operating system determine the size of a file?
3. What are the challenges associated with implementing direct access and indexed access?

Multiple Choice Questions:
1. What is the most used method of file access?
A. Sequential Access
B. Direct Access
C. Indexed Access
D. Random Access

2. What is the purpose of a pointer in sequential access?
A. To store the base address of the file
B. To store the size of the file
C. To store the index of the file
D. To store the address of the next record

3. How does indexed access improve searching in a large database?
A. By providing direct access to the records
B. By providing random access to the records
C. By providing faster access to the records
D. By providing indexed access to the records"
"Directory Structure in OS (Operating System):What is a directory?:Directory can be defined as the listing of the related files on the disk. The directory may store some or the entire file attributes.To get the benefit of different file systems on the different operating systems, A hard disk can be divided into the number of partitions of different sizes. The partitions are also called volumes or mini disks.Each partition must have at least one directory in which, all the files of the partition can be listed. A directory entry is maintained for each file in the directory which stores all the information related to that file.A directory can be viewed as a file which contains the Meta data of the bunch of files.Every Directory supports a number of common operations on the file:",os directory structure,"

Easy Questions:
1. What is a directory?
2. What is a partition?
3. What is a directory entry?

Medium Questions:
1. What is the purpose of a directory?
2. How is a directory different from a file?
3. What are the common operations on a file supported by a directory?

Hard Questions:
1. How can a hard disk be divided into different partitions?
2. How can the different file systems on different operating systems be used?
3. What is the minimum number of directories that must be present in a partition?

Multiple Choice Questions:
1. What is a directory?
A. A listing of related files on the disk
B. A file which contains the Meta data of a bunch of files
C. A partition of a hard disk
D. A directory entry

Answer: A. A listing of related files on the disk

2. What is the minimum number of directories that must be present in a partition?
A. 1
B. 2
C. 3
D. 4

Answer: A. 1

3. What are the common operations on a file supported by a directory?
A. Create, Read, Update, Delete
B. Create, Read, Write, Delete
C. Create, Read, Update, Copy
D. Create, Read, Write, Copy

Answer: B. Create, Read, Write, Delete"
Single Level Directory:The simplest method is to have one big list of all the files on the disk. The entire system will contain only one directory which is supposed to mention all the files present in the file system. The directory contains one entry per each file present on the file system.This type of directories can be used for a simple system.Advantages:Disadvantages:,os single level directory,"

Easy Questions:
1. What is a single level directory?
2. What is the simplest method of organizing files on a disk?
3. What is the purpose of a single level directory?

Medium Questions:
1. What are the advantages of using a single level directory?
2. How does a single level directory work?
3. What type of system is best suited for a single level directory?

Hard Questions:
1. How can a single level directory be improved to better suit a complex system?
2. What are the security implications of using a single level directory?
3. How can a single level directory be optimized for better performance?

Multiple Choice Questions:
1. What is the purpose of a single level directory?
a. To organize files on a disk
b. To store files on a disk
c. To provide access to files on a disk
d. To manage files on a disk

2. What type of system is best suited for a single level directory?
a. Simple system
b. Complex system
c. Small system
d. Large system

3. What are the advantages of using a single level directory?
a. Easy to use
b. Easy to maintain
c. Easy to access
d. All of the above"
"Two Level Directory:In two level directory systems, we can create a separate directory for each user. There is one master directory which contains separate directories dedicated to each user. For each user, there is a different directory present at the second level, containing group of user's file. The system doesn't let a user to enter in the other user's directory without permission.Characteristics of two level directory system:Every Operating System maintains a variable as PWD which contains the present directory name (present user name) so that the searching can be done appropriately.",os two level directory,"

Easy Questions:
1. What is a two level directory system?
2. What is the purpose of the master directory?
3. What is the purpose of the PWD variable?

Medium Questions:
1. How does the two level directory system prevent users from entering other user's directories?
2. What is the difference between a two level directory system and a single level directory system?
3. How does the two level directory system keep track of the present user name?

Hard Questions:
1. How can the two level directory system be used to improve security?
2. How can the two level directory system be used to improve efficiency?
3. What are the advantages and disadvantages of using a two level directory system?

Multiple Choice Questions:
1. What is the purpose of the master directory in a two level directory system?
A. To store user files
B. To store user directories
C. To store user permissions
D. To store user passwords

2. What is the purpose of the PWD variable?
A. To store the present user name
B. To store the present directory name
C. To store the present user permissions
D. To store the present user password

3. How does the two level directory system prevent users from entering other user's directories?
A. By using passwords
B. By using permissions
C. By using encryption
D. By using authentication"
"Tree Structured Directory:In Tree structured directory system, any directory entry can either be a file or sub directory. Tree structured directory system overcomes the drawbacks of two level directory system. The similar kind of files can now be grouped in one directory.Each user has its own directory and it cannot enter in the other user's directory. However, the user has the permission to read the root's data but he cannot write or modify this. Only administrator of the system has the complete access of root directory.Searching is more efficient in this directory structure. The concept of current working directory is used. A file can be accessed by two types of path, either relative or absolute.Absolute path is the path of the file with respect to the root directory of the system while relative path is the path with respect to the current working directory of the system. In tree structured directory systems, the user is given the privilege to create the files as well as directories.Permissions on the file and directory:A tree structured directory system may consist of various levels therefore there is a set of permissions assigned to each file and directory.The permissions are R W X which are regarding reading, writing and the execution of the files or directory. The permissions are assigned to three types of users: owner, group and others.There is a identification bit which differentiate between directory and file. For a directory, it is d and for a file, it is dot (.)The following snapshot shows the permissions assigned to a file in a Linux based system. Initial bit d represents that it is a directory.",os tree structured directory,"

Easy Questions:
1. What is a Tree Structured Directory?
2. What is the purpose of a Tree Structured Directory?
3. What are the permissions assigned to a file in a Linux based system?

Medium Questions:
1. What are the two types of paths used to access a file?
2. What are the three types of users that have permissions on a file or directory?
3. What is the identification bit that differentiates between a directory and a file?

Hard Questions:
1. How does a Tree Structured Directory system overcome the drawbacks of a two level directory system?
2. How does the concept of current working directory work in a Tree Structured Directory system?
3. What are the advantages of using a Tree Structured Directory system?

Multiple Choice Questions:
1. What is the identification bit that differentiates between a directory and a file?
A. d
B. .
C. R
D. W

2. What are the permissions assigned to a file in a Linux based system?
A. R W X
B. Read Write Execute
C. Read Write Delete
D. Read Only

3. What are the two types of paths used to access a file?
A. Relative and Absolute
B. Read and Write
C. Read and Execute
D. Write and Execute"
"Acyclic-Graph Structured Directories:The tree structured directory system doesn't allow the same file to exist in multiple directories therefore sharing is major concern in tree structured directory system. We can provide sharing by making the directory an acyclic graph. In this system, two or more directory entry can point to the same file or sub directory. That file or sub directory is shared between the two directory entries.These kinds of directory graphs can be made using links or aliases. We can have multiple paths for a same file. Links can either be symbolic (logical) or hard link (physical).If a file gets deleted in acyclic graph structured directory system, then1. In the case of soft link, the file just gets deleted and we are left with a dangling pointer.2. In the case of hard link, the actual file will be deleted only if all the references to it gets deleted.",os acyclic graph directories,"

Easy Questions:
1. What is an acyclic graph structured directory?
2. What is the advantage of using an acyclic graph structured directory?
3. What is the difference between a soft link and a hard link?

Medium Questions:
1. How can an acyclic graph structured directory be made?
2. What happens when a file is deleted in an acyclic graph structured directory?
3. What is the purpose of using links or aliases in an acyclic graph structured directory?

Hard Questions:
1. How does an acyclic graph structured directory system provide sharing?
2. What are the implications of using a tree structured directory system for sharing?
3. How can multiple paths for a same file be created in an acyclic graph structured directory?

Multiple Choice Questions:
1. What happens when a file is deleted in an acyclic graph structured directory?
A. The file is deleted and a dangling pointer is left
B. The actual file is deleted
C. The file is moved to a different directory
D. The file is shared between two directory entries

Answer: A. The file is deleted and a dangling pointer is left

2. What is the purpose of using links or aliases in an acyclic graph structured directory?
A. To create multiple paths for a same file
B. To provide sharing
C. To make the directory an acyclic graph
D. To delete the file

Answer: A. To create multiple paths for a same file

3. How does an acyclic graph structured directory system provide sharing?
A. By allowing the same file to exist in multiple directories
B. By making the directory an acyclic graph
C. By using links or aliases
D. By deleting the file

Answer: C. By using links or aliases"
"File Systems:File system is the part of the operating system which is responsible for file management. It provides a mechanism to store the data and access to the file contents including data and programs. Some Operating systems treats everything as a file for example Ubuntu.The File system takes care of the following issuesWe have seen various data structures in which the file can be stored. The task of the file system is to maintain an optimal file structure.Whenever a file gets deleted from the hard disk, there is a free space created in the disk. There can be many such spaces which need to be recovered in order to reallocate them to other files.The major concern about the file is deciding where to store the files on the hard disk. There are various disks scheduling algorithm which will be covered later in this tutorial.A File may or may not be stored within only one block. It can be stored in the non contiguous blocks on the disk. We need to keep track of all the blocks on which the part of the files reside.",os file system,"

Easy Questions:
1. What is a file system?
2. What is the purpose of a file system?
3. What operating system treats everything as a file?

Medium Questions:
1. What are the various data structures in which a file can be stored?
2. What is the task of the file system?
3. What is the major concern about the file?

Hard Questions:
1. How does the file system recover free spaces in the disk?
2. What are the various disk scheduling algorithms?
3. How does the file system keep track of all the blocks on which the part of the files reside?

Multiple Choice Questions:
1. What is the purpose of a file system?
a. To store data
b. To access file contents
c. To maintain an optimal file structure
d. All of the above

2. What is the major concern about the file?
a. Deciding where to store the files on the hard disk
b. Recovering free spaces in the disk
c. Storing the files in the non contiguous blocks
d. All of the above

3. What are the various disk scheduling algorithms?
a. First Come First Serve
b. Shortest Seek Time First
c. Round Robin
d. All of the above"
"File System Structure:File System provide efficient access to the disk by allowing data to be stored, located and retrieved in a convenient way. A file System must be able to store the file, locate the file and retrieve the file.Most of the Operating Systems use layering approach for every task including file systems. Every layer of the file system is responsible for some activities.The image shown below, elaborates how the file system is divided in different layers, and also the functionality of each layer.",os file system structure,"

Easy Questions:
1. What is a File System?
2. What is the purpose of a File System?
3. What is the layering approach used for File System?

Medium Questions:
1. What are the different layers of a File System?
2. What is the purpose of each layer of the File System?
3. How does the File System provide efficient access to the disk?

Hard Questions:
1. How does the File System ensure data integrity?
2. How does the File System ensure data security?
3. How does the File System ensure data availability?

Multiple Choice Questions:
1. What is the purpose of a File System?
A. To store data
B. To locate data
C. To retrieve data
D. All of the above

2. What is the layering approach used for File System?
A. Hierarchical
B. Modular
C. Object-oriented
D. Functional

3. How does the File System provide efficient access to the disk?
A. By storing data
B. By locating data
C. By retrieving data
D. By organizing data"
"Master Boot Record (MBR):Master boot record is the information present in the first sector of any hard disk. It contains the information regarding how and where the Operating system is located in the hard disk so that it can be booted in the RAM.MBR is sometimes called master partition table because it includes a partition table which locates every partition in the hard disk.Master boot record (MBR) also includes a program which reads the boot sector record of the partition that contains operating system.What happens when you turn on your computer?:Due to the fact that the main memory is volatile, when we turn on our computer, CPUcannot access the main memory directly. However, there is a special program called as BIOS stored in ROM is accessed for the first time by the CPU.BIOS contains the code, by executing which, the CPU access the very first partition of hard disk that is MBR. It contains a partition table for all the partitions of the hard disk.Since, MBR contains the information about where the operating system is being stored and it also contains a program which can read the boot sector record of the partition, hence the CPU fetches all this information and load the operating system into the main memory.",os master boot record,"

Easy Questions:
1. What is a Master Boot Record?
2. What is stored in the MBR?
3. What is the purpose of the MBR?

Medium Questions:
1. What is the role of BIOS in the boot process?
2. What is the difference between MBR and BIOS?
3. What is the purpose of the partition table in the MBR?

Hard Questions:
1. How does the CPU access the MBR when the computer is turned on?
2. How does the CPU know which partition contains the operating system?
3. What is the difference between the main memory and ROM?

Multiple Choice Questions:
1. What is the first sector of a hard disk called?
a. BIOS
b. MBR
c. Partition Table
d. Boot Sector

2. What is the purpose of the MBR?
a. To store the operating system
b. To locate the operating system
c. To read the boot sector record
d. To access the main memory

3. What is the role of BIOS in the boot process?
a. To store the operating system
b. To locate the operating system
c. To read the boot sector record
d. To access the main memory"
"On Disk Data Structures:There are various on disk data structures that are used to implement a file system. This structure may vary depending upon the operating system.Boot Control Block contains all the information which is needed to boot an operating system from that volume. It is called boot block in UNIX file system. In NTFS, it is called the partition boot sector.Volume control block all the information regarding that volume such as number of blocks, size of each block, partition table, pointers to free blocks and free FCB blocks. In UNIX file system, it is known as super block. In NTFS, this information is stored inside master file table.A directory structure (per file system) contains file names and pointers to corresponding FCBs. In UNIX, it includes inode numbers associated to file names.File Control block contains all the details about the file such as ownership details, permission details, file size,etc. In UFS, this detail is stored in inode. In NTFS, this information is stored inside master file table as a relational database structure. A typical file control block is shown in the image below.",os on disk data structures,"

Easy Questions:
1. What is the Boot Control Block?
2. What is the Volume Control Block?
3. What is the Directory Structure?

Medium Questions:
1. What is the purpose of the File Control Block?
2. How is the File Control Block stored in UNIX file system?
3. How is the File Control Block stored in NTFS?

Hard Questions:
1. What are the differences between the Boot Control Block in UNIX and NTFS?
2. How does the Volume Control Block store information in UNIX and NTFS?
3. What are the differences between the Directory Structure in UNIX and NTFS?

Multiple Choice Questions:
1. What is the File Control Block?
a. A structure that contains all the information which is needed to boot an operating system from that volume
b. A structure that contains all the details about the file such as ownership details, permission details, file size,etc
c. A structure that contains file names and pointers to corresponding FCBs
d. A structure that contains all the information regarding that volume such as number of blocks, size of each block, partition table, pointers to free blocks and free FCB blocks

2. What is the Volume Control Block?
a. A structure that contains all the information which is needed to boot an operating system from that volume
b. A structure that contains all the details about the file such as ownership details, permission details, file size,etc
c. A structure that contains file names and pointers to corresponding FCBs
d. A structure that contains all the information regarding that volume such as number of blocks, size of each block, partition table, pointers to free blocks and free FCB blocks

3. How is the File Control Block stored in UNIX file system?
a. In a super block
b. In a master file table
c. In an inode
d. In a partition boot sector"
"In Memory Data Structure:Till now, we have discussed the data structures that are required to be present on the hard disk in order to implement file systems. Here, we will discuss the data structures required to be present in memory in order to implement the file system.The in-memory data structures are used for file system management as well as performance improvement via caching. This information is loaded on the mount time and discarded on ejection.In-memory mount table contains the list of all the devices which are being mounted to the system. Whenever the connection is maintained to a device, its entry will be done in the mount table.This is the list of directory which is recently accessed by the CPU. The directories present in the list can also be accessed in the near future so it will be better to store them temporally in cache.This is the list of all the open files in the system at a particular time. Whenever the user open any file for reading or writing, the entry will be made in this open file table.It is the list of open files subjected to every process. Since there is already a list which is there for every open file in the system thereforeIt only contains Pointers to the appropriate entry in the system wide table.",os in memory data structures,"

Easy Questions:
1. What is an in-memory data structure?
2. What is the purpose of an in-memory mount table?
3. What is an open file table?

Medium Questions:
1. What information is loaded on mount time?
2. How does caching improve performance?
3. What is the purpose of an open file table for every process?

Hard Questions:
1. How does the in-memory data structure interact with the file system?
2. How does the in-memory mount table ensure the connection is maintained?
3. What is the difference between the system wide open file table and the open file table for every process?

Multiple Choice Questions:
1. What is the purpose of an in-memory mount table?
A. To store the list of all the devices which are being mounted to the system
B. To store the list of all the open files in the system
C. To store the list of all the directories which are recently accessed by the CPU
D. To store the list of all the open files subjected to every process

2. What is the purpose of an open file table for every process?
A. To store the list of all the devices which are being mounted to the system
B. To store the list of all the open files in the system
C. To store the list of all the directories which are recently accessed by the CPU
D. To store the list of all the open files subjected to every process

3. How does caching improve performance?
A. By storing the list of all the devices which are being mounted to the system
B. By storing the list of all the open files in the system
C. By storing the list of all the directories which are recently accessed by the CPU
D. By storing the data in memory for quick access"
"Directory Implementation:There is the number of algorithms by using which, the directories can be implemented. However, the selection of an appropriate directory implementation algorithm may significantly affect the performance of the system.The directory implementation algorithms are classified according to the data structure they are using. There are mainly two algorithms which are used in these days.1. Linear List:In this algorithm, all the files in a directory are maintained as singly lined list. Each file contains the pointers to the data blocks which are assigned to it and the next file in the directory.Characteristics2. Hash Table:To overcome the drawbacks of singly linked list implementation of directories, there is an alternative approach that is hash table. This approach suggests to use hash table along with the linked lists.A key-value pair for each file in the directory gets generated and stored in the hash table. The key can be determined by applying the hash function on the file name while the key points to the corresponding file stored in the directory.Now, searching becomes efficient due to the fact that now, entire list will not be searched on every operating. Only hash table entries are checked using the key and if an entry found then the corresponding file will be fetched using the value.",os directory implementation,"

Easy Questions:
1. What are the two algorithms used for directory implementation?
2. What is the purpose of a hash table in directory implementation?
3. What is the data structure used in linear list algorithm?

Medium Questions:
1. What are the characteristics of linear list algorithm?
2. How does the hash table approach improve the performance of the system?
3. What is the purpose of a hash function in directory implementation?

Hard Questions:
1. How does the hash table approach reduce the complexity of searching?
2. What are the advantages and disadvantages of linear list algorithm?
3. How can the directory implementation algorithms be classified?

Multiple Choice Questions:
1. What is the data structure used in linear list algorithm?
a. Hash Table
b. Linked List
c. Binary Tree
d. Array

2. What is the purpose of a hash table in directory implementation?
a. To store the file names
b. To store the data blocks
c. To generate a key-value pair for each file
d. To search the entire list

3. How does the hash table approach improve the performance of the system?
a. By reducing the complexity of searching
b. By increasing the complexity of searching
c. By increasing the speed of searching
d. By reducing the speed of searching"
Allocation Methods:There are various methods which can be used to allocate disk space to the files. Selection of an appropriate allocation method will significantly affect the performance and efficiency of the system. Allocation method provides a way in which the disk will be utilized and the files will be accessed.We will discuss three of the most used methods in detail.,os allocation methods,"

Easy Questions:
1. What are the various methods used to allocate disk space?
2. What is the purpose of an allocation method?
3. How does the selection of an allocation method affect the system?

Medium Questions:
1. What are the three most used allocation methods?
2. How does an allocation method affect the performance of the system?
3. What are the advantages and disadvantages of each allocation method?

Hard Questions:
1. How can the allocation method be optimized for better performance?
2. How can the allocation method be used to improve the efficiency of the system?
3. What are the implications of using different allocation methods in different scenarios?

Multiple Choice Questions:
1. What is the purpose of an allocation method?
A. To provide a way to access files
B. To provide a way to store files
C. To provide a way to utilize disk space
D. To provide a way to manage disk space

2. How does the selection of an allocation method affect the system?
A. It increases the performance
B. It decreases the performance
C. It increases the efficiency
D. It decreases the efficiency

3. What are the three most used allocation methods?
A. First-fit, Best-fit, Worst-fit
B. First-come, First-served, Worst-fit
C. First-come, First-served, Best-fit
D. First-fit, Worst-fit, Best-fit"
"Contiguous Allocation:If the blocks are allocated to the file in such a way that all the logical blocks of the file get the contiguous physical block in the hard disk then such allocation scheme is known as contiguous allocation.In the image shown below, there are three files in the directory. The starting block and the length of each file are mentioned in the table. We can check in the table that the contiguous blocks are assigned to each file as per its need.Advantages:Disadvantages:",os contiguous allocation,"

Easy Questions:
1. What is Contiguous Allocation?
2. What is shown in the image below?
3. What are the advantages of Contiguous Allocation?

Medium Questions:
1. What is the purpose of assigning contiguous blocks to each file?
2. How does Contiguous Allocation improve disk performance?
3. What are the limitations of Contiguous Allocation?

Hard Questions:
1. How does Contiguous Allocation handle fragmentation?
2. How does Contiguous Allocation handle file growth?
3. How does Contiguous Allocation handle file deletion?

Multiple Choice Questions:
1. What is the purpose of assigning contiguous blocks to each file?
a. To improve disk performance
b. To reduce fragmentation
c. To improve file access
d. To reduce file growth

2. What are the advantages of Contiguous Allocation?
a. Improved disk performance
b. Reduced fragmentation
c. Improved file access
d. All of the above

3. How does Contiguous Allocation handle file deletion?
a. It creates more fragmentation
b. It reduces disk performance
c. It improves file access
d. It reduces file growth"
"Linked List Allocation:Linked List allocation solves all problems of contiguous allocation. In linked list allocation, each file is considered as the linked list of disk blocks. However, the disks blocks allocated to a particular file need not to be contiguous on the disk. Each disk block allocated to a file contains a pointer which points to the next disk block allocated to the same file.Advantages:Disadvantages :",os linked list allocation,"

Easy Questions:
1. What is Linked List Allocation?
2. What are the advantages of Linked List Allocation?
3. What are the disadvantages of Linked List Allocation?

Medium Questions:
1. How does Linked List Allocation solve the problems of contiguous allocation?
2. What is the purpose of the pointer in each disk block allocated to a file?
3. How does Linked List Allocation differ from contiguous allocation?

Hard Questions:
1. What are the implications of Linked List Allocation on disk access time?
2. How does Linked List Allocation affect the performance of the system?
3. What are the security implications of Linked List Allocation?

Multiple Choice Questions:
1. What is the purpose of the pointer in each disk block allocated to a file?
A. To point to the next disk block allocated to the same file
B. To point to the previous disk block allocated to the same file
C. To point to the first disk block allocated to the same file
D. To point to the last disk block allocated to the same file

2. What are the advantages of Linked List Allocation?
A. Faster disk access time
B. More efficient memory utilization
C. Easier to manage
D. All of the above

3. What are the disadvantages of Linked List Allocation?
A. Slower disk access time
B. Less efficient memory utilization
C. Harder to manage
D. None of the above"
"File Allocation Table:The main disadvantage of linked list allocation is that the Random access to a particular block is not provided. In order to access a block, we need to access all its previous blocks.File Allocation Table overcomes this drawback of linked list allocation. In this scheme, a file allocation table is maintained, which gathers all the disk block links. The table has one entry for each disk block and is indexed by block number.File allocation table needs to be cached in order to reduce the number of head seeks. Now the head doesn't need to traverse all the disk blocks in order to access one successive block.It simply accesses the file allocation table, read the desired block entry from there and access that block. This is the way by which the random access is accomplished by using FAT. It is used by MS-DOS and pre-NT Windows versions.Advantages:Disadvantages:",os file allocation table,"

Easy Questions:
1. What is File Allocation Table?
2. What is the main disadvantage of linked list allocation?
3. What is the purpose of caching the File Allocation Table?

Medium Questions:
1. How does File Allocation Table overcome the disadvantage of linked list allocation?
2. What is the purpose of the File Allocation Table?
3. How does the File Allocation Table enable random access?

Hard Questions:
1. What are the advantages and disadvantages of using File Allocation Table?
2. How does the File Allocation Table reduce the number of head seeks?
3. How does the File Allocation Table enable the head to access successive blocks?

Multiple Choice Questions:
1. What is the main disadvantage of linked list allocation?
a. Random access to a particular block is not provided
b. File Allocation Table needs to be cached
c. Head needs to traverse all the disk blocks
d. File Allocation Table is used by MS-DOS

2. How does File Allocation Table enable random access?
a. By accessing the file allocation table
b. By accessing all the disk blocks
c. By caching the file allocation table
d. By traversing all the disk blocks

3. What is the purpose of caching the File Allocation Table?
a. To reduce the number of head seeks
b. To access the desired block entry
c. To access all the disk blocks
d. To traverse all the disk blocks"
"Indexed Allocation:Limitation of FAT:Limitation in the existing technology causes the evolution of a new technology. Till now, we have seen various allocation methods; each of them was carrying several advantages and disadvantages.File allocation table tries to solve as many problems as possible but leads to a drawback. The more the number of blocks, the more will be the size of FAT.Therefore, we need to allocate more space to a file allocation table. Since, file allocation table needs to be cached therefore it is impossible to have as many space in cache. Here we need a new technology which can solve such problems.Indexed Allocation Scheme:Instead of maintaining a file allocation table of all the disk pointers, Indexed allocation scheme stores all the disk pointers in one of the blocks called as indexed block. Indexed block doesn't hold the file data, but it holds the pointers to all the disk blocks allocated to that particular file. Directory entry will only contain the index block address.Advantages:Disadvantages:",os indexed allocation,"

Easy Questions:
1. What is Indexed Allocation?
2. What is the limitation of FAT?
3. What is the purpose of an Indexed Block?

Medium Questions:
1. What are the advantages of Indexed Allocation?
2. How does Indexed Allocation differ from FAT?
3. What is the purpose of a Directory Entry?

Hard Questions:
1. How does Indexed Allocation improve the caching of a File Allocation Table?
2. How does Indexed Allocation reduce the size of a File Allocation Table?
3. What are the implications of using Indexed Allocation for large files?

Multiple Choice Questions:
1. What is the purpose of an Indexed Block?
A. To store the file data
B. To store the disk pointers
C. To store the directory entry
D. To store the file allocation table

2. What is the limitation of FAT?
A. It is difficult to cache
B. It takes up too much space
C. It is not secure
D. It is slow

3. How does Indexed Allocation reduce the size of a File Allocation Table?
A. By storing the disk pointers in an indexed block
B. By caching the file allocation table
C. By storing the directory entry in an indexed block
D. By storing the file data in an indexed block"
"Linked Index Allocation:Single level linked Index Allocation:In index allocation, the file size depends on the size of a disk block. To allow large files, we have to link several index blocks together. In linked index allocation,For the larger files, the last entry of the index block is a pointer which points to another index block. This is also called as linked schema.Advantage: It removes file size limitationsDisadvantage: Random Access becomes a bit harderMultilevel Index Allocation:In Multilevel index allocation, we have various levels of indices. There are outer level index blocks which contain the pointers to the inner level index blocks and the inner level index blocks contain the pointers to the file data.Advantage: Random Access becomes better and efficient.Disadvantage: Access time for a file will be higher.",os linked index allocation,"

Easy Questions:
1. What is Linked Index Allocation?
2. What is the advantage of Linked Index Allocation?
3. What is Multilevel Index Allocation?

Medium Questions:
1. What is the disadvantage of Linked Index Allocation?
2. What is the advantage of Multilevel Index Allocation?
3. How does Multilevel Index Allocation improve Random Access?

Hard Questions:
1. How does the file size affect the Index Allocation?
2. How does the Access time for a file change in Multilevel Index Allocation?
3. How does the structure of Multilevel Index Allocation work?

Multiple Choice Questions:
1. What is the advantage of Linked Index Allocation?
A. It removes file size limitations
B. Random Access becomes better and efficient
C. Access time for a file will be higher
D. All of the above

2. What is the disadvantage of Linked Index Allocation?
A. It removes file size limitations
B. Random Access becomes better and efficient
C. Access time for a file will be higher
D. None of the above

3. What is the advantage of Multilevel Index Allocation?
A. It removes file size limitations
B. Random Access becomes better and efficient
C. Access time for a file will be higher
D. All of the above"
"Inode:In UNIX based operating systems, each file is indexed by an Inode. Inode are the special disk block which is created with the creation of the file system. The number of files or directories in a file system depends on the number of Inodes in the file system.An Inode includes the following information",os inode,":

Easy Questions:
1. What is an Inode?
2. What operating systems use Inodes?
3. What information is stored in an Inode?

Medium Questions:
1. How is an Inode created?
2. How does the number of Inodes affect the number of files or directories in a file system?
3. What is the purpose of an Inode?

Hard Questions:
1. How does the operating system use Inodes to locate files?
2. How does the operating system determine the size of an Inode?
3. How does the operating system manage Inodes when files are deleted?

Multiple Choice Questions:
1. What is an Inode?
A. A special disk block
B. A file system
C. A type of operating system
D. A type of file

2. How does the number of Inodes affect the number of files or directories in a file system?
A. It increases the number of files or directories
B. It decreases the number of files or directories
C. It has no effect on the number of files or directories
D. It depends on the type of file system

3. What information is stored in an Inode?
A. File name and size
B. File type and size
C. File name and type
D. File name, type, and size"
"Free Space Management:A file system is responsible to allocate the free blocks to the file therefore it has to keep track of all the free blocks present in the disk. There are mainly two approaches by using which, the free blocks in the disk are managed.1. Bit Vector:In this approach, the free space list is implemented as a bit map vector. It contains the number of bits where each bit represents each block.If the block is empty then the bit is 1 otherwise it is 0. Initially all the blocks are empty therefore each bit in the bit map vector contains 1.LAs the space allocation proceeds, the file system starts allocating blocks to the files and setting the respective bit to 0.2. Linked List:It is another approach for free space management. This approach suggests linking together all the free blocks and keeping a pointer in the cache which points to the first free block.Therefore, all the free blocks on the disks will be linked together with a pointer. Whenever a block gets allocated, its previous free block will be linked to its next free block.",os free space management,"

Easy Questions:
1. What is a file system responsible for?
2. What are the two approaches for free space management?
3. What does a bit vector contain?

Medium Questions:
1. How does the file system set the respective bit to 0 when a block is allocated?
2. How does the linked list approach work for free space management?
3. What is the purpose of the pointer in the cache?

Hard Questions:
1. How does the file system determine which blocks are free?
2. How does the file system ensure that the free blocks are not allocated twice?
3. How does the file system keep track of the free blocks when the disk is full?

Multiple Choice Questions:
1. What is the initial value of each bit in the bit map vector?
A. 0
B. 1
C. 2
D. 3

2. What is the purpose of the pointer in the cache?
A. To link together all the free blocks
B. To point to the first free block
C. To represent each block
D. To set the respective bit to 0

3. How does the file system set the respective bit to 0 when a block is allocated?
A. By linking together all the free blocks
B. By pointing to the first free block
C. By setting the bit to 0
D. By representing each block"
"Disk Scheduling Algorithms in OS (Operating System):As we know, a process needs two type of time, CPU time and IO time. For I/O, it requests the Operating system to access the disk.However, the operating system must be fare enough to satisfy each request and at the same time, operating system must maintain the efficiency and speed of process execution.The technique that operating system uses to determine the request which is to be satisfied next is called disk scheduling.Let's discuss some important terms related to disk scheduling.Seek Time:Seek time is the time taken in locating the disk arm to a specified track where the read/write request will be satisfied.Rotational Latency:It is the time taken by the desired sector to rotate itself to the position from where it can access the R/W heads.Transfer Time:It is the time taken to transfer the data.Disk Access Time:Disk access time is given as,Disk Access Time = Rotational Latency + Seek Time + Transfer TimeDisk Response Time:It is the average of time spent by each request waiting for the IO operation.Purpose of Disk Scheduling:The main purpose of disk scheduling algorithm is to select a disk request from the queue of IO requests and decide the schedule when this request will be processed.Goal of Disk Scheduling Algorithm:Disk Scheduling Algorithms:The list of various disks scheduling algorithm is given below. Each algorithm is carrying some advantages and disadvantages. The limitation of each algorithm leads to the evolution of a new algorithm.",os disk scheduling,"

Easy Questions:
1. What is the purpose of disk scheduling?
2. What is the goal of disk scheduling algorithm?
3. What is the disk access time?

Medium Questions:
1. What is the difference between seek time and rotational latency?
2. What is the disk response time?
3. What are the advantages and disadvantages of each disk scheduling algorithm?

Hard Questions:
1. How does the operating system maintain the efficiency and speed of process execution?
2. How does the operating system decide which request to satisfy next?
3. How does the operating system ensure fairness when satisfying requests?

Multiple Choice Questions:
1. What is the disk access time?
A. Seek Time + Rotational Latency + Transfer Time
B. Seek Time + Rotational Latency
C. Rotational Latency + Transfer Time
D. Seek Time + Transfer Time

2. What is the goal of disk scheduling algorithm?
A. To select a disk request from the queue of IO requests
B. To decide the schedule when this request will be processed
C. To maintain the efficiency and speed of process execution
D. To ensure fairness when satisfying requests

3. What is the purpose of disk scheduling?
A. To select a disk request from the queue of IO requests
B. To decide the schedule when this request will be processed
C. To maintain the efficiency and speed of process execution
D. To ensure fairness when satisfying requests"
"FCFS Scheduling Algorithm:It is the simplest Disk Scheduling algorithm. It services the IO requests in the order in which they arrive. There is no starvation in this algorithm, every request is serviced.Disadvantages:Example:Consider the following disk request sequence for a disk with 100 tracks
45, 21, 67, 90, 4, 50, 89, 52, 61, 87, 25Head pointer starting at 50 and moving in left direction. Find the number of head movements in cylinders using FCFS scheduling.Solution:Number of cylinders moved by the head= (50-45)+(45-21)+(67-21)+(90-67)+(90-4)+(50-4)+(89-50)+(61-52)+(87-61)+(87-25)= 5 + 24 + 46 + 23 + 86 + 46 + 49 + 9 + 26 + 62= 376",os fcfs scheduling algorithm,"

Easy Questions:
1. What is FCFS Scheduling Algorithm?
2. What is the starting point of the head pointer in the example?
3. What is the total number of head movements in the example?

Medium Questions:
1. What is the advantage of using FCFS Scheduling Algorithm?
2. What is the disadvantage of using FCFS Scheduling Algorithm?
3. How does the head pointer move in the example?

Hard Questions:
1. How can the number of head movements be minimized in the example?
2. How can the FCFS Scheduling Algorithm be improved?
3. What is the time complexity of the FCFS Scheduling Algorithm?

Multiple Choice Questions:
1. What is the total number of head movements in the example?
A. 376
B. 5
C. 24
D. 46

2. What is the starting point of the head pointer in the example?
A. 50
B. 45
C. 21
D. 4

3. How does the head pointer move in the example?
A. Left
B. Right
C. Up
D. Down"
"SSTF Scheduling Algorithm:Shortest seek time first (SSTF) algorithm selects the disk I/O request which requires the least disk arm movement from its current position regardless of the direction. It reduces the total seek time as compared to FCFS.It allows the head to move to the closest track in the service queue.Disadvantages:Example:Consider the following disk request sequence for a disk with 100 tracks45, 21, 67, 90, 4, 89, 52, 61, 87, 25Head pointer starting at 50. Find the number of head movements in cylinders using SSTF scheduling.Solution::Number of cylinders = 5 + 7 + 9 + 6 + 20 + 2 + 1 + 65 + 4 + 17 = 136",os sstf scheduling algorithm,"

Easy Questions:
1. What is the SSTF Scheduling Algorithm?
2. What is the purpose of the SSTF Scheduling Algorithm?
3. What is the starting position of the head pointer in the example?

Medium Questions:
1. What is the total number of head movements in cylinders using SSTF scheduling?
2. What is the disadvantage of using the SSTF Scheduling Algorithm?
3. How does the SSTF Scheduling Algorithm reduce the total seek time?

Hard Questions:
1. How does the SSTF Scheduling Algorithm select the disk I/O request?
2. How does the SSTF Scheduling Algorithm allow the head to move to the closest track in the service queue?
3. What is the difference between the SSTF Scheduling Algorithm and the FCFS Scheduling Algorithm?

Multiple Choice Questions:
1. What is the total number of head movements in cylinders using SSTF scheduling?
a. 5
b. 7
c. 9
d. 136

2. What is the disadvantage of using the SSTF Scheduling Algorithm?
a. It increases the total seek time
b. It reduces the total seek time
c. It does not allow the head to move to the closest track in the service queue
d. It does not select the disk I/O request which requires the least disk arm movement from its current position

3. How does the SSTF Scheduling Algorithm select the disk I/O request?
a. It selects the disk I/O request which requires the most disk arm movement from its current position
b. It selects the disk I/O request which requires the least disk arm movement from its current position
c. It selects the disk I/O request which requires the most disk arm movement from its starting position
d. It selects the disk I/O request which requires the least disk arm movement from its starting position"
"SCAN and C-SCAN algorithm:Scan Algorithm:It is also called as Elevator Algorithm. In this algorithm, the disk arm moves into a particular direction till the end, satisfying all the requests coming in its path,and then it turns backand moves in the reverse direction satisfying requests coming in its path.It works in the way an elevator works, elevator moves in a direction completely till the last floor of that direction and then turns back.Example:Consider the following disk request sequence for a disk with 100 tracks98, 137, 122, 183, 14, 133, 65, 78Head pointer starting at 54 and moving in left direction. Find the number of head movements in cylinders using SCAN scheduling.Number of Cylinders = 40 + 14 + 65 + 13 + 20 + 24 + 11 + 4 + 46 = 237C-SCAN algorithm:In C-SCAN algorithm, the arm of the disk moves in a particular direction servicing requests until it reaches the last cylinder, then it jumps to the last cylinder of the opposite direction without servicing any request then it turns back and start moving in that direction servicing the remaining requests.Example:Consider the following disk request sequence for a disk with 100 tracks98, 137, 122, 183, 14, 133, 65, 78Head pointer starting at 54 and moving in left direction. Find the number of head movements in cylinders using C-SCAN scheduling.No. of cylinders crossed = 40 + 14 + 199 + 16 + 46 + 4 + 11 + 24 + 20 + 13 = 387",os scan and c scan algorithm,"

Easy Questions:
1. What is the Scan Algorithm also known as?
2. What is the C-SCAN Algorithm?
3. What is the purpose of the Scan and C-SCAN Algorithms?

Medium Questions:
1. What is the difference between the Scan and C-SCAN Algorithms?
2. How does the Scan Algorithm work?
3. How does the C-SCAN Algorithm work?

Hard Questions:
1. How can the Scan and C-SCAN Algorithms be used to optimize disk access time?
2. What are the advantages and disadvantages of using the Scan and C-SCAN Algorithms?
3. How can the Scan and C-SCAN Algorithms be used to improve disk performance?

Multiple Choice Questions:
1. What is the purpose of the Scan and C-SCAN Algorithms?
A. To optimize disk access time
B. To improve disk performance
C. To reduce disk latency
D. All of the above

2. How does the Scan Algorithm work?
A. It moves in a particular direction servicing requests until it reaches the last cylinder
B. It moves in a particular direction satisfying all requests coming in its path
C. It jumps to the last cylinder of the opposite direction without servicing any request
D. It turns back and start moving in that direction servicing the remaining requests

3. How does the C-SCAN Algorithm work?
A. It moves in a particular direction servicing requests until it reaches the last cylinder
B. It moves in a particular direction satisfying all requests coming in its path
C. It jumps to the last cylinder of the opposite direction without servicing any request
D. It turns back and start moving in that direction servicing the remaining requests"
"Look Scheduling:It is like SCAN scheduling Algorithm to some extant except the difference that, in this scheduling algorithm, the arm of the disk stops moving inwards (or outwards) when no more request in that direction exists. This algorithm tries to overcome the overhead of SCAN algorithm which forces disk arm to move in one direction till the end regardless of knowing if any request exists in the direction or not.Example:Consider the following disk request sequence for a disk with 100 tracks98, 137, 122, 183, 14, 133, 65, 78Head pointer starting at 54 and moving in left direction. Find the number of head movements in cylinders using LOOK scheduling.Number of cylinders crossed = 40 + 51 + 13 + +20 + 24 + 11 + 4 + 46 = 209C Look Scheduling:C Look Algorithm is similar to C-SCAN algorithm to some extent. In this algorithm, the arm of the disk moves outwards servicing requests until it reaches the highest request cylinder, then it jumps to the lowest request cylinder without servicing any request then it again start moving outwards servicing the remaining requests.It is different from C SCAN algorithm in the sense that, C SCAN force the disk arm to move till the last cylinder regardless of knowing whether any request is to be serviced on that cylinder or not.Example:Consider the following disk request sequence for a disk with 100 tracks98, 137, 122, 183, 14, 133, 65, 78Head pointer starting at 54 and moving in left direction. Find the number of head movements in cylinders using C LOOK scheduling.Number of cylinders crossed = 11 + 13 + 20 + 24 + 11 + 4 + 46 + 169 = 298",os look and c look scheduling,"

Easy Questions:
1. What is Look Scheduling?
2. What is the difference between Look Scheduling and SCAN Scheduling?
3. What is the example given in the paragraph?

Medium Questions:
1. What is the difference between C Look Scheduling and C SCAN Scheduling?
2. How many cylinders were crossed in the example given in the paragraph?
3. What is the starting point of the head pointer in the example given in the paragraph?

Hard Questions:
1. How does the C Look Scheduling algorithm work?
2. What is the purpose of C Look Scheduling?
3. How does C Look Scheduling reduce the overhead of SCAN Scheduling?

Multiple Choice Questions:
1. What is the starting point of the head pointer in the example given in the paragraph?
A. 54
B. 98
C. 137
D. 183

2. How many cylinders were crossed in the example given in the paragraph?
A. 40
B. 51
C. 169
D. 209

3. What is the difference between Look Scheduling and SCAN Scheduling?
A. Look Scheduling is faster than SCAN Scheduling
B. Look Scheduling moves the disk arm in one direction
C. Look Scheduling stops the disk arm when no more requests exist in that direction
D. Look Scheduling moves the disk arm to the last cylinder regardless of requests"
"Numerical on SSTF and SCAN:Question:Suppose the following disk request sequence (track numbers) for a disk with 100 tracks is given: 45, 20, 90, 10, 50, 60, 80 and 70. Assume that the initial position of the R/W head is on track 50. The additional distance that will be traversed by the R/W head when the Shortest Seek Time First (SSTF) algorithm is used compared to the SCAN (Elevator) algorithm (assuming that SCAN algorithm moves towards 100 when it starts execution) is _________ tracks(A) 5
(B) 9
(C) 10
(D) 11Using SSTF Algorithm:Number of track are 100.Initial Position of R/W head is 50.The requests are: 45, 20, 90, 10, 50, 60, 80 and 70Number of crossed cylinders = 5 + 15 + 10 + 10 + 10 + 70 + 10 = 130Using SCAN Algorithm:Number of cylinders crosses = 0 + 10 + 10 + 10 + 10 + 10 + 55 + 25 + 10 = 140Therefore the answer is (C). The SCAN algorithm travels for 10 additional tracks.",os numerical on sstf and scan,"

Easy Questions:
1. What is the initial position of the R/W head?
2. What is the total number of tracks?
3. What algorithm is being used?

Medium Questions:
1. What is the total number of cylinders crossed when using the SSTF algorithm?
2. What is the total number of cylinders crossed when using the SCAN algorithm?
3. How many additional tracks does the SCAN algorithm travel compared to the SSTF algorithm?

Hard Questions:
1. What is the difference between the SSTF and SCAN algorithms?
2. How can the SCAN algorithm be optimized to reduce the number of cylinders crossed?
3. What other algorithms can be used to optimize disk request sequences?

Multiple Choice Questions:
1. What is the additional distance that will be traversed by the R/W head when the Shortest Seek Time First (SSTF) algorithm is used compared to the SCAN (Elevator) algorithm?
(A) 5
(B) 9
(C) 10
(D) 11

2. How many cylinders are crossed when using the SCAN algorithm?
(A) 5
(B) 10
(C) 55
(D) 140

3. How many additional tracks does the SCAN algorithm travel compared to the SSTF algorithm?
(A) 5
(B) 9
(C) 10
(D) 11"
"Numerical on Disk Scheduling Algorithms:Q. Consider a disk with 200 tracks and the queue has random requests from different processes in the order::55, 58, 39, 18, 90, 160, 150, 38, 184Initially arm is at 100. Find the Average Seek length using FIFO, SSTF, SCAN and C-SCAN algorithm.Solution :",os numerical on disk scheduling algorithms,"

Easy Questions:
1. What is the initial position of the arm?
2. How many tracks are on the disk?
3. What is the queue of requests?

Medium Questions:
1. What is the purpose of disk scheduling algorithms?
2. What is the difference between FIFO and SSTF algorithms?
3. How does the C-SCAN algorithm work?

Hard Questions:
1. How does the SCAN algorithm handle requests when the arm reaches the end of the disk?
2. How can the Average Seek Length be minimized?
3. What are the trade-offs between the different disk scheduling algorithms?

Multiple Choice Questions:
1. What is the initial position of the arm?
A. 50
B. 55
C. 100
D. 150

2. What is the purpose of disk scheduling algorithms?
A. To minimize the Average Seek Length
B. To maximize the Average Seek Length
C. To reduce the number of requests
D. To increase the number of requests

3. How does the SCAN algorithm handle requests when the arm reaches the end of the disk?
A. It reverses direction
B. It stops
C. It continues in the same direction
D. It starts from the beginning"